{"meta":{"title":"ZBlog","subtitle":"waiting","description":"技术和生活，融为一体","author":"赵鑫涛","url":"http://yoursite.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-08-21T07:59:02.711Z","updated":"2019-08-21T06:37:13.678Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":"404 Not Found **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"Friends","date":"2019-08-24T07:46:44.899Z","updated":"2019-08-24T07:46:44.899Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":""},{"title":"关于","date":"2019-08-24T05:33:04.340Z","updated":"2019-08-24T05:33:04.340Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"一个努力的程序猿，在线分享自己学习以及工作的心得，希望能与大家一起交流，共同进步！"},{"title":"projects","date":"2019-08-24T05:48:58.982Z","updated":"2019-08-24T05:48:58.982Z","comments":true,"path":"projects/index.html","permalink":"http://yoursite.com/projects/index.html","excerpt":"","text":"Spider简单爬虫 基于Java的简单爬虫程序详情请点击：https://github.com/zhaoxintaoaa/Spider"},{"title":"所有分类","date":"2019-08-21T08:53:32.192Z","updated":"2019-08-21T06:39:15.328Z","comments":true,"path":"blog/categories/index.html","permalink":"http://yoursite.com/blog/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-08-21T07:59:15.792Z","updated":"2019-08-21T06:40:20.856Z","comments":true,"path":"blog/tags/index.html","permalink":"http://yoursite.com/blog/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-21T11:36:09.246Z","updated":"2019-08-21T11:36:09.246Z","comments":true,"path":"blog/archives/index.html","permalink":"http://yoursite.com/blog/archives/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-21T11:33:39.255Z","updated":"2019-08-21T11:33:39.255Z","comments":true,"path":"blog/mylist/index.html","permalink":"http://yoursite.com/blog/mylist/index.html","excerpt":"","text":""}],"posts":[{"title":"Flink详细介绍","slug":"2018-06-24-Flink详细介绍","date":"2018-06-23T16:00:00.000Z","updated":"2019-09-05T07:38:46.145Z","comments":true,"path":"2018/06/24/2018-06-24-Flink详细介绍/","link":"","permalink":"http://yoursite.com/2018/06/24/2018-06-24-Flink详细介绍/","excerpt":"Flink详细介绍Flink API的抽象级别","text":"Flink详细介绍Flink API的抽象级别 Flink DataStreamAPIDataSource source是程序的数据源输入，可以通过StreamExecutionEnvironment.addSource(sourceFunction来给程序添加一个source Flink提供了大量已经实现好的source方法，我们也可以自定义source 通过实现sourceFunction接口来自定义无并行度的source 或者你也可以通过实现ParallelSourceFunction 接口or 继承RichParallelSourceFunction 来自定义有并行度的source。 source的类型 基于socket socketTextStream从socket中读取数据，元素可以通过一个分隔符切开。 基于集合 fromCollection(Collection) 通过java的collection集合创建一个数据流，集合中的所有元素必须是相同类型的。 自定义输入 addSource 可以实现读取第三方数据源的数据 系统内置提供了一批connectors，连接器会提供对应的source支持 内置connectors 连接器 Apache Kafka (source/sink) RabbitMQ (source/sink) Apache ActiveMQ (source/sink) source的容错性保证 Source 语义保证 备注 kafka exactly once(仅一次) 建议使用0.10及以上 Collections exactly once Files exactly once Sockets at most once Transformation map：输入一个元素，然后返回一个元素，中间可以做一些清洗转换等操作 flatmap：输入一个元素，可以返回零个，一个或者多个元素 filter：过滤函数，对传入的数据进行判断，符合条件的数据会被留下 keyby：根据指定的key进行分组，相同key的数据会进入同一个分区 reduce：对数据进行聚合操作，结合当前元素和上一次reduce返回的值进行聚合操作，然后返回一个新的值 aggregations：sum(),min(),max()等 window：后面会详细描述 union：合并多个流，新的流会包含所有流中的数据 注意：所有合并的流类型必须是一致的 connect：和union类似，但是只能连接两个流，两个流的数据类型可以不同，会对两个流中的数据应用不同的处理方法。 CoMap, CoFlatMap：在ConnectedStreams中需要使用这种函数，类似于map和flatmap sink 输出的类型 writeAsText（）：将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取 print（）/printToErr（）：打印每个元素的toString()方法的值到标准输出或者标准错误输出流中 自定义输出addSink 例如：kafka Redis等 内置connectors连接器 Apache Kafka (source/sink) Apache Cassandra (sink) Elasticsearch (sink) Hadoop FileSystem (sink) RabbitMQ (source/sink) Apache ActiveMQ (source/sink) Redis (sink) sink的容错性保证 Sink 语义保证 备注 hdfs exactly once elasticsearch at least once kafka produce at least once/exactly once Kafka 0.9 and 0.10提供at least once Kafka 0.11提供exactly once file at least once redis at least once 自定义sink 实现自定义sink 实现SinkFunction接口 继承RichSinkFunction Flink DataSetAPIDataSource 基于文件 readTextFile(path) 基于集合 fromCollection(Collection) Transformations map：输入一个元素，然后返回一个元素，中间可以做一些清洗转换等操作 flatmap：输入一个元素，可以返回零个，一个或者多个元素 mappartition：类似map，一次处理一个分区的数据【如果在进行map处理的时候需要获取第三方资源链接，建议使用MapPartition】 filter：过滤函数，对传入的数据进行判断，符合条件的数据会被留下 reduce：对数据进行聚合操作，结合当前元素和上一次reduce返回的值进行聚合操作，然后返回一个新的值 Aggregate：sum、max、min等 distinct：返回一个数据集中去重之后的元素，data.distinct() cross：获取两个数据集的笛卡尔积 union：返回两个数据集的总和，数据类型需要一致 first-n: 获取集合中的前N个元素 Sort Partition：在本地对数据集的所有分区进行排序，通过sortPartition()的链接调用来完成对多个字段的排序 sink writeAsText():将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取 writeAsCsv():将元组以逗号分隔写入文件中，行及字段之间的分隔是可配置的。每个字段的值来自对象的toString()方法 print():打印每个元素的toString()方法的值到标准输出或者标准错误输出流中 Flink SQL Flink针对流处理和批处理提供了相关的API-Table API和SQL。 注意：目前Table API和SQL功能尚未全部完成，官方正在积极开发中。 暂时不推荐使用 故暂时不解释其用法 Flink Broadcast &amp; Accumulators &amp; CountersBroadcast广播变量 广播变量允许编程人员在每台机器上保持1个只读的缓存变量，而不是传送变量的副本给tasks 广播变量创建后，它可以运行在集群中的任何function上，而不需要多次传递给集群节点。另外需要记住，不应该修改广播变量，这样才能确保每个节点获取到的值都是一致的 一句话解释，可以理解为是一个公共的共享变量，我们可以把一个dataset数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。如果不使用broadcast，则在每个节点中的每个task中都需要拷贝一份dataset数据集，比较浪费内存(也就是一个节点中可能会存在多份dataset数据)。 用法： 初始化数据 DataSet toBroadcast = env.fromElements(1, 2, 3); 广播数据 .withBroadcastSet(toBroadcast,”broadcastSetName”); 获取广播数据 Collection broadcastSet =getRuntimeContext().getBroadcastVariable(“broadcastSetName”); 注意： 广播出去的变量存在于每个节点的内存中，所以这个数据集不能太大。因为广播出去的数据，会常驻内存，除非程序执行结束 广播变量在初始化广播出去以后不支持修改，这样才能保证每个节点的数据都是一致的。 Accumulators 累加器 Accumulator即累加器，与Mapreduce counter的应用场景差不多，都能很好地观察task在运行期间的数据变化 可以在Flink job任务中的算子函数中操作累加器，但是只能在任务执行结束之后才能获得累加器的最终结果。 Counters 计数器 Counter是一个具体的累加器(Accumulator)实现 IntCounter, LongCounter 和 DoubleCounter 用法： 创建累加器 private IntCounter numLines = new IntCounter(); 注册累加器 getRuntimeContext().addAccumulator(“num-lines”,this.numLines); 使用累加器 this.numLines.add(1); 获取累加器的结果 myJobExecutionResult.getAccumulatorResult(“num-lines”) 广播变量 Broadcast 与 累加变量 Accumlators的区别 Broadcast(广播变量)允许程序员将一个只读的变量缓存在每台机器上，而不用在任务之间传递变量。广播变量可以进行共享，但是不可以进行修改 Accumulators(累加器)是可以在不同任务中对同一个变量进行累加操作。 Flink Window和Time详解Window窗口 聚合事件例如计数、求和，在流上的工作方式与批处理不同 比如，对流中的所有元素进行计数是不可能的，因为通常流是无限的（无界的）。所以，流上的聚合需要由window 来划定范围，比如 “计算过去的5分钟” ，或者“最后100个元素的和” window是一种可以把无限数据切割为有限数据块的手段 窗口可以是 时间驱动的 【Time Window】（比如：每30秒）或者数据驱动的【Count Window】（比如：每100个元素） widow窗口的类型 tumbling windows：滚动窗口 【没有重叠】 sliding windows：滑动窗口 【有重叠】 widow窗口的应用 TimeWindow的应用 Count window 自定义window window聚合分类 增量聚合 在窗口中每进一条数据就进行一次聚合计算 reduce(reduceFunction) aggregate(aggregateFunction) sum(),min(),max() 实现过程 全量聚合 等窗口中的所有数据到齐以后才进行聚合计算，可以实现对窗口内所有数据的排序等需求 apply(windowFunction) process(processWindowFunction) 提供了更多上下文的信息 Time介绍 针对stream中的时间，可以分为以下三种 Event Time：事件产生的时间，通常由事件中的时间戳来描述 Ingestion Time: 事件进入Flink的时间 Processing time ： 时间倍处理时当前系统时间 Time实例分析 原始日志 2018-10-10 10:00:01,134 INFO executor.Executor: Finished task in state 0.0 数据进入Flink的时间是：2018-10-10 20:00:00,102 数据到达window进行处理的时间是：2018-10-10 20:00:01,100 如果我们想要统计每分钟内接口调用失败的错误日志个数，使用哪个时间才有意义？ Flink中，默认Time是ProcessingTime 可以在代码中设置 但是Processing time 并不是我们特别想要的，因为日志文件在传输过程中顺序可能已经发生了变化，而我们想要的是错误日志产生时的信息，就是Event Time，所以我们需要考虑一下Event Time的乱序问题 EventTime和Watermarks 在使用eventTime的时候如何处理乱序数据？ 我们知道，流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络延迟等原因，导致乱序的产生，特别是使用kafka的话，多个分区的数据无法保证有序。所以在进行window计算的时候，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark，watermark是用于处理乱序事件的。 watermark可以翻译为水位线 有序流与无序流图解 有序流 指定在那个时间点断开进行计算就可以直接断开进行计算 无序流 因为顺序被打乱 指定在某时间断开 其中的数据是错乱的无法进行立即聚合处理 watermarks的使用 watermarks的生成方式有两种 With Periodic Watermarks：周期性的触发watermark 的生成和发送 With Punctuated Watermarks：基于某些事件触发watermark 的生成和发送 第一种是我们常用的方法，所以就第一种来进行详细的分析 参考官方文档中With Periodic Watermarks的使用方法 代码中的extractTimestamp方法是从数据本身提取数据的Event time，getCurrentWatermar方法是获取当前水位线，利用currentMaxTimestamp - maxOutOfOrderness ，maxOutOfOrderness表示的是允许数据最大乱序时间 所以在这里我们需要使用的话就需要实现接口AssignerWithPeriodicWatermarks 实现watermark的相关代码 从socket模拟接收数据，然后使用map进行处理，后面在调用assignTimestampsAndWatermarks方法抽取timestamp并生成watermark。在调用window 打印信息来验证window被触发的时机 具体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package Flink;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;import org.apache.flink.streaming.api.functions.windowing.WindowFunction;import org.apache.flink.streaming.api.watermark.Watermark;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import javax.annotation.Nullable;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.Collections;import java.util.Iterator;import java.util.List;/** * Watermark 案例 */public class StreamingWindowWatermark &#123; public static void main(String[] args) throws Exception &#123; //定义 socket 的端口号 int port = 9001; // 获取运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置使用 eventtime，默认是使用 processtime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); //设置并行度为 1,默认并行度是当前机器的 cpu 数量 env.setParallelism(1); //连接 socket 获取输入的数据 DataStream&lt;String&gt; text = env.socketTextStream(\"hadoop100\", port, \"\\n\"); //解析输入的数据 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; inputMap = text.map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public Tuple2&lt;String, Long&gt; map(String value) throws Exception &#123; String[] arr = value.split(\",\"); return new Tuple2&lt;&gt;(arr[0], Long.parseLong(arr[1])); &#125; &#125;); //抽取 timestamp 和生成 watermark DataStream&lt;Tuple2&lt;String, Long&gt;&gt; waterMarkStream = inputMap.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; Long currentMaxTimestamp = 0L; // 最大允许的乱序时间是 10s final Long maxOutOfOrderness = 10000L; SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\"); /*** * 定义生成 watermark 的逻辑 * 默认 100ms 被调用一次 * */ @Nullable @Override public Watermark getCurrentWatermark() &#123; return new Watermark(currentMaxTimestamp - maxOutOfOrderness); &#125; //定义如何提取 timestamp @Override public long extractTimestamp(Tuple2&lt;String, Long&gt; element, long previousElementTimestamp) &#123; long timestamp = element.f1; currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp); System.out.println(\"key:\" + element.f0 + \",eventtime:[\" + element.f1 + \"|\" + sdf.format(element.f1) + \"], currentMaxTimestamp:[\" + currentMaxTimestamp + \"|\" + sdf.format(currentMaxTimestamp) + \"],watermark:[\" + getCurrentWatermark().getTimestamp() + \"|\" + sdf.format(getCurrentWatermark().getTimestamp()) + \"]\"); return timestamp; &#125; &#125;); //分组，聚合 DataStream&lt;String&gt; window = waterMarkStream.keyBy(0) .window(TumblingEventTimeWindows.of(Time.seconds(3)))// 按 照 消 息 的 EventTime 分配窗口，和调用 TimeWindow 效果一样 .apply(new WindowFunction&lt;Tuple2&lt;String, Long&gt;, String, Tuple, TimeWindow&gt;() &#123; /*** 对 window 内的数据进行排序，保证数据的顺序 * * @param tuple * @param window * @param input * @param out * @throws Exception * */ @Override public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out) throws Exception &#123; String key = tuple.toString(); List&lt;Long&gt; arrarList = new ArrayList&lt;Long&gt;(); Iterator&lt;Tuple2&lt;String, Long&gt;&gt; it = input.iterator(); while (it.hasNext()) &#123; Tuple2&lt;String, Long&gt; next = it.next(); arrarList.add(next.f1); &#125; Collections.sort(arrarList); SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\"); String result = key + \",\" + arrarList.size() + \",\" + sdf.format(arrarList.get(0)) + \",\" + sdf.format(arrarList.get(arrarList.size() - 1)) + \",\" + sdf.format(window.getStart()) + \",\" + sdf.format(window.getEnd()); out.collect(result); &#125; &#125;); //测试-把结果打印到控制台即可 window.print(); //注意：因为 flink 是懒加载的，所以必须调用 execute 方法，上面的代码才会执行 env.execute(\"eventtime-watermark\"); &#125;&#125; 程序简单解释 接收socket传来的数据 将每行数据按照逗号分隔，每行数据调用map 转换成tuple&lt;String,Long&gt;类型。其中tuple 中的第一个元素代表具体的数据，第二个元素代表数据的eventtime 抽取timestamp ， 生成watermar ， 允许的最大乱序时间是10s ， 并打印（key,eventtime,currentMaxTimestamp,watermark）等信息 分组聚合，window 窗口大小为3 秒，输出（key，窗口内元素个数，窗口内最早元素的时间，窗口内最晚元素的时间，窗口自身开始时间，窗口自身结束时间） 测试数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768测试数据-1如下：watermark+window处理乱序数据0001,1538359882000 2018-10-01 10:11:220001,1538359886000 2018-10-01 10:11:260001,1538359892000 2018-10-01 10:11:320001,1538359893000 2018-10-01 10:11:330001,1538359894000 2018-10-01 10:11:340001,1538359896000 2018-10-01 10:11:360001,1538359897000 2018-10-01 10:11:370001,1538359899000 2018-10-01 10:11:390001,1538359891000 2018-10-01 10:11:310001,1538359903000 2018-10-01 10:11:430001,1538359892000 2018-10-01 10:11:320001,1538359891000 2018-10-01 10:11:31测试数据-2如下：延迟数据被丢弃0001,1538359890000 2018-10-01 10:11:300001,1538359903000 2018-10-01 10:11:430001,1538359890000 2018-10-01 10:11:300001,1538359891000 2018-10-01 10:11:310001,1538359892000 2018-10-01 10:11:32测试数据-3如下：allowedLateness 0001,1538359890000 2018-10-01 10:11:300001,1538359903000 2018-10-01 10:11:430001,1538359890000 2018-10-01 10:11:300001,1538359891000 2018-10-01 10:11:310001,1538359892000 2018-10-01 10:11:320001,1538359904000 2018-10-01 10:11:440001,1538359890000 2018-10-01 10:11:300001,1538359891000 2018-10-01 10:11:310001,1538359892000 2018-10-01 10:11:320001,1538359905000 2018-10-01 10:11:450001,1538359890000 2018-10-01 10:11:300001,1538359891000 2018-10-01 10:11:310001,1538359892000 2018-10-01 10:11:32测试数据-4如下：sideOutputLateData 0001,1538359890000 2018-10-01 10:11:300001,1538359903000 2018-10-01 10:11:430001,1538359890000 2018-10-01 10:11:300001,1538359891000 2018-10-01 10:11:310001,1538359892000 2018-10-01 10:11:32测试数据-5如下：多并行度下的watermark-80001,1538359882000 2018-10-01 10:11:220001,1538359886000 2018-10-01 10:11:260001,1538359892000 2018-10-01 10:11:320001,1538359893000 2018-10-01 10:11:330001,1538359894000 2018-10-01 10:11:340001,1538359896000 2018-10-01 10:11:360001,1538359897000 2018-10-01 10:11:37测试数据-6如下：0001,1538359890000 2018-10-01 10:11:300001,1538359903000 2018-10-01 10:11:430001,1538359908000 2018-10-01 10:11:48 注意：多并行度的情况下，watermark对齐会取所有channel最小的watermark watermarks的生成方式 With Periodic Watermarks 周期性的触发watermark的生成和发送，默认是100ms 每隔N秒自动向流里注入一个WATERMARK 时间间隔由ExecutionConfig.setAutoWatermarkInterval 决定. 每次调用getCurrentWatermark 方法, 如果得到的WATERMARK 不为空并且比之前的大就注入流中 可以定义一个最大允许乱序的时间，这种比较常用 实现AssignerWithPeriodicWatermarks接口 With Punctuated Watermarks 基于某些事件触发watermark的生成和发送 基于事件向流里注入一个WATERMARK，每一个元素都有机会判断是否生成一个WATERMARK. 如果得到的WATERMARK 不为空并且比之前的大就注入流中 实现AssignerWithPunctuatedWatermarks接口 watermark 与 event time结合使用 触发widow进行合并的条件是 watermark &gt;= window_end_time 并且当前窗口内有数据 这样就可以在允许最大乱序时间内将同一个窗口的数据进行处理，如果数据超过了这个最大允许乱序时间，要怎么解决呢 late element 延迟数据的处理方案 丢弃 系统默认的方法 直接将超过允许最大乱序时间的数据丢弃，不做任何处理 allowedLateness 指定允许数据延迟时间 再给迟到的数据一个提供一个宽容时间 加上这个时间以后 在超过最大允许乱序时间以后 在宽容时间内 如果数据出现了 依然可以出发widow执行。 sideOutputLateDate 收集迟到的数据 通过sideOutputLateDate将迟到的数据进行统一收集进行存储，方便 以后的问题排查处理 Flink应该如何设置最大乱序时间 这个要结合自己的业务以及数据情况去设置。如果maxOutOfOrderness设置的太小，而自身数据发送时由于网络等原因导致乱序或者late太多，那么最终的结果就是会有很多单条的数据在window中被触发，数据的正确性影响太大 对于严重乱序的数据，需要严格统计数据最大延迟时间，才能保证计算的数据准确，延时设置太小会影响数据准确性，延时设置太大不仅影响数据的实时性，更加会加重Flink作业的负担，不是对eventTime要求特别严格的数据，尽量不要采用eventTime方式来处理，会有丢数据的风险。 Flink 并行度详解(Parallel )TaskManager 与 Slot Flink的每个TaskManager为集群提供solt。 solt的数量通常与每个TaskManager节点的可用CPU内核数成比例。一般情况下你的slot数是你每个节点的cpu的核数。 并行度（Parallel） 一个Flink程序由多个任务组成(source、transformation和 sink)。 一个任务由多个并行的实例(线程)来执行， 一个任务的并行实例(线程)数目就被称为该任务的并行度。 一个任务的并行度设置可以从多个层次指定 Operator Level（算子层次） Execution Environment Level（执行环境层次） Client Level（客户端层次） System Level（系统层次） 并行度设置之Operator Level 一个算子、数据源和sink的并行度可以通过调用 setParallelism()方法来指定 并行度设置之Execution Environment Level 执行环境(任务)的默认并行度可以通过调用setParallelism()方法指定。为了以并行度3来执行所有的算子、数据源和data sink， 可以通过如下的方式设置执行环境的并行度 执行环境的并行度可以被算子的并行度覆盖重写 并行度设置之Client Level 并行度可以在客户端将job提交到Flink时设定。 对于CLI客户端，可以通过-p参数指定并行度 ./bin/flink run -p 10 WordCount-java.jar 并行度设置之System Level 在系统级可以通过设置flink-conf.yaml文件中的parallelism.default属性来指定所有执行环境的默认并行度 Flink的checkpoint机制checkpoint简介 为了保证state的容错性，Flink需要对state进行checkpoint Checkpoint是Flink实现容错机制最核心的功能，它能够根据配置周期性地基于Stream中各个Operator/task的状态来生成快照，从而将这些状态数据定期持久化存储下来，当Flink程序一旦意外崩溃时，重新运行程序时可以有选择地从这些快照进行恢复，从而修正因为故障带来的程序数据异常 Flink的checkpoint机制可以与(stream和state)的持久化存储交互的前提： 持久化的source，它需要支持在一定时间内重放事件。这种sources的典型例子是持久化的消息队列（比如Apache Kafka，RabbitMQ等）或文件系统（比如HDFS，S3，GFS等） 用于state的持久化存储，例如分布式文件系统（比如HDFS，S3，GFS等） checkpoint的配置 默认的情况下 checkpoint是disabled不可用状态，想要使用的时候先开启 checkpoint的checkPointMode有两种，Exactly-once(默认)和At-least-once Exactly-once对于大多数应用来说是最合适的。At-least-once可能用在某些延迟超低的应用程序（始终延迟为几毫秒） 1234567891011121314StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 每隔1000 ms进行启动一个检查点【设置checkpoint的周期】env.enableCheckpointing(1000);// 高级选项：// 设置模式为exactly-once （这是默认值）env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// 确保检查点之间有至少500 ms的间隔【checkpoint最小间隔】env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// 检查点必须在一分钟内完成，或者被丢弃【checkpoint的超时时间】env.getCheckpointConfig().setCheckpointTimeout(60000);// 同一时间只允许进行一个检查点env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// 表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpointenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); state Backend 状态的后端存储 默认情况下，state会保存在taskmanager的内存中，checkpoint会存储在JobManager的内存中。 state 和checkpoint的存储位置取决于State Backend的配置 可以在程序中修改 env.setStateBackend(…) 三种state Backend MemoryStateBackend state数据保存在java堆内存中，执行checkpoint的时候，会把state的快照数据保存到jobmanager的内存中 基于内存的state backend在生产环境下不建议使用 FsStateBackend state数据保存在taskmanager的内存中，执行checkpoint的时候，会把state的快照数据保存到配置的文件系统中 可以使用hdfs等分布式文件系统 RocksDBStateBackend RocksDB跟上面的都略有不同，它会在本地文件系统中维护状态，state会直接写入本地rocksdb中。同时它需要配置一个远端的filesystem uri（一般是HDFS），在做checkpoint的时候，会把本地的数据直接复制到filesystem中。fail over的时候从filesystem中恢复到本地 RocksDB克服了state受内存限制的缺点，同时又能够持久化到远端文件系统中，比较适合在生产中使用 修改state Backend 单任务调整 修改当前任务代码 123env.setStateBackend(new FsStateBackend(&quot;hdfs://namenode:9000/flink/checkpoints&quot;));or new MemoryStateBackend()or new RocksDBStateBackend(filebackend, true);【需要添加第三方依赖】 全局调整 修改flink-conf.yaml 123state.backend: filesystemstate.checkpoints.dir: hdfs://namenode:9000/flink/checkpoints注意：state.backend的值可以是下面几种：jobmanager(MemoryStateBackend), filesystem(FsStateBackend), rocksdb(RocksDBStateBackend) Flink Kafka-Connector详解kafka-connector简介 Kafka中的partition机制和Flink的并行度机制深度结合 Kafka可以作为Flink的source和sink 任务失败，通过设置kafka的offset来恢复应用 kafka-consumer消费者策略 setStartFromGroupOffsets()【默认消费策略】 默认读取上次保存的offset信息 如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据 setStartFromEarliest() 从最早的数据开始进行消费，忽略存储的offset信息 setStartFromLatest() 从最新的数据进行消费，忽略存储的offset信息 setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition,Long&gt;) kafka的容错 当checkpoint机制开启的时候，Kafka Consumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。 为了能够使用支持容错的kafka Consumer，需要开启checkpoint env.enableCheckpointing(5000); // 每5s checkpoint一次 动态加载topic kafka consumer offset 自动提交 针对job是否开启checkpoint来区分 Checkpoint关闭时： 可以通过下面两个参数配置 enable.auto.commit auto.commit.interval.ms Checkpoint开启时：当执行checkpoint的时候才会保存offset，这样保证了kafka的offset和checkpoint的状态偏移量保持一致。 可以通过这个参数设置setCommitOffsetsOnCheckpoints(boolean)这个参数默认就是true。表示在checkpoint的时候提交offset 此时，kafka中的自动提交机制就会被忽略 Kafka Producer kafka producer 的容错 kafka0.9 与 0.10 如果Flink开启了checkpoint，针对FlinkKafkaProducer09和FlinkKafkaProducer010 可以提供 at-least-once的语义，还需要配置下面两个参数 setLogFailuresOnly(false) setFlushOnCheckpoint(true) 注意：建议修改kafka 生产者的重试次数 retries【这个参数的值默认是0】 Kafka Producer的容错-Kafka 0.11 如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供exactly-once的语义 但是需要选择具体的语义 Semantic.NONE Semantic.AT_LEAST_ONCE【默认】 Semantic.EXACTLY_ONCE","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://yoursite.com/tags/Flink/"}]},{"title":"Flink入门简介","slug":"2018-06-20-Flink入门简介","date":"2018-06-19T16:00:00.000Z","updated":"2019-09-05T07:38:15.197Z","comments":true,"path":"2018/06/20/2018-06-20-Flink入门简介/","link":"","permalink":"http://yoursite.com/2018/06/20/2018-06-20-Flink入门简介/","excerpt":"Flink入门简介Flink的基本原理以及应用场景","text":"Flink入门简介Flink的基本原理以及应用场景 Flink的简介 Flink是一个开源的分布式、高性能、高可用、准确的流处理框架。 支持实时流处理以及实时批处理，批处理其实就是流处理的一个特例 原生支持迭代计算、内存管理、程序优化等 Flink的架构图： Flink的基本组件： Flink的流处理与批处理 在大数据领域中，批处理任务以及流处理任务一般被认为是两种不同的任务，一个大数据的框架只能处理其中的一种任务 例如：Storm只支持流处理，spark、MapReduce只支持批处理，spark Streaming也是采用了一种micro-batch的架构，即把输入的数据流切分成细粒度的batch，并且为每一个batch提交一个批处理的spark任务，所以spark Streaming的流处理与storm的完全不同。 Flink通过灵活的执行引擎，能够同时支持批处理任务以及流处理任务。 在执行引擎这一层，批处理与流处理的最大的区别在于节点之间数据的传输方式。 流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理 批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点 这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求 Flink以固定的缓存块为单位进行网络数据传输，用户可以通过设置缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则Flink的数据传输方式类似上文所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟 如果缓存块的超时值为无限大，则Flink的数据传输方式类似上文所提到批处理系统的标准模型，此时系统可以获得最高的吞吐量 同时缓存块的超时值也可以设置为0到无限大之间的任意值。缓存块的超时阈值越小，则Flink流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，反之亦然。通过调整缓存块的超时阈值，用户可根据需求灵活地权衡系统延迟和吞吐量 Flink的应用场景 优化电商网站的实时搜索结果 阿里巴巴的所有基础设施团队使用flink实时更新产品细节和库存信息(Blink) 针对数据分析团队提供实时流数据处理服务 通过flink数据分析平台提供实时数据分析服务，及时发现问题 网络/传感器检测和错误检测 Bouygues电信公司，是法国最大的电信供应商之一，使用flink监控其有线和无线网络，实现快速故障响应 商业智能分析ETL Zalando使用flink转换数据以便于加载到数据仓库，将复杂的转换操作转化为相对简单的并确保分析终端用户可以更快的访问数据(实时ETL) Flink与其他框架的对比 Flink与Storm以及Spark Streaming的对比： 实时处理框架的选择 需要关注流数据是否需要进行状态管理 At-least-once或者Exectly-once消息投递模式是否有特殊要求 对于小型独立的项目，并且需要低延迟的场景，建议使用storm 如果你的项目已经使用了spark，并且秒级别的实时处理可以满足需求的话，建议使用sparkStreaming 要求消息投递语义为Exactly Once 的场景；数据量较大，要求高吞吐低延迟的场景；需要进行状态管理或窗口统计的场景，建议使用flink Flink的入门案例 需求：手工通过socket实时产生一些单词，使用flink实时接收数据，对指定时间窗口内(例如：2秒)的数据进行聚合统计，并且把时间窗口内计算的结果打印出来 代码实现：可以使用Java或者Scala 这里推荐使用Scala并使用Scala演示 1234567891011添加Scala连Flink的核心依赖包&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334package FlinkTestimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.streaming.api.windowing.time.Time/** * 实时计算 * 手工通过socket实时产生一些单词，使用flink实时接收数据， * 对指定时间窗口内(例如：2秒)的数据进行聚合统计，并且把时间窗口内计算的结果打印出来 */object WordCount &#123; def main(args: Array[String]): Unit = &#123; //获取执行环境 val env = StreamExecutionEnvironment.getExecutionEnvironment //指定source 获取数据源 val text = env.socketTextStream(\"hadoop110\",9999,'\\n') //使用flatmap的时候需要使用隐式转换 添加隐式转换 import org.apache.flink.api.scala._ //对数据切分 val words = text.flatMap(_.split(\" \")) //转换单词格式 val wordsCount = words.map((_,1)) //吧相同的key放在一起 val keys = wordsCount.keyBy(0) //指定一个基于时间的滑动窗口 每隔一秒计算前两秒的数据 val windowData = keys.timeWindow(Time.seconds(2),Time.seconds(1)) //根据tuple中的数据求和 val res = windowData.sum(1) //打印结果 设置并行度为1 res.print().setParallelism(1) //执行代码 env.execute(\"WordCount\") &#125;&#125; 12345678910111213141516171819202122package FlinkTestimport org.apache.flink.api.scala.ExecutionEnvironmentobject BatchWordCount &#123; def main(args: Array[String]): Unit = &#123; val env = ExecutionEnvironment.getExecutionEnvironment val data = env.readTextFile(\"D:\\\\data\\\\words.txt\") import org.apache.flink.api.scala._ val res = data.flatMap(_.split(\" \")) .map((_,1)) .groupBy(0) .sum(1) res.print() res.writeAsCsv(\"D:\\\\data\\\\words.csv\",\"\\n\",\" \") env.execute(\"BatchWordCount\") &#125;&#125; Flink的集群介绍简单本地集群安装部署 依赖环境：Linux，jdk1.8以上 下载地址：https://archive.apache.org/dist/flink 选择合适的版本 local模式快速安装启动 解压 进入 启动 ./bin/start-cluster.sh 进入web界面查看 8081 可以将任务提交到进群之中尝试运行 集群搭建中的重点 Flink-Standalone集群 各个参数详解 jobmanager.heap.mb：jobmanager节点可用的内存大小 taskmanager.heap.mb：taskmanager节点可用的内存大小 taskmanager.numberOfTaskSlots：每台机器可用的cpu数量 parallelism.default：默认情况下任务的并行度 taskmanager.tmp.dirs：taskmanager的临时数据存储目录 slot和parallelism总结 slot是静态的概念，是指taskmanager具有的并发执行能力 parallelism是动态的概念，是指程序运行时实际使用的并发能力 设置合适的parallelism能提高运算效率，太多了和太少了都不行 容错 jobmanager挂掉 正在执行的任务会失败 存在单点故障 但是Flink支持HA taskmanager挂掉 如果有多余的taskmanager节点，flink会自动把任务调度到其它节点执行 Flink on Yarn 使用on yarn 的好处 提高集群的利用率 一套集群，可以执行mr任务，spark任务，Flink任务 内部实现结构 Flink on Yarn有两种方式 第一种运行方式 yarn-session.sh(开辟资源)+flink run(提交任务) 启动一个一直运行的flink集群 bin/yarn-session.sh -n 2 -jm 1024 -tm 1024 附着到一个已存在的flink yarn session bin/yarn-session.sh -id application_1463870264508_0029 执行任务 bin/flink run ./examples/batch/WordCount.jar 第二种运行方式 flink run -m yarn-cluster(开辟资源+提交任务) 启动集群，执行任务 bin/flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 ./examples/batch/WordCount.jar 注意：client端必须要设置YARN_CONF_DIR或者HADOOP_CONF_DIR或者HADOOP_HOME环境变量，通过这个环境变量来读取YARN和HDFS的配置信息，否则启动会失败 参数解释 -n,–container 分配多少个yarn容器(=taskmanager的数量) 必选 -D 动态属性 -d,–detached 独立运行 -jm,–jobManagerMemory JobManager的内存 [in MB] -nm,–name 在YARN上为一个自定义的应用设置一个名字 -q,–query 显示yarn中可用的资源 (内存,cpu核数) -qu,–queue 指定YARN队列 -s,–slots 每个TaskManager使用的slots数量 -tm,–taskManagerMemory 每个TaskManager的内存 [in MB] -z,–zookeeperNamespace 针对HA模式在zookeeper上创建NameSpace id,–applicationId YARN集群上的任务id，附着到一个后台运行的yarn session中 bin/flink run 命令分析 run [OPTIONS] “run”操作参数: -c,–class 如果没有在jar包中指定入口类，则需要在这里通过这个参数指定 需要放在jar包前面 -m,–jobmanager host:port 指定需要连接的jobmanager(主节点)地址，使用这个参数可以指定一个不同于配置文件中的jobmanager -p,–parallelism 指定程序的并行度。可以覆盖配置文件中的默认值 默认查找当前yarn集群中已有的yarn-session信息中的jobmanager bin/flink run ./examples/batch/WordCount.jar 连接指定host和port的jobmanager bin/flink run -m hadoop100:1234 ./examples/batch/WordCount.jar 启动一个新的yarn-session bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar yarn session命令行的选项也可以使用./bin/flink 工具获得。它们都有一个y或者yarn的前缀 Flink的HA高可用 jobManage协调每一个Flink任务的部署，他负责调度和资源管理。 默认情况下，一个Flink集群中只有一个jobmanege,这样就很容易出现单点故障而导致不能提交新的任务并且现在执行的任务也会失败 使用jobManage HA 集群可以快速从jobManage故障中恢复，避免单点故障spof,可以再standalone或者yarn集群模式下配置集群的高可用机制 Standolone高可用详解 Standalone模式（独立模式）下JobManager的高可用性的基本思想是，任何时候都有一个Master JobManager ，并且多个Standby JobManagers 。 Standby JobManagers可以在Master JobManager 挂掉的情况下接管集群成为Master JobManager。 这样保证了没有单点故障，一旦某一个Standby JobManager接管集群，程序就可以继续运行。 Standby JobManager和Master JobManager实例之间没有明确区别。每个JobManager都可以成为Master或Standby节点 Yarn高可用详解 主要是利用yarn自己的job恢复机制 Flink 的 Scala shell代码调试 因为每次打jar包进行测试有些麻烦，并且不好定位问题，所以可以再Scala shell中进行调试 scala shell方式支持流处理和批处理。当启动shell命令行之后，两个不同的ExecutionEnvironments会被自动创建。使用senv(Stream)和benv(Batch)分别去处理流处理和批处理程序。(类似于spark-shell中sc变量) 启动指令bin/start-scala-shell.sh","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://yoursite.com/tags/Flink/"}]},{"title":"ELK的简单部署以及使用","slug":"2018-06-18-ELK简单部署以及使用","date":"2018-06-18T05:46:56.000Z","updated":"2019-09-05T08:06:26.486Z","comments":true,"path":"2018/06/18/2018-06-18-ELK简单部署以及使用/","link":"","permalink":"http://yoursite.com/2018/06/18/2018-06-18-ELK简单部署以及使用/","excerpt":"ELK简单部署以及使用简介","text":"ELK简单部署以及使用简介 此项目是使用filebeat轻量化日志采集工具，将日志采集到kafka，在使用logstash工具将日志采集到Elasticsearch中，使用kibana工具在web界面上进行各种搜索查看建立图标等操作。默认kafka以及Elasticsearch已经装好了。 下载安装 进入到官网https://www.elastic.co 找到产品 点击下载 选择需要下载的工具点击下载 在past releases中可以选择其他版本点击下载就可以 下载好安装包以后进行解压、配置环境变量 修改配置文件 filebeat变量配置 首先进入到filebeat解压后的文件中 1cd filebeat-6.4.3-linux-x86_64 修改filebeat.yml中的配置 1vi filebeat.yml 修改成以下值 1234567891011121314filebeat.inputs:- type: log enabled: true paths: - /data/filebeattest/logs/*.log //需要采集的日志地址output.kafka: hosts: [&quot;hadoop110:9092&quot;] //kafka的地址 如果使用hadoop这种类型的名字需要在本节点上配置 hosts文件 topic: &apos;filelog&apos; //kafka中的topic partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 启动filebeat的指令 1filebeat -c filebeat.yml logstash变量配置 进入到logstash的解压文件中 1cd logstash-6.4.3 创建一个配置文件的文件夹conf 并在文件夹里创建一个配置文件kafka-elasticsearch.conf 123mkdir confcd conftouch kafka-elasticsearch.conf 给配置文件加上配置信息 1234567891011121314vi kafka-elasticsearch.confinput &#123; kafka &#123; topics =&gt; [&quot;filelog&quot;] bootstrap_servers =&gt; &quot;hadoop110:9092&quot; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;hadoop110:9200&quot;] &#125;&#125; 启动logstash 1logstash -f conf/kafka-elasticsearch.conf 配置kibana 进入到kibana的解压文件中 1cd kibana-6.4.3-linux-x86_64 修改一些需要的变量配置 12345cd confogvi kibana.ymlserver.port: 5601elasticsearch.url: &quot;http://hadoop110:9200&quot;根据实际需要修改 启动kibana 1bin/kibana 启动检查效果 进入到kibana的web界面查看效果 看到我们创建的索引已经存在了就说明我们的真个过程已经成功了","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://yoursite.com/tags/ELK/"}]},{"title":"Elasticsearch问题以及调优","slug":"2018-06-15-Elasticsearch问题以及调优","date":"2018-06-14T16:00:00.000Z","updated":"2019-09-05T08:06:17.838Z","comments":true,"path":"2018/06/15/2018-06-15-Elasticsearch问题以及调优/","link":"","permalink":"http://yoursite.com/2018/06/15/2018-06-15-Elasticsearch问题以及调优/","excerpt":"Elasticsearch问题以及调优Elasticsearch脑裂问题分析","text":"Elasticsearch问题以及调优Elasticsearch脑裂问题分析 脑裂问题的图解 脑裂问题就是在集群环境之中，由于节点之间的通信问题导致节点对集群的状态理解不同，导致es有些查询非常缓慢甚至查询失败 最好的解决办法就是重启集群，详细的问题分析可以查看我的博客《Elasticsearch脑裂问题详细分析及解决方案》 Elasticsearch索引模板以及索引名索引模板index template 在我们的工作中，针对一个大批量的数据存储时需要使用多个索引库，如果我们手工去为每个索引库配置信息就很麻烦，所以就有了索引模板，创建一个模板，制定好配置信息，如果我们闯进的索引库匹配到了模板就会使用模板中的配置信息 创建模板 1234567891011121314curl -H &quot;Content-Type: application/json&quot; -XPUT localhost:9200/_template/template_1 -d &apos;&#123; &quot;template&quot; : &quot;*&quot;, &quot;order&quot; : 0, &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 1 &#125;, &quot;mappings&quot; : &#123; &quot;type1&quot; : &#123; &quot;_source&quot; : &#123; &quot;enabled&quot; : false &#125; &#125; &#125;&#125;其中的order值是用来进行模板之间的优先级排序的，如果一个索引匹配到多个模板则比较模板的order值，选取最大order值的模板进行配置信息匹配 查看模板信息 curl -XGET localhost:9200/_template/temp*?pretty 删除模板 curl -XDELETE localhost:9200/_template/temp_1 索引别名index alias 索引别名就是为索引起一个或者多个别名方便引用 公司使用es收集应用的日志，每个星期创建一个索引库，这样时间长了就会创建很多个索引库，操作和管理非常不方便 由于新增的索引只会操作最新的这一周的索引库，所以我们就可以创建两个别名 curr_week :此别名执行这个星期的索引库，新增的数据操作这个索引库 last_3_month:这个别名指向的是近三个月的索引库，因为我们需要查询近三个月的数据 后期只需要修改两个别名与索引库的指向关系即可，应用层的代码不许要更新、 还需要将三个月之前的索引库close掉，将一年前的索引库删除 es默认对查询分片的数量是有限制的，默认是1000个，使用通配符查询多个索引库的时候会出问题，正好可以使用别名解决 增加索引别名 可同时增减多个 12345678curl -H &quot;Content-Type: application/json&quot; -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;, &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test2&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125;&apos; 删除索引别名 123456curl -H &quot;Content-Type: application/json&quot; -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;&#123; &quot;actions&quot; : [ &#123; &quot;remove&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125;&apos; Elasticsearch参数调优 解决es启动警告信息 123456789vi /etc/security/limits.conf增加以下内容* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096vi /etc/security/limits.d/90-nproc.conf 把1024修改为4096* soft nproc 4096 修改配置文件调整es的jvm内存大小 修改bin/elasticsearch.in.sh中ES_MIN_MEM和ES_MAX_MEM的大小，建议设置一样大，避免频繁的分配内存，根据服务器内存大小，一般分配60%左右(默认256M) 注意内存最大不要超过32G 一旦你越过这个神奇的32GB边界，指针会切换回普通对象指针.。每个指针的大小增加，使用更多的CPU内存带宽。事实上，你使用40~50G的内存和使用32G的内存效果是一样的。 设置memory_lock来锁定进程的物理内存地址 避免交换（swapped）来提高性能 修改文件conf/elasticsearch.yml bootstrap.memory_lock: true 需要根据es启动日志修改/etc/security/limits.conf文件(重启系统) 改变分片的数量 分片多的话，可以提升建立索引的能力，5-20个比较合适。 如果分片的数量过多或者过少都会导致检索比较慢 分片过多会导致查询的额时候打开较多的文件，而分片数过少会导至单个分片索引过大，所以检索速度也会慢。 建议单个分片存储20G左右的索引数据【最高也不要超过50G，否则性能会很差】，所以，分片数量=数据总量/20G 副本多的话，可以提升搜索的能力，但是如果设置很多副本的话也会对服务器造成额外的压力，因为主分片需要给所有副本同步数据。所以建议最多设置1-2个即可。 针对不使用的index，建议close，减少内存占用。因为只要索引处于open状态，索引库中的segement就会占用内存，close之后就只会占用磁盘空间了。 curl -XPOST ‘localhost:9200/test/_close’ 要定时对索引进行合并优化，不然segment越多，占用的segment memory越多，查询的性能也越差 索引量不是很大的话可以将segment设为1 在es2.1.0以前调用_optimize接口，后期改为_forcemerge接口 123curl -XPOST &apos;http://localhost:9200/test/_forcemerge?max_num_segments=1&apos;client.admin().indices().prepareForceMerge(&quot;test&quot;).setMaxNumSegments(1).get();注意：索引合并是针对分片的。segment设置为1，则每个分片都有一个索引片段。 删除文档：在es中删除文档，数据不会马上在硬盘上除去，而是在es索引中产生一个.del的文件，而在检索过程中这部分数据也会参与检索，es在检索过程会判断是否删除了，如果删除了在过滤掉。这样也会降低检索效率。所以可以执行清除删除文档 1234shell:curl -XPOST &apos;http://localhost:9200/test/_forcemerge?only_expunge_deletes=true&apos;java:client.admin().indices().prepareForceMerge(&quot;test&quot;).setOnlyExpungeDeletes(true).get(); 如果在项目开始的时候需要批量入库大量数据的话，建议将副本数设置为0 因为es在索引数据的时候，如果有副本存在，数据也会马上同步到副本中，这样会对es增加压力。可以等索引完成后将副本按需要改回来。这样可以提高索引效率 Elasticsearch在建立索引时，根据id或(id,类型)进行hash，得到hash值之后再与该索引的分片数量取模，取模的值即为存入的分片编号 可以指定把数据存储到某一个分片中，通过routing参数 可以显著提高性能 12curl -XPOST &apos;localhost:9200/yehua/emp?routing=rout_param&apos; -d &apos;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:20&#125;&apos;routing(路由参数)","categories":[{"name":"调优","slug":"调优","permalink":"http://yoursite.com/categories/调优/"}],"tags":[{"name":"Elasticsearch调优","slug":"Elasticsearch调优","permalink":"http://yoursite.com/tags/Elasticsearch调优/"}]},{"title":"Elasticsearch脑裂问题详细分析以及解决方案","slug":"2018-06-14-Elasticsearch脑裂问题详细分析以及解决方案","date":"2018-06-13T16:00:00.000Z","updated":"2019-09-05T08:05:45.725Z","comments":true,"path":"2018/06/14/2018-06-14-Elasticsearch脑裂问题详细分析以及解决方案/","link":"","permalink":"http://yoursite.com/2018/06/14/2018-06-14-Elasticsearch脑裂问题详细分析以及解决方案/","excerpt":"Elasticsearch脑裂问题详细分析以及解决方案","text":"Elasticsearch脑裂问题详细分析以及解决方案 什么是脑裂问题 脑裂问题其实就是同一个集群的不同节点对于整个几位群的状态有不同的理解，导致操作错乱，类似于精神分裂 怎么发现集群产生脑裂问题 Elasticsearch出现查询非常缓慢的情况 通过命令查看集群的状态 curl -XGET ‘http://localhost:9200/_cluster/health&#39; 发现集群状态为red，且集群数量明显错误，再向不同的节点查询集群状态的时候，总体状态都是red，但是返回的集群数量却不太一样 正常情况下，访问每一个节点，对集群中的状态返回应该是一致的。不一致的信息表示集群中不同节点对master节点的选择出现了问题。导致集群不能正常工作 产生脑裂问题的原因 网络 由于某些节点之间的网络通信出现问题，导致一些节点认为master节点已经挂了，所以有重新选举了新的master节点，从而导致集群信息混乱，可以检查Ganglia集群监控，来查看是否是网络原因 节点负载过大：由于master节点与data节点都是混在一起的，有可能master节点的负载过大，导致对应的es实例停止响应，这时一部分节点会一位master节点已经挂掉从而重新选举，导致多master节点运行。同时由于data节点上ES进程占用的内存较大，较大规模的内存回收操作也能造成ES进程失去响应。所以，这个原因的可能性应该是最大的。 如何解决脑裂问题 对于网络问题，只能进行网络修复，在重启集群 对于负载的问题 一个直观的解决方案就是将master节点与data节点分离，准备几台机器加入集群中，这几台机器只能充当master节点，不可担任存储和搜索的角色 配置信息 12345node.master: truenode.data: false其他节点 只能充当data不能充当masternode.master: falsenode.data: true 还有两个参数的修改可以减少脑裂问题的出现 discovery.zen.ping_timeout（默认值是3秒）：默认情况下，一个节点会认为，如果master节点在3秒之内没有应答，那么这个节点就是死掉了，而增加这个值，会增加节点等待响应的时间，从一定程度上会减少误判。 discovery.zen.minimum_master_nodes（默认是1）：这个参数控制的是，一个节点需要看到的具有master节点资格的最小数量，然后才能在集群中做操作。官方的推荐值是(N/2)+1，其中N是具有master资格的节点的数量 如果脑裂问题已经发生该如何解决 当脑裂发生后，唯一的修复办法是解决这个问题并重启集群。 当elasticsearch集群启动时，会选出一个主节点（一般是启动的第一个节点被选为主）。由于索引的两份拷贝已经不一样了，elasticsearch会认为选出来的主保留的分片是“主拷贝”并将这份拷贝推送给集群中的其他节点。这很严重。让我们设想下你是用的是node客户端并且一个节点保留了索引中的正确数据。但如果是另外的一个节点先启动并被选为主，它会将一份过期的索引数据推送给另一个节点，覆盖它，导致丢失了有效数据。 所以怎么从脑裂中恢复？第一个建议是给所有数据重新索引。第二，如果脑裂发生了，要十分小心的重启你的集群。停掉所有节点并决定哪一个节点第一个启动。 如果需要，单独启动每个节点并分析它保存的数据。如果不是有效的，关掉它，并删除它数据目录的内容（删前先做个备份）。如果你找到了你想要保存数据的节点，启动它并且检查日志确保它被选为主节点。这之后你可以安全的启动你集群里的其他节点了。","categories":[{"name":"问题分析","slug":"问题分析","permalink":"http://yoursite.com/categories/问题分析/"}],"tags":[{"name":"Elasticsearch脑裂问题","slug":"Elasticsearch脑裂问题","permalink":"http://yoursite.com/tags/Elasticsearch脑裂问题/"}]},{"title":"Elasticsearch高级","slug":"2018-06-12-Elasticsearch高级","date":"2018-06-11T16:00:00.000Z","updated":"2019-09-05T08:05:27.357Z","comments":true,"path":"2018/06/12/2018-06-12-Elasticsearch高级/","link":"","permalink":"http://yoursite.com/2018/06/12/2018-06-12-Elasticsearch高级/","excerpt":"Elasticsearch高级二Elasticsearch查询详解查询Query","text":"Elasticsearch高级二Elasticsearch查询详解查询Query 代码 1234567891011121314151617181920212223242526public class EsDemo2 &#123; static String index = &quot;test&quot;; static String type = &quot;emp&quot;; public static void main(String[] args) throws Exception&#123; Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;) .put(&quot;client.transport.sniff&quot;, true) .build(); TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300)); testSearch(client); &#125; public static void testSearch(TransportClient client)&#123; SearchResponse searchResponse = client.prepareSearch(index) .setQuery(QueryBuilders.&lt; !-- matchAllQuery() 具体的查询方法 -- &gt;) .get(); SearchHits hits = searchResponse.getHits(); //获取总条数 long totalHits = hits.getTotalHits(); System.out.println(&quot;数据的总条数&quot;+totalHits); //打印所有数据内容 SearchHit[] hits1 = hits.getHits(); for (SearchHit hit:hits1) &#123; System.out.println(hit.getSourceAsString()); &#125; &#125; matchAllQuery() 查询所有数据 12345678结果：数据的总条数6&#123;&quot;name&quot;:&quot;jessic&quot;,&quot;age&quot;:18&#125;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:19&#125; matchQuery((“name”,”zs”)) 根据指定列进行模糊查询 不支持通配符 123数据的总条数2&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125; multiMatchQuery(“zs”,”name”,”city”) 在多个列中进行模糊查询 查询的列不存在不会报错 123数据的总条数2&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125; queryStringQuery(“name:z*”) Lucene提供的方法支持对某一列查询的时候使用通配符 123数据的总条数2&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125; boolQuery() .should(QueryBuilders.matchQuery(“name”,”zs”).boost(10.0f)) .should(QueryBuilders.matchQuery(“age”,19).boost(1.0f)) 根据不同的条件进行多次查询 可以根据boost值来设置两条语句的结果先后顺序 12345数据的总条数4&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:19&#125; termQuery(“name”,”abc xyz”) 查询的时候不会进行分词的精确查询 1数据的总条数0 在默认情况之下es会将所有词进行分词索引，这是你试试用及精确查询时查不到的 解决方案 在创建索引的时候将不需要进行分词的特殊索引指定不分词 根据Lucene提供的方法直接查询可以对已经分词的索引进行精确查询 queryStringQuery(“name:&quot;abc xyz&quot;“) 12数据的总条数1&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125; matchQuery(“name”,”abc xyz”).operator(Operator.AND) 也可以对已经分词的索引进行精确查询 12数据的总条数1&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125; 其他查询 from、size 分页 12345.setFrom(2).setSize(3)&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125; sort 排序 123456789.setQuery(QueryBuilders.matchAllQuery()) //取出所有数据.addSort(&quot;age&quot;, SortOrder.DESC) //按照年龄降序排序&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:19&#125;&#123;&quot;name&quot;:&quot;jessic&quot;,&quot;age&quot;:18&#125;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:16&#125;&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125; filter 过滤 1234567.setPostFilter(QueryBuilders.rangeQuery(\"age\").from(\"16\").to(20)) 过滤出来年龄在16到20之间的&#123;\"name\":\"jessic\",\"age\":18&#125;&#123;\"name\":\"zs\",\"age\":16&#125;&#123;\"name\":\"jack\",\"age\":19&#125;&#123;\"name\":\"lili\",\"age\":16&#125;&#123;\"name\":\"tom\",\"age\":19&#125; highlight 高亮 将搜索出的结果加上高亮的效果 按查询匹配度排序 .setExplain(true) 两个简单练习 聚合分组求count 12345678910111213141516/** * 统计测试 * 聚合分组求count */ public static void testAggregation(TransportClient client)&#123; SearchResponse searchResponse = client.prepareSearch(index) //.setTypes(type) .setQuery(QueryBuilders.matchAllQuery()) .addAggregation(AggregationBuilders.terms(&quot;term_age&quot;).field(&quot;age&quot;)) .get(); Terms term_age = searchResponse.getAggregations().get(&quot;term_age&quot;); List&lt;? extends Terms.Bucket&gt; buckets = term_age.getBuckets(); for (Terms.Bucket bk:buckets) &#123; System.out.println(bk.getKey()+&quot;----------&quot;+bk.getDocCount()); &#125; &#125; 聚合分组求sum 1234567891011121314151617/** * 统计测试 * 聚合分组求sum */public static void testAggregation2(TransportClient client)&#123; SearchResponse searchResponse = client.prepareSearch(index) .setQuery(QueryBuilders.matchAllQuery()) .addAggregation(AggregationBuilders.terms(\"term_name\").field(\"name.keyword\") //name是text类型不支持分组，所以取他的keyword .subAggregation(AggregationBuilders.sum(\"sum_score\").field(\"score\"))) .get(); Terms term_name = searchResponse.getAggregations().get(\"term_name\"); List&lt;? extends Terms.Bucket&gt; buckets = term_name.getBuckets(); for (Terms.Bucket bk:buckets) &#123; Sum sumScore = bk.getAggregations().get(\"sum_score\"); System.out.println(bk.getKey()+\"----------\"+sumScore.getValue()); &#125;&#125; Elasticsearch中的setting以及mapping详解 setting是修改索引库默认的配置 查看页面的setting信息 curl -XGET http://localhost:9200/test/_settings?pretty 修改已经存在的索引库信息 1curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test/_settings&apos; -d&apos;&#123;&quot;index&quot;:&#123;&quot;number_of_replicas&quot;:1&#125;&#125;&apos; 修改不存在的索引库的信息 1curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test1/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_shards&quot;:3,&quot;number_of_replicas&quot;:0&#125;&#125;&apos; mapping 是对索引库中的索引的名称以及数据类型进行定义，类似于MySQL的表名以及表的结构信息。但是es的mapping比较灵活，可以动态识别各字段的信息，一般不需要自定义mapping 查看索引库mapping信息 1curl -XGET http://localhost:9200/test/emp/_mapping?pretty 操作已经存在的索引 指定分词器 1curl -H &quot;Content-Type: application/json&quot; -XPOST http://localhost:9200/test/emp/_mapping -d&apos;&#123;&quot;properties&quot;:&#123;&quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;&#125;&#125;&apos; 操作不存在的索引 1curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test2&apos; -d&apos;&#123;&quot;mappings&quot;:&#123;&quot;emp&quot;:&#123;&quot;properties&quot;:&#123;&quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;&#125;&#125;&#125;&#125;&apos; Elasticsearch的分片查询方式 默认的是randomize across shards 表示随机选取，即随机从分片中取数据 _local :表示执行查询操作的时候回优先在本地节点中的分片中进行查询，没有的话在去其他节点 _only_local:表示只在本地分片中查询 _primary:表示只在主分片中查询 _primary_first:表示优先在主分片中查询，如果主分片出现问题数据丢失或者其他就会去副分片中查询 _replica_first:表示优先在副分片上查询，有问题了再去主分片查询 _only_node:在指定ID的节点上查询，只有该节点上有相关分片就回进行查询，可能导致查询结果不够完整 _only_nodes:指定ID的节点是多个 _prefer_node:优先在指定ID的节点查询 查不到再去其他节点 _shards:查询指定分片的信息 可以实现急速查询 但需要指定索引所在分片的信息","categories":[{"name":"提高","slug":"提高","permalink":"http://yoursite.com/categories/提高/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch简介","slug":"2018-06-10-Elasticsearch简介","date":"2018-06-10T05:10:12.000Z","updated":"2019-09-05T08:05:15.510Z","comments":true,"path":"2018/06/10/2018-06-10-Elasticsearch简介/","link":"","permalink":"http://yoursite.com/2018/06/10/2018-06-10-Elasticsearch简介/","excerpt":"ElasticsearchElasticsearch简介​ Elasticsearch是一个实时分布式搜索和分析引擎。它对Lucene进行了封装。能够满足实时搜索的稳定、可靠、快速等。基于REST接口。","text":"ElasticsearchElasticsearch简介​ Elasticsearch是一个实时分布式搜索和分析引擎。它对Lucene进行了封装。能够满足实时搜索的稳定、可靠、快速等。基于REST接口。 ES与MySQL的对比 Elasticsearch MySQL index 索引库 database 数据库 type 类型 table 类型 document 文档 row 行 field 字段 column 列 Elasticsearch安装部署 安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了） 下载Elasticsearch 网址：https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3 选择合适的版本下载就可以 下载完成以后上传到Linux系统中，解压 进入到解压后的文件中尝试进行开启 bin/elasticsearch （-d 后台运行） 注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off) 会发现执行报错 就是不可以在root用户下打开，选择一个其他用户就可以了 修改配置变量 修改Elasticsearch中config变量 （vi config/elasticsearch.yml) 增加两行代码 12bootstrap.system_call_filter: false network.host: 192.168.32.110 //后面的的IP设置你自己的本地IP就可以 修改Linux的配置变量 12345678910vi /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096vi /etc/sysctl.confvm.max_map_count=262144vi /etc/security/limits.d/90-nproc.conf 把1024修改为4096* soft nproc 4096 修改完配置以后重启操作系统在启动就可以啦 启动之后出现以下情况 没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了 然后可以使用web界面查看 在浏览器输入 http://yourip:9200 就可以查看了 简单基本操作CURL简介 curl起始就是一个可以在命令行下访问URL的工具 curl可以利用URL语法在命令行的方式下操作开源的文件 这样即可以方便我们其他不同部门对我们数据库的操作，也方便我们管理数据库，方便管理其他用户的权限 CURL的简单操作 -x 是指定http请求的方法 他的类型有很多种包括 GET POST PUT DELETE 查询、修改、增加、删除等很多操作 -d 是指需要传递的参数 首先我们先创建一个简单的索引 curl -XPUT ‘http://localhost:9200/test/&#39; localhost一定要换成你之前设置的IP 这样它就为我们创建了test索引库 然后我们可以创建一个索引并为创建的索引添加一些内容然后进行一些列查询 123456curl -H &quot;Content-Type: application/json&quot; //—H指定添加内容的类型为json类型-XPOST http://localhost:9200/test/emp/1 //1是指定索引的IP 不加系统也会自动生成-d &apos;&#123;&quot;name&quot; : &quot;tom&quot;,&quot;age&quot; : 25&#125;&apos; //加入索引中的具体内容 查询我们刚刚创建的索引 1curl -XGET http://localhost:9200/test/emp/1?pretty 检索索引中的一部分内容 1curl -XGET &apos;http://localhost:9200/test/emp/1?_source=name&amp;pretty&apos; 查询指定索引库指定类型的所有数据 12curl -XGET http://hadoop110:9200/test/emp/_search?pretty 查看tem类型下的所有数据 对ES进行更新操作，ES中可以使用put或者post两种方式进行更新操作 执行更新操作的时候ES的操作细节 首先将旧的文件标记为删除状态 添加新的文件 旧文件不会立即消失但是我们看不见 ES在后续你添加更多文件的时候在后台清理掉标记为删除状态的文件 执行局部更新的操作 1curl -H &quot;Content-Type: application/json&quot; -XPOST http://hadoop110:9200/test/emp/1/_update -d &apos;&#123;&quot;doc&quot;:&#123;&quot;age&quot;:20&#125;&#125;&apos; 我们接着进行一次查询看数据是否已经更新 可以看到年龄已将改成20了 所以说明更新操作成功了 我们可以根据这个操作做很多事情 对ES进行删除操作 删除我们之前创建的索引 1curl -XDELETE http://hadoop110:9200/test/emp/1 删除以后我们在进行get获取操作就会报错说明我们的删除操作已经执行成功了 如果删除文档存在 则会返回：200 ok的状态码，found属性值为true，_version属性的值+1 如果想要删除的文件不存在就会返回：404 NotFound的状态码，found属性值为false，但是_version属性的值依然会+1，这个就是内部管理的一部分，它保证了我们在多个节点间的不同操作的顺序都被正确标记了 对ES进行批量操作 包括很多步的增删改查等 批量操作就是bulk API帮助我们同时执行多个操作 语法的格式： 123456action：index/create/update/delete //需要执行的操作类型metadata：_index,_type,_id //指定需要操作的索引的索引库、类型、ID等request body：_source(删除操作不需要) &#123; action: &#123; metadata &#125;&#125; //具体要执行的操作&#123; request body &#125;......... create与index的区别 在创建数据时，如果数据已存在 create会返回创建失败，文件已存在，但是index会执行成功 使用方法： 我们创建一个文件保存我们需要执行的操作 12345vi requests&#123; &quot;index&quot; : &#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;emp&quot;,&quot;_id&quot;:&quot;21&quot;&#125;&#125;&#123; &quot;name&quot; : &quot;test21&quot;&#125;执行：curl -H &quot;Content-Type: application/json&quot; -XPUT localhost:9200/test/emp/_bulk --data-binary @requests 出现下面结果表示执行成功了 我们可以放多条指令进去 同时执行多条指令但是要保证中间格式不出错 插件的介绍Elasticsearch Head Plugin站点插件可以以网页形式展现ES 注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt nodejs下载地址https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz 复制此链接就可以直接下或者https://nodejs.org/dist 使用这个两节找适合自己的版本 因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压 如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz 123解压：xz -d node-v10.15.3-linux-x64.tar.xztar -xvf node-v10.15.3-linux-x64.tar 创建软连接 12ln -s /data/soft/node-v10.15.3-linux-x64/bin/node /usr/bin/nodeln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm 设定nodejs安装软件的dialing服务器 1npm config set registry https://registry.npm.taobao.org 安装grunt 12npm install -g grunt npm install -g grunt-cli 创建软连接 1ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt /usr/bin/grunt 安装phantomjs 下载地址http://phantomjs.org/download.html 解压 12bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2tar -xvf phantomjs-2.1.1-linux-x86_64.tar 创建软连接 1ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/bin/phantomjs 安装依赖软件 1yum -y install wget fontconfig 安装head插件 下载地址：https://github.com/mobz/elasticsearch-head/archive/master.zip 解压 需要使用unzip命令 下载unzip 1yum install -y unzip 解压 1unzip elasticsearch-head-master.zip 然后cd进入到解压的文件中 安装两个插件 12npm audit fixnpm audit fix --force 执行安装命令安装head 1sudo npm install 启动之前修改Gruntfile.js文件，增加hostname参数 1234567vi Gruntfile.jsoptions: &#123; hostname: &apos;hadoop100&apos;, port: 9101, base: &apos;.&apos;, keepalive: true&#125; 启动服务 1grunt Server 启动服务后还需要修改Elasticsearch中的一些配置 123vi config/elasticsearch.ymlhttp.cors.enabled: true http.cors.allow-origin: &quot;*&quot; 修改完成后重启ES 进入网址http://hadoop100:9101/可以看到以下信息就说明插件安装成功了 配置参数详解 配置文件elasticsearch.yml ES已经为大多数的参数设置了合理的默认值 我们只需要在有特殊需求的时候进行修改 书写规范 属性顶格写，不能有空格 缩进一定要是用空格而不能使用制表符 属性与属性值之间必须有一个空格 常见的配置文件以及其含义 cluster.name: 集群名称 node.name 节点名称 path.data: /path/to/data es的数据存储目录 path.logs: /path/to/logs es的日志存储目录 bootstrap.memory_lock: true 锁定物理内存地址，防止elasticsearch内存被交换出去,也就是避免es使用swap交换分区中的内存 network.host: 192.168.0.1 为es设置ip绑定 http.port: 9200 为es设置自定义端口，默认是9200 discovery.zen.ping.unicast.hosts: [“host1”, “host2”] 当启动新节点时，通过这个ip列表进行节点发现，组建集群 discovery.zen.minimum_master_nodes: 通过配置这个参数来防止集群脑裂现象 (集群总节点数量/2)+1 gateway.recover_after_nodes: 3 一个集群中的N个节点启动后,才允许进行数据恢复处理，默认是1 action.destructive_requires_name: true 设置是否可以通过正则或者_all删除或者关闭索引库 核心概念 cluster 代表的是一个集群，集群中有很多节点，其中有一个主节点，这个主节点通过选举产生，主从节点时对于集群内部而言的。es有一个概念叫去中心化，就是说没有中心节点，这个是对于外部来说的，在外部来看，集群就是一个整体，我们两节集群中的任何一个节点与集群通信跟直接与集群通信是等价的。 主节点的主要职责就是负责管理集群的状态，包括管理分片以及副本的状态，以及节点的删除、新节点的发现等 注意：主节点不负责对进群的增删改查处理，只负责管理集群状态 shards 代表的是索引分片，ES将一个完整的索引分成多个分片，这样的好处是可以把一个大的索引分成多个分片后分布到不同的节点上，构成分布式搜索。提高性能和吞吐量 分片的的数量只能在创建索引库的时候指定，索引库创建以后不可以更改 索引库默认是5个分片 每个分片最多存储2,147,483,519条数据 1curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_shards&quot;:3&#125;&#125;&apos; replicas 代表的是分片的副本，es给分片设置副本是为了提高系统的容错性，当某个节点的某个分片损坏或者丢失了可以从副本中恢复。 提高es的查询效率，es会自动搜索并请求进行负载均衡 默认每个分区只有一个副本，主副本不会存在于一个节点之上，副本数量可以在创建索引库的时候进行设置吧 1curl -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_replicas&quot;:2&#125;&#125;&apos; recovery 代表数据的恢复或者数据的重新分布 es在所有节点的加入或者退出后会根据机器的负载对索引分片进行重新分配，挂掉的节点重启时也会进行数据恢复 ElasticsearchJavaAPI操作使用Java对ES进行操作 添加maven依赖 可以maven仓库中寻找适合你的版本 12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.4.3&lt;/version&gt;&lt;/dependency&gt; Java中对ES的简单增删改查操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package EsTest;import org.elasticsearch.action.delete.DeleteResponse;import org.elasticsearch.action.get.GetResponse;import org.elasticsearch.action.index.IndexResponse;import org.elasticsearch.action.update.UpdateResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.TransportAddress;import org.elasticsearch.common.xcontent.XContentType;import org.elasticsearch.transport.client.PreBuiltTransportClient;import java.net.InetAddress;import java.util.HashMap;/** * Es简单操作测试 */public class EsDemo1 &#123; public static void main(String[] args) throws Exception&#123; //给集群添加自动嗅探的功能 Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;) //集群名称 .put(&quot;client.transport.sniff&quot;, true) //开启自动嗅探功能，可以自动识别集群内的其他节点信息 .build(); //创建连接 TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) //可添加多个节点 //.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop100&quot;), 9300)) .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300)); //获取节点的信息 int size = client.connectedNodes().size(); //System.out.println(size); String index = &quot;test&quot;; String type = &quot;emp&quot;; //添加数据 使用json字符串 String json = &quot;&#123;\\&quot;name\\&quot;:\\&quot;jack\\&quot;,\\&quot;age\\&quot;:10&#125;&quot;; IndexResponse res = client.prepareIndex(index, type, &quot;1&quot;) .setSource(json, XContentType.JSON).get(); //System.out.println(res.toString()); //添加数据 使用map结构 HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;name&quot;,&quot;zs&quot;); map.put(&quot;age&quot;,21); IndexResponse res2 = client.prepareIndex(index, type, &quot;101&quot;) .setSource(map) .execute() .actionGet(); //更新操作 update UpdateResponse updateResponse = client.prepareUpdate(index, type, &quot;101&quot;).setDoc(&quot;&#123;\\&quot;age\\&quot;:18&#125;&quot;, XContentType.JSON).get(); //根据ID进行数据查询 GetResponse get1 = client.prepareGet(index, type, &quot;101&quot;).get(); System.out.println(get1.getSourceAsString()); //删除操作 delete DeleteResponse deleteResponse = client.prepareDelete(index, type, &quot;101&quot;).get(); System.out.println(deleteResponse.toString()); &#125;&#125;","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch Head Plugin","slug":"2018-06-05-Elasticsearch Head Plugin 详细安装教程","date":"2018-06-05T03:25:46.000Z","updated":"2019-09-05T08:05:03.887Z","comments":true,"path":"2018/06/05/2018-06-05-Elasticsearch Head Plugin 详细安装教程/","link":"","permalink":"http://yoursite.com/2018/06/05/2018-06-05-Elasticsearch Head Plugin 详细安装教程/","excerpt":"Elasticsearch Head Plugin 详细安装教程Elasticsearch Head Plugin站点插件可以以网页形式展现ES","text":"Elasticsearch Head Plugin 详细安装教程Elasticsearch Head Plugin站点插件可以以网页形式展现ES 注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt nodejs下载地址https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz 复制此链接就可以直接下或者https://nodejs.org/dist 使用这个两节找适合自己的版本 因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压 如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz 123解压：xz -d node-v10.15.3-linux-x64.tar.xztar -xvf node-v10.15.3-linux-x64.tar 创建软连接 12ln -s /data/soft/node-v10.15.3-linux-x64/bin/node /usr/bin/nodeln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm 设定nodejs安装软件的dialing服务器 1npm config set registry https://registry.npm.taobao.org 安装grunt 12npm install -g grunt npm install -g grunt-cli 创建软连接 1ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt /usr/bin/grunt 安装phantomjs 下载地址http://phantomjs.org/download.html 解压 12bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2tar -xvf phantomjs-2.1.1-linux-x86_64.tar 创建软连接 1ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/bin/phantomjs 安装依赖软件 1yum -y install wget fontconfig 安装head插件 下载地址：https://github.com/mobz/elasticsearch-head/archive/master.zip 解压 需要使用unzip命令 下载unzip 1yum install -y unzip 解压 1unzip elasticsearch-head-master.zip 然后cd进入到解压的文件中 安装两个插件 12npm audit fixnpm audit fix --force 执行安装命令安装head 1sudo npm install 启动之前修改Gruntfile.js文件，增加hostname参数 1234567vi Gruntfile.jsoptions: &#123; hostname: &apos;hadoop100&apos;, port: 9101, base: &apos;.&apos;, keepalive: true&#125; 启动服务 1grunt Server 启动服务后还需要修改Elasticsearch中的一些配置 123vi config/elasticsearch.ymlhttp.cors.enabled: true http.cors.allow-origin: &quot;*&quot; 修改完成后重启ES 进入网址http://hadoop100:9101/可以看到以下信息就说明插件安装成功了","categories":[{"name":"安装部署","slug":"安装部署","permalink":"http://yoursite.com/categories/安装部署/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch高级","slug":"2018-06-04-Elasticsearch基础","date":"2018-06-04T05:46:56.000Z","updated":"2019-09-05T08:04:40.465Z","comments":true,"path":"2018/06/04/2018-06-04-Elasticsearch基础/","link":"","permalink":"http://yoursite.com/2018/06/04/2018-06-04-Elasticsearch基础/","excerpt":"Elasticsearch高级","text":"Elasticsearch高级 Elasticsearch批量操作的查询类型Bulk批量查询的Java实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package EsTest;import org.elasticsearch.action.bulk.BulkItemResponse;import org.elasticsearch.action.bulk.BulkRequestBuilder;import org.elasticsearch.action.bulk.BulkResponse;import org.elasticsearch.action.delete.DeleteRequest;import org.elasticsearch.action.index.IndexRequest;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.TransportAddress;import org.elasticsearch.transport.client.PreBuiltTransportClient;import java.net.InetAddress;/** * Es简单操作测试 */public class EsDemo2 &#123; static String index = &quot;test&quot;; static String type = &quot;emp&quot;; public static void main(String[] args) throws Exception&#123; Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;) .put(&quot;client.transport.sniff&quot;, true) .build(); TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300)); testBulk(client); &#125; /** * bulk批量查询测试 */ public static void testBulk(TransportClient client)&#123; BulkRequestBuilder bulkRequestBuilder = client.prepareBulk(); //创建请求 IndexRequest indexRequest = new IndexRequest(index, type) .source(&quot;&#123;\\&quot;name\\&quot;:\\&quot;zs1\\&quot;,\\&quot;age\\&quot;:25&#125;&quot;); //删除请求 DeleteRequest deleteRequest = new DeleteRequest(index, type, &quot;21&quot;,,XContentType.JSON); //将操作整合到buider中 bulkRequestBuilder.add(indexRequest); bulkRequestBuilder.add(deleteRequest); //执行批量操作 BulkResponse bulkItemResponses = bulkRequestBuilder.get(); //查看执行过程中失败的信息 if(bulkItemResponses.hasFailures())&#123; BulkItemResponse[] items = bulkItemResponses.getItems(); for (BulkItemResponse item: items) &#123; System.out.println(item.getFailureMessage()); &#125; &#125;else &#123; System.out.println(&quot;所有bulk指令都执行成功了&quot;); &#125; &#125;&#125; SearchType的详解 query and fetch :当客户机向es集群中的某一个节点发送请求时，这个节点会将请求复制到每一个节点上，然后每一个节点会将所请求的数据返回到查询节点上，然后由查询节点返回到客户机上，这样的优点就是速度快，缺点是不准确，客户想要10条数据，集群返回的是10*n条数据，n是集群的节点数 query then fetch:当客户机向集群发送请求时，集群中接收请求的节点也会将查询请求发送到每一个节点之上，但是每个节点只返回查询结果的ID等值给主节点，主节点将受到的数据在进行排序取出所需要的条数，然后根据其ID等到相应节点上取的数据，在将数据返回至客户机。优点是可以准确返回需要条数的请求，且结果相对来说准确，缺点是查询速度慢，是es的默认查询类型 DFS D是Distributed，F是frequency的缩写，S是Scatter的缩写，整个单词可能是分布式词频率和文档频率散发的缩写 dfs简称是初始化散发 官方解释是初始化散发其实就是在进行真正的查询之前，先把各个分片的词频率和文档频率收集一下，然后进行词搜索的时候，各分片依据全局的词频率和文档频率进行搜索和排名。 通俗一点来说就是统计所有节点的搜索排名的算法，总结到一起可以对整个文档进行精确的算法排名 dfs query and fetch：就是加了dfs的query and fetch依然是速度快，但是结果条数多 dfs query then fetch:执行过程：首先，从各个节点的搜索排序算法即词频率文档频率等，然后根据整合好的算法在每个节点上取出相应数据的ID等信息，在主节点上再次通过该算法获取准确的数据信息，在通过他们的ID等信息去各个节点获取具体数据返回至客户机上。优点是查询准确率高，但是查询速度慢 代码写法 1234567891011121314151617181920212223242526272829303132333435363738/** * Es简单操作测试 */public class EsDemo2 &#123; static String index = &quot;test&quot;; static String type = &quot;emp&quot;; public static void main(String[] args) throws Exception&#123; Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;) .put(&quot;client.transport.sniff&quot;, true) .build(); TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300)); //testBulk(client); testSeType(client); &#125; /** * 查询类型的测试 */ public static void testSeType(TransportClient client)&#123; SearchResponse searchResponse = client.prepareSearch(index) //索引库信息 .setQuery(QueryBuilders.matchAllQuery()) //查询规则 所有队列 .setPreference(&quot;_shards:1&quot;) //指点分片 .setSearchType(SearchType.QUERY_THEN_FETCH) //类型可以自己指定 .get(); SearchHits hits = searchResponse.getHits(); //获取总条数 long totalHits = hits.getTotalHits(); System.out.println(&quot;数据的总条数&quot;+totalHits); //打印所有数据内容 SearchHit[] hits1 = hits.getHits(); for (SearchHit hit:hits1) &#123; System.out.println(hit.getSourceAsString()); &#125; &#125; &#125; Elasticsearch分词详解es索引建立和搜索过程图解 倒排索引介绍 表中的各个单词表示文档意思 单词ID：记录每个单词的编号 单词：单词ID对应的单词 文档频率：单词在几个文档中出现过 倒排列表：单词出现的文档信息以及单词出现位置信息 DocID：单词出现的文档的ID TF：单词在该文档出现的次数 POS：单词在文档中出现的位置 分词器Analyzer的介绍 分词器就是将数据按以此为单位分开 分词器的作用 是吧文本中的词按照一定的规则进行切分。 分词器所对应的的类是Analyzer是一个抽象类，具体的实现方法要靠他的子类，所以对于不同的语言就可以提供不同的分词器 在创建索引以及搜索的时候都会用到分词器，而且这两个过程所用到的分析器必须是同一种分词器 分词器的工作流程 切分关键词 取出停用词 对于英文字母，将所有字母转换为小写 停用词的介绍 有些词在文本中出现的概率很高但是对于文本所携带的信息并没有什么影响 英文中的 a,an,the,of 等 http://www.ranks.nl/stopwords 中文的 的，了，是，着，以及各种标点符号等等 http://www.ranks.nl/stopwords/chinese-stopwords 文本经过分词的过程以后，这种停用词一般都会被过滤掉，不会被索引 如果搜索的词含有停用词一本也会被过滤掉 过滤掉停用词可以加快建立索引，减小索引库的文件的大小 几个重要的分词器介绍 分词器 分词方式 StandardAnalyzer 单词分词器 ChineseAnalyzer 单字分词器 CJKAnalyzer 二分法分词器 IKAnalyzer 词库分词器 单字分词以及单词分词的意思是一样的 “我们是中国人”效果：“我”“们”“是”“中”“国”“人” 二分法分词器：按两个字的方式分词 “我们是中国人”，效果：“我们”、“们是”、“是中”、“中国”、“国人” 词库分词器 按某种算法造词，然后将词存入到词库，把搜索内容匹配到词库的词然后进行拆分。 Elasticsearch分词插件介绍以及使用es-ik 官方默认的分词插件对中文的支持不是很好，所以我们需要采用第三方的词库来进行分词，IK就是一个分成不错的分词工具 下载地址 https://github.com/medcl/elasticsearch-analysis-ik/releases 将下载好的文件放在ES_HOME/plugins/ik目录下解压 解压后就可以使用 自己添加词库 进入到config文件中 创建一个自己存放自己词库的文件夹 在文件夹中创建dic文件将自己的词库内容放到所创建的文件中 修改IKAnalyzer.cfg.xml文件信息 将自己创建的词库加进去 重启es就可以使用自己创建的词库了 实现热词的自动更新不需要重启es 在一台服务器上部署一个Tomcat 在Tomcat中的webapp/ROOT 创建hot.doc热词词库 通过访问网络端口确定这个热词库可以访问 修改IK的配置 http://192.168.80.100:8080/hot.dic 重启es之后就可以在hot.dic文件中动态添加热词，IK会定时从端口中访问该文件然后进行更新热词","categories":[{"name":"提高","slug":"提高","permalink":"http://yoursite.com/categories/提高/"}],"tags":[{"name":"Elasticsearch IK","slug":"Elasticsearch-IK","permalink":"http://yoursite.com/tags/Elasticsearch-IK/"}]},{"title":"Elasticsearch的安装部署","slug":"2018-06-02-Elasticsearch的安装部署","date":"2018-06-02T02:35:11.000Z","updated":"2019-09-05T08:02:32.821Z","comments":true,"path":"2018/06/02/2018-06-02-Elasticsearch的安装部署/","link":"","permalink":"http://yoursite.com/2018/06/02/2018-06-02-Elasticsearch的安装部署/","excerpt":"Elasticsearch安装部署 安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了）","text":"Elasticsearch安装部署 安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了） 下载Elasticsearch 网址：https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3 选择合适的版本下载就可以 下载完成以后上传到Linux系统中，解压 进入到解压后的文件中尝试进行开启 bin/elasticsearch （-d 后台运行） 注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off) 会发现执行报错 就是不可以在root用户下打开，选择一个其他用户就可以了 修改配置变量 修改Elasticsearch中config变量 （vi config/elasticsearch.yml) 增加两行代码 12bootstrap.system_call_filter: false network.host: 192.168.32.110 //后面的的IP设置你自己的本地IP就可以 修改Linux的配置变量 12345678910vi /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096vi /etc/sysctl.confvm.max_map_count=262144vi /etc/security/limits.d/90-nproc.conf 把1024修改为4096* soft nproc 4096 修改完配置以后重启操作系统在启动就可以啦 启动之后出现以下情况 没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了 然后可以使用web界面查看 在浏览器输入 http://yourip:9200 就可以查看了","categories":[{"name":"安装部署","slug":"安装部署","permalink":"http://yoursite.com/categories/安装部署/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Storm高级以及优化","slug":"2018-04-01-Storm高级","date":"2018-04-01T10:32:45.000Z","updated":"2019-09-05T08:07:08.139Z","comments":true,"path":"2018/04/01/2018-04-01-Storm高级/","link":"","permalink":"http://yoursite.com/2018/04/01/2018-04-01-Storm高级/","excerpt":"Storm高级","text":"Storm高级 Storm核心之流分组 stream grouping 分类 Shuffle Grouping：随机分组。将stream中的tuple缓存后随机发放给所有bolt，可以使每个bolt中的数据量大致相等（可以较好的实现负载均衡） Fields Grouping：按字段分组，例如按groupID字段进行分组，将同一个分组的tuple分到统一任务中 All Grouping:广播发送，每一个tuple都会发送到所有任务中，所以每一个bolt都会有所有的tuple Global Grouping：全局分组，这个tuple会被分配到storm中的某一个bolt,具体一点就是分配到ID值最小的一个bolt之中 Non Grouping：随机分派，效果和shuffle一样 Direct Grouping：直接分组，将tuple发送给制定好的任务中 localOrShuffleGrouping：指如果目标Bolt 中的一个或者多个Task 和当前产生数据的Task在同一个Worker 进程里面，那么就走内部的线程间通信，将Tuple 直接发给在当前Worker进程的目的Task。否则，同shuffleGrouping。 Storm可靠性剖析Storm可能出现的问题 worker进程死掉 supervisor进程死掉 nimbus进程死掉 节点宕机 解决方案 (acker机制)ack/fail消息确认机制(确保一个tuple被完全处理) 在spout中发射tuple的时候需要同时发送messageid，这样才相当于开启了消息确认机制 如果你的topology里面的tuple比较多的话，那么把acker的数量设置多一点,效率会高一点。 通过config.setNumAckers(num)来设置一个topology里面的acker的数量，默认值是1。 注意：acker用了特殊的算法，使得对于追踪每个spout tuple的状态所需要的内存量是恒定的（20 bytes) 注意：如果一个tuple在指定的timeout(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS默认值为30秒)时间内没有被成功处理，那么这个tuple会被认为处理失败了。 Storm定时器分析 可以指定每隔一段时间将数据整合一次存入数据库 在main中设置conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, 60);// 设置本Bolt定时发射数据 在bolt中使用下面代码判断是否是触发用的bolt tuple.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID) StormUI的详解 deactive：未激活(暂停) emitted:emitted tuple数 与emitted的区别：如果一个task，emitted一个tuple到2个task中，则transferred tuple数是emitted tuple数的两倍 completelatency: spout emitting 一个tuple到spout ack这个tuple的平均时间(可以认为是tuple以及该tuple树的整个处理时间) processlatency: bolt收到一个tuple到bolt ack这个tuple的平均时间，如果没有启动acker机制，那么值为0 execute latency：bolt处理一个tuple的平均时间，不包含acker操作，单位是毫秒(也就是bolt执行 execute 方法的平均时间) capacity：这个值越接近1，说明bolt或者spout基本一直在调用execute方法，说明并行度不够，需要扩展这个组件的executor数量。(调整组件并行度的依据) 总结：execute latency和proces latnecy是处理消息的时效性，而capacity则表示处理能力是否已经饱和，从这3个参数可以知道topology的瓶颈所在。 Storm的优化并行度的优化 worker为storm提供工作进程，程序的并行度可以设置（包括spout和bolt的并行度，如果有acker的话还包括acker的并行度），并行度即为executor的数目。 一般情况下worker与executor的比例是一比十到十五，也可以根据实际需要修改。 worker的优化 CPU 16核，建议配置20个worker。CPU 24或32核，30个worker 默认情况下，Storm启动worker进程时，JVM的最大内存是768M，可以通过在Strom的配置文件storm.yaml中设置worker的启动参数worker.childopts: “-Xmx2048m” 一个topology使用的worker数量，12个是比较合理的，这个时候吞吐量和整体性能最优。如果多增加worker进程的话，会将一些原本线程间的内存通信变为进程间的网络通信。 acker优化 如果可靠性对你来说不是那么重要，那么你可以通过不跟踪这些tuple树来获取更好的性能。不去跟踪消息的话会使得系统里面的消息数量减少一半，因为对于每一个tuple都要发送一个ack消息。 三种去掉可靠性的方法 第一是把config.setNumAckers(0)设置为0，在这种情况下，storm会在spout发射一个tuple之后马上调用spout的ack方法。也就是说这个tuple树不会被跟踪。 第二个方法是在tuple层面去掉可靠性。你可以在发射tuple的时候不指定messageid来达到不跟踪spout中tuple的目的。 最后一个方法是如果你对于一个tuple树里面的某一部分到底成不成功不是很关心，那么可以在发射这些tuple的时候unanchor它们(anchor是锚定的意思，unanchor表示不把当前这个tuple包含到tuple树中，也就是说不跟踪这个消息了)。这样这些tuple就不在tuple树里面， 也就不会被跟踪了。 雪崩问题的出现原因以及解决方法 原因：spout发送的速度大于bolt接收的速度，导致数据堆积，不断消耗内存，最终系统崩溃，并引起数据链上多节点down掉。 解决方案 增加bolt的并行度 增加它接收的速度 可以通过topology.max.spout.pending来控制spout发送消息的速度，通过代码这样设置config.setMaxSpoutPending(num); 注意：这个参数表示，当下游的bolt还有topology.max.spout.pending个 tuple 没有消费完时，spout会停止调用nexttuple方法发射数据。等待下游bolt去消费，当tuple的个数少于topology.max.spout.pending个数时，spout 会继续发射数据(这个属性只对可靠消息处理有用，也就是说需要启用acker消息确认机制，在spout中emit数据的时候需要带有messageid)","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"Storm","slug":"2018-03-28-Storm的详细分析","date":"2018-03-28T05:46:56.000Z","updated":"2019-09-05T08:06:54.133Z","comments":true,"path":"2018/03/28/2018-03-28-Storm的详细分析/","link":"","permalink":"http://yoursite.com/2018/03/28/2018-03-28-Storm的详细分析/","excerpt":"Storm的详细分析","text":"Storm的详细分析 Storm人的概述 Storm是Twitter开源的一个实时处理框架 Storm能够实现高频数据和大规模数据的实时处理 Storm与MapReduce的区别Storm type MapReduce Storm 数据来源 hdfs上TB级别历史数据 实时新增的某一条数据 处理过程 map阶段和reduce阶段 可以有很多阶段包含spout以及bolt 是否会结束 执行完结束 不会结束 处理速度 主要以执行TB级别数据速度较慢 只处理新增数据速度很快 适用场景 处理批数据不讲时效性 处理新增数据将时效性 Spark Streaming与Storm的区别 type Spark Streaming Storm 计算模型 是近实时处理框架 全实时处理框架 延迟度 最高支持秒级别的延迟 可以支持毫秒级别的延迟 吞吐量 因为是批处理所以吞吐量高 吞吐量相对来说较低 动态调整并行度 不支持 支持 事务机制 支持但是不够完善 支持且完善 Storm各个组件解释 Topology：用于封装一个实时计算应用程序的逻辑 Stream：消息流，是一个没有边界的tuple序列，这些tuple会以分布式的方式进行创建以及处理 Spout：消息源，消息的生产者，会从外部源获取消息然后向Topology发出：tuple Bolt：消息处理者，消息的处理逻辑被封装到bolt之中，处理输入的数据然后产生新的输出数据流 Storm的设计思想 是对stream流的一个抽象即一个不间断的连续tuple 将流中的元素抽象为一个tuple，一个tuple就是一个值列表value list，list中的每个value都有一个name，并且这个value可以是很多数据类型例如基本类型、字符类型等 每一个stream流都有一个数据源，称为Spout stream从spout中获取不间断数据tuple需要经过处理。处理的过程就是stream流转换的过程称为bolt，bolt可以消费任意数量的流，它是将stream汇总的tuple挨个实时进行处理转换成一个新的stream流经过多个bolt处理就可以得到目标数据 spout+tuple+bolt这个过程可以称为是Topology拓扑。Topology是Storm中最高的一个抽象概念他可以被提交到集群中执行 Topology的每个节点都要指定他所发射数据的name，其他节点只需要订阅该name就可以接收数据进行处理 Topology的整个流程 如果将stream比作是一列火车的话 spout就是这列火车的始发站每一节车厢就是一个tuple乘客就是tuple中的values 中间的站点就相当于是bolt进行处理上下乘客终点站就相当于stream的目标数据 Storm的整体架构图 Storm的简单实例开发 需求：一个源源不断的数据1，2，3，4……求每出现一个数字就要计算出现的所有数字的和 开发过程 在IDE中创建maven工程 在pom中添加、Storm依赖 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;1.0.6&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package Storme;import java.util.Map;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.generated.StormTopology;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.apache.storm.utils.Utils;/** * 需求：实现数字累加求和 * 分析： * 需要有一个spout负责源源不断的产生从1开始的递增数字 * 还需要有一个bolt负责对spout产生的数据进行累加求和，并且把结果打印到控制台 * 最后把这个spout和bolt组装成一个topology * */public class WordCount &#123; /** * 实现自己的数据源spout， * 该spout负责源源不断产生从1开始的递增数字 * */ public static class MySpout extends BaseRichSpout&#123; private Map conf;//这里面存储配置信息 private TopologyContext context;//代表上下文 private SpoutOutputCollector collector;//收集器，主要负责向外面发射数据 /** * 是一个初始化的方法，这个方法在本实例运行的之后，首先被调用，仅且仅被调用一次 * 所以这个方法内一般放一些初始化的代码 * 例子：针对操作mysql数据的案例，使用jdbc获取数据库连接的代码需要放到这里面实现 */ public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; this.conf = conf; this.context = context; this.collector = collector; //System.err.println(&quot;spout------&quot;+conf.get(&quot;name&quot;)); &#125; /** * 这个方法会被循环调用 */ int i = 1; public void nextTuple() &#123; // 注意：针对需要发射的数据，需要封装成tuple，可以使用storm中的values对象快速封装tuple System.out.println(&quot;spout:&quot;+i); this.collector.emit(new Values(i++)); // 让线程每发射一条数据，休息1秒 Utils.sleep(1000); &#125; /** * 声明输出字段 * 定义两个组件之间数据传输的一个规则 * 注意：只要这个组件(spout/spout)向外发射了数据，那么这个declareOutputFields就需要实现 */ public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; // 注意：Fields中的字段列表和Values中的数据列表是一一对应的 declarer.declare(new Fields(&quot;num&quot;)); &#125; &#125; /** * 聚合的Bolt，负责把Spout发射出来的数据进行累加求和，并且打印到控制台 * */ public static class SumBolt extends BaseRichBolt&#123; private Map stormConf; private TopologyContext context; private OutputCollector collector; /** * prepare是一个初始化方法，只会执行一次，这里面也是可以放一些初始化的代码 */ public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.stormConf = stormConf; this.context = context; this.collector = collector; //System.err.println(&quot;bolt------&quot;+stormConf.get(&quot;name&quot;)); &#125; int sum = 0; /** * 这个方法也是会被循环调用 * 主要上一个组件向外发射了数据，那么这个方法就会被调用一次 */ public void execute(Tuple input) &#123; //input.getInteger(0);// 通过角标获取数据 Integer num = input.getIntegerByField(&quot;num&quot;); sum += num; System.out.println(&quot;和为：&quot;+sum); &#125; /** * 注意：这个方法在这里就不需要实现了，因为这个bolt没有向下一个组件发射数据 */ public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125; &#125; public static void main(String[] args) &#123; TopologyBuilder builder = new TopologyBuilder(); // 组装spout builder.setSpout(&quot;spoutid&quot;, new MySpout()); // 组装bolt,并且告诉bolt接收哪个组件的数据 builder.setBolt(&quot;bolt-1&quot;, new SumBolt()).shuffleGrouping(&quot;spoutid&quot;); StormTopology createTopology = builder.createTopology(); // 通过代码创建一个本地集群 LocalCluster localCluster = new LocalCluster(); String topologyName = WordCount.class.getSimpleName(); Config config = new Config(); config.put(&quot;name&quot;, &quot;zs&quot;); // 把代码提交到本地集群中运行 localCluster.submitTopology(topologyName, config, createTopology); &#125;&#125; Storm核心之并行度组件解释 worker：worker是一个进程，每一个worker进程里面都执行的是一个Topology的任务（不会出现一个worker执行多个Topology的任务）。一个worker中会启动一个或多个executor线程来执行Topology的spout或者bolt组件。一个Topology会使用一个或者多个worker来执行任务 executor：是worker进程内部启动的独立线程，每一个executor会产生一个或者多个task（storm默认是一个task即一个spout或者bolt有一个task，如果有多个task，executor会循环调用所有task中的实例） task：是最终运行spout或者bolt中具体代码的执行单元。Topology启动（spout或者bolt）的task的数目是不变的，但是executor线程的数量可以动态进行调整（例如：1个executor线程可以执行该(spout或bolt)的1个或多个task实例）。这意味着，对于1个(spout或bolt)存在这样的条件：#threads&lt;=#tasks（即：线程数小于等于task数目）。默认情况下task的数目等于executor线程数目，即1个executor线程只运行1个task。 默认情况下，一个supervisor节点最多可以启动4个worker进程，每一个topology默认占用一个worker进程，每个spout或者bolt会占用1个executor，每个executor启动1个task。 提高Storm组件的并行度 worker（slot）【间接】 默认一个节点最多可以启动四个worker进程，可以修改进程数量strom-core.jar/defaults.yaml/supervisor.slots.ports 默认一个Topology只有一个worker进程，可以通过代码指定一个Topology使用多个worker进程config.setNumWorkers(workersnum) 注意：如果worker使用完在提交Topology就不会执行，会处于等待状态。worker之是通过Netty通信的 executor【直接】 默认情况下一个executor只会运行一个task，可以直接通过代码修改增加task数量，会直接提高Storm组件的并行度 builder.setSpout(id, spout,parallelism_hint); builder.setBolt(id, bolt,parallelism_hint); task【间接】 通过boltDeclarer.setNumTasks(num);来设置实例的个数 executor的数量会小于等于task的数量(为了rebalance) 弹性计算rebalance 前提是Topology中的task数量要大于executor线程数 通过shell调整 storm rebalance mytopology -w 10 -n 5 -e blue-spout=3 -e yellow-bolt=10 注意：acker的树木运行时是不会变化的，所以多指定几个worker进程，acker的数量也不会增加 -w：表示超时时间，Rebalance会在一个超时时间内注销掉Topology，然后在集群中重新分配worker -n：表示的是worker的数量 -e：调整组件的并行度 注：-n 以及 -e 都可以单独使用或者组合起来使用 通过UI界面进行调整，不建议使用所以就不具体解释使用方法了 并行度设置多少合适 单spout每秒大概可以发送500个tuple 单bolt每秒大概可以接收2000个tuple 单acker每秒大概可以接收6000个tuple 根据需要进行调整","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"Zookeeper简单总结","slug":"2018-03-25-Zookeeper简单总结","date":"2018-03-24T16:00:00.000Z","updated":"2019-09-05T07:43:56.951Z","comments":true,"path":"2018/03/25/2018-03-25-Zookeeper简单总结/","link":"","permalink":"http://yoursite.com/2018/03/25/2018-03-25-Zookeeper简单总结/","excerpt":"Zookeeper简单总结","text":"Zookeeper简单总结 简介 zookeeper是一个分布式协调服务 分布式应用程序可以基于它实现同步服务，配置维护以及命名维护等 zookeeper可以保证数据在zookeeper集群之间的数据的事务性一致 zookeeper中的组件角色 leader 领导者，负责进行肉票的发起以及决议，更新系统状态 learner 学习者 ，包括follower 跟随者 以及 observer 观察者 follower 跟随者：用于接收客户端的请求 并向客户端返回结果 在选择主节点的时候参与投票 observer 观察者：可以接收少部分的客户端连接，将写请求转发给leader，不参与投票，主要是用来 同步leader的状态，目的是为了扩展系统，提高读取速度 client 客户端：请求发起方 命令行操作 ls / 查看指定节点下的信息 create /test/ data 创建节点 get / test 获取节点上保存的数据 set /tets datass 修改节点的内容 rmr /test 删除节点 节点的介绍zookeeper的数据模型 层次化的目录结构，命名符合常规文件系统规范 每个节点在zookeeper中叫做znode,并且其有一个唯一的路径标识 节点Znode可以包含数据和子节点，但是EPHEMERAL临时类型的节点不能有子节点 Znode中的数据可以有多个版本，比如某一个路径下存有多个数据版本，那么查询这个路径下的数据就需要带上版本 客户端应用可以在节点上设置监视器 节点不支持部分读写，而是一次性完整读写 zookeeper的节点 Znode有两种类型，临时节点（ephemeral）和永久节点（persistent） Znode的类型在创建时确定并且之后不能再修改 临时znode的客户端会话结束时，zookeeper会将该临时znode删除，临时znode不可以有子节点 永久znode不依赖于客户端会话，只有当客户端明确要删除该znode时才会被删除 Znode有四种形式的目录节点，PERSISTENT、PERSISTENT_SEQUENTIAL、EPHEMERAL、EPHEMERAL_SEQUENTIAL 实际应用zookeeper实现分布式进程监控 假设要监控多台服务器上的A程序运行状态，当发现有服务器上的A程序下线的时候，给管理员发短信，并且尝试重启A程序。 主要利用zk的临时节点和watcher监视器的特性 大致实现的思路 首先在A程序启动的时候在zookeeper的/monitor节点下创建一个临时节点，临时节点的名称可以用这个服务器的主机名或者ip信息，只要A程序一直正常运行，这个临时节点就会一直存在。 给zk的/monitor节点注册一个watcher监视器，监视monitor节点下面的所有子节点的变化情况，当有子节点变化的时候，获取到具体是哪一个子节点发生了变化，就知道是哪台机器上的A程序有问题了。 可以给管理员发短信，打电话，发邮件之类的，并且还可以实现对那一台服务器上的A程序进行重启。 zookeeper实现分布式共享锁 首先假设有两个线程，两个线程要同时到mysql中更新一条数据，对数据库中的数据进行累加更新。由于在分布式环境下，这两个线程可能存在于不同的机器上的不同jvm进程中，所以这两个线程的关系就是垮主机跨进程，使用java中的synchronized锁是搞不定的。 在这我们主要利用了zookeeper的临时有序节点的特性和watcher监视器。 我们认为最小的节点具备执行权，也就是获取到了锁。 大致思路如下： 当这两个线程去mysql更新数据之前，先到zookeeper的/locks(永久节点)下面注册一个临时有序节点，这样每个线程都注册了一个临时节点，两个临时节点肯定是有序的。 线程1：/locks/000000001 线程2：/locks/000000002 当每个线程注册完节点之后，需要尝试获取锁，这个时候，哪个节点最小，哪个线程就获取到锁，这个时候，线程2注册的节点最小，所以线程2 就获取到锁，执行更新数据库的代码，更新完成之后，删除自己注册的临时节点 同时线程1会判断自己不是最小的，所以就会监控比自己小1的那个节点，当发现那个节点消失的话，也就意味着它的节点就是最小的节点，获取锁，执行更新数据库的代码… 遇到过的问题 kafka集群无法正常工作，提示连不上zk集群 分析产生的原因：zk集群的某一个节点磁盘故障导致进程假死 恢复过程： 先把线上zk集群的一个正常节点转化为单节点运行，恢复kafka集群服务 重新部署一个新的zk集群 把老zk集群的数据恢复到新zk集群中，kafka集群切换新的zk集群","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"Python提高","slug":"2017-10-25 -Python提高","date":"2017-10-24T16:00:00.000Z","updated":"2019-09-05T07:43:11.983Z","comments":true,"path":"2017/10/25/2017-10-25 -Python提高/","link":"","permalink":"http://yoursite.com/2017/10/25/2017-10-25 -Python提高/","excerpt":"Python提高","text":"Python提高 Python操作文件 1234567891011121314151617181920212223# coding=utf8'''读文件操作'''def fun(): #此方式如果遇到错误就直接导致程序出错建立的连接无法关闭可以使用try也可以使用with #打开文件 r 模式只读 文件不存在就会报错 file = open(\"D:\\\\data\\\\pythontest.txt\",'r') for line in file: #不对文件进行去除操作的话会自动识别文件中的\\n然后执行换行 print (line) print (line.strip()) #关闭连接 file.close()def fun2(): # python 封装好的方式 不需要我们手动的关闭流会自动关闭 with open(\"D:\\\\data\\\\pythontest.txt\",'r') as fp: for line in fp: print (line.strip())if __name__ == \"__main__\": #fun() fun2() Python执行shell指令 1234567891011121314151617181920212223242526272829303132333435# coding=utf8import os, commands , sysdef fun(): # 执行Windows中的命令行指令 status = os.system(&quot;java -version&quot;) print (&quot;status:&quot;,status)def fun2(): &apos;&apos;&apos; 使用commands也可以执行该命令 并且可以获得命令的执行状态码以及执行的输出结果 status：状态码 output：输出内容 :return: &apos;&apos;&apos; (status , output) = commands.getstatusoutput(&quot;java -version&quot;) print (&quot;statsus: &quot;,status) print (&quot;output: &quot;, output)def fun3(): # len 可以返回集合中元素的个数 print(len(sys.argv)) # 获取集合中第一个元素 print(sys.argv[0]) # os._exit(1) 结束程序，返回指定的状态码 if len(sys.argv) == 3: print(sys.argv[1]) print(sys.argv[2])if __name__ == &quot;__main__&quot;: fun() fun2() fun3() Python访问数据库 123456789101112131415161718192021222324252627# coding=utf8import MySQLdbdef qurey(): db = MySQLdb.connect(&quot;127.0.0.1&quot;,&quot;root&quot;,&quot;root&quot;,&quot;test&quot;) qucursor = db.cursor() sql = &quot;select * from student&quot; qucursor.execute(sql) lines = qucursor.fetchall() for line in lines: print (line)if __name__ == &quot;__main__&quot;: qurey() E:\\python2.7\\python.exe E:/ideapython/PythonDemo/DBTest.py(1L, &apos;zs&apos;, 18L)(2L, &apos;ls&apos;, 19L)(3L, &apos;ww&apos;, 20L)(4L, &apos;qq&apos;, 18L)(5L, &apos;aa&apos;, 15L)(6L, &apos;bb&apos;, 17L)(7L, &apos;cc&apos;, 25L)Process finished with exit code 0 Python接口开发 打开Tomcat 在E:\\apache-tomcat-8.5.32\\webapps\\ROOT 目录下创建一个简单的测试文本文件 进去到Tomcat的bin目录下找到startup.bat脚本文件执行 在本地浏览器中进入 localhost：8080/test.txt 查看是否能获取到值 使用代码操作获取 12345678910111213141516171819202122232425262728# coding=utf8import httplib,jsondef req(): url = \"http://localhost:8080/test.txt\" # 获取连接 conn = httplib.HTTPConnection(\"localhost\",\"8080\") # 执行get请求 conn.request(\"GET\",url) # 获取相应内容 res = conn.getresponse() # 读取相应内容 data = res.read() # 把获取到的json字符串转成json对象 loads = json.loads(data) get = loads.get(\"name\", \"defautname\") print (get)if __name__ == \"__main__\": req() E:\\python2.7\\python.exe E:/ideapython/PythonDemo/HttpTest.pyzsProcess finished with exit code 0","categories":[{"name":"提高","slug":"提高","permalink":"http://yoursite.com/categories/提高/"}],"tags":[{"name":"Python Python提高","slug":"Python-Python提高","permalink":"http://yoursite.com/tags/Python-Python提高/"}]},{"title":"Python快速入门","slug":"2017-10-18-Python快速入门","date":"2017-10-17T16:00:00.000Z","updated":"2019-09-05T07:41:49.179Z","comments":true,"path":"2017/10/18/2017-10-18-Python快速入门/","link":"","permalink":"http://yoursite.com/2017/10/18/2017-10-18-Python快速入门/","excerpt":"Python快速入门Python简介","text":"Python快速入门Python简介 Python是著名的“龟叔”Guido van Rossum（吉多·范罗苏姆）在1989年圣诞节期间，为了打发无聊的圣诞节而编写的一个编程语言；1991年初，Python发布了第一个公开发行版。Python是用C编写的高级的、面向对象的、开放源代码的编程语言。 龟叔给Python的定位是“优雅”、“明确”、“简单”，所以Python程序看上去总是简单易懂，初学者学Python，不但入门容易，而且将来深入下去，可以编写那些非常非常复杂的程序。 现在Python是由一个核心开发团队在维护，Guido van Rossum 仍然占据着至关重要的作用，指导其进展。 Python的学习成本低，上手快，而且学习后的用处很多，也可以利用Python写出非常复杂的程序，以及复杂的算法等 Python是一门解释性编程语言，在实际开发中，如果是开发小型工具等可以优先使用Python，开发效率会明显优于Java等编译性语言 Python也可以开发web网站、http接口等 Python安装 现在Python常用的版本是2.x以及3.x。Linux中会自带Python大部分默认安装的是2.x的版本。 在官方网站下载需要的版本 https://www.python.org 在Linux系统中Python的默认安装目录是:/user/bin/python 下载安装完成以后需要进行环境变量配置就如同配置JDK一样 安装完毕以后再命令行下执行Python命令查看 我使用的是windows操作系统 可以直接在命令行里面输入代码进行操作 123print(\"Hello，World\")结果是：Hello，World 想退出Python命令行可以输入指令exit（）或者quit（） 我们使用idea作为Python的IDE，需要自行下载Python插件就可以，直接在idea里面搜索下载就可以，在这里就不演示了。 Python常用操作 Python的一些基础语法 没有大括号，直接使用缩进来代表逻辑的层次 单行注释 以#开头 多行注释使用三个单引号或者双引号都可以 ‘’‘ 注释 ’‘’ “”“ 注释 ”“” 注意：如果注释里面要写中文的话需要指定coding=utf8 否在在别人的IDE中会出现注释乱码的现象 Python是大小写敏感的 基本数据类型 与其他语言基本相似 整数、浮点数、字符串、布尔值 空值：none 转义符 \\n 换行符 \\t 制表符 r 表示禁止字符转义，即转义字符会被当作为普通字符解析 type(arg) 获取arg的数据类型 isinstance（arg,type） 判断arg是否是type这个类型的 例如 isinstance(1.22,int) 注意：bool类型的数据底层存储的是0，1如果是布尔值判断是否为int类型的返回值为true 字符串常用操作 123456789101112131415161718192021222324252627282930313233343536373839404142# coding=utf8\"\"\"string字符串测试\"\"\"str1 = \" Hello World \"# 所有字母转换为小写print (str.lower(str1))# 所有字母转换为大写print (str.upper(str1))str1 = \" \\tHello World \"# 删除字符串开头结尾的空格 默认会删除空白符（'\\n','\\t','\\r')print (str1.strip())# 只删除开头结尾的空格print (str1.strip(\" \"))str1 = \" Hello World \"# 分割字符串 查看分割结果的数据类型split = str1.split(\" \")print (split)print (type(split))# 使用指定字符对数据进行拼接arr = [\"1\",\"2\",\"3\",\"4\",\"5\"]joinstr = \"_\".join(arr)print (joinstr)# 使用+可以直接拼接两个字符串str2 = str1 + \"hhhhh\"print (str2)# str3 = str2 + 1 是错误的 不支持字符串直接+上int类型值拼接str3 = \"hello %d\"%(3)print (str3)E:\\python2.7\\python.exe E:/ideapython/PythonDemo/StrTest.py hello world HELLO WORLD Hello World Hello World['', 'Hello', 'World', '']&lt;type 'list'&gt;1_2_3_4_5 Hello World hhhhhhello 3Process finished with exit code 0 List常用操作 list是一种有序集合，可以添加和删除其中的元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# coding=utf8\"\"\"List测试\"\"\"# 创建一个listlist1 = [1, 2, 3, 2, 4, 5, 6]# 向末尾插入一个数据list1.append(10)print (list1)# 向指定索引位置添加数据list1.insert(0, 10)print (list1)# 删除元素 不加索引位置参数默认删除list列表最后一个list1.pop()print (list1)list1.pop(0)print (list1)# 删除第一次出现的 2 只会删除一个list1.remove(2)print (list1)# 替换某个位置的元素list1[0] = 100print (list1)# 计算某个元素出现的次数count = list1.count(3)print (count)# 将list列表中的数据倒置print (list1)list1.reverse()print (list1)# tuple和list类似 但是tuple初始化以后将不能够在进行修改tuple1 = tuple(list1)print (tuple1)# 计算tuple内某个元素出现的次数tcount = tuple1.count(100)print (tcount)E:\\python2.7\\python.exe E:/ideapython/PythonDemo/ListTest.py[1, 2, 3, 2, 4, 5, 6, 10][10, 1, 2, 3, 2, 4, 5, 6, 10][10, 1, 2, 3, 2, 4, 5, 6][1, 2, 3, 2, 4, 5, 6][1, 3, 2, 4, 5, 6][100, 3, 2, 4, 5, 6]1[100, 3, 2, 4, 5, 6][6, 5, 4, 2, 3, 100](6, 5, 4, 2, 3, 100)1Process finished with exit code 0 Dict常用操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# coding=utf8\"\"\"Dict测试\"\"\"# 创建一个Dict并赋值 Dict相当于是Java中的mapdict1 = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 100&#125;print (dict1)# 修改指定key的valuedict1['Bob'] = 60print (dict1)# 取出指定key的value 使用此方法如果可以不存在程序就会报错data = dict1['Bob']print (data)# 使用 dice.get()如果key不存在将会返回none值data2 = dict1.get('bob')print (data2)# 删除指定key的键值对 如果指定key不存在就会报错dict1.pop('Bob')print (dict1)# 删除第一个键值对并返回删除的元素data3 = dict1.popitem()print (data3)print (dict1)# 清理所有元素dict1.clear()print (dict1)d = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 'hello'&#125;print (d)# 判断对列中是否存在指定的keyres = d.has_key('bob')print (res)# 返回一个list，list中的元素是tuple类型的res2 = d.items()print (res2)# 返回一个包含所有key的listres3 = d.keys()print (res3)# 返回一个包含所有value的listres4 = d.values()print (res4)E:\\python2.7\\python.exe E:/ideapython/PythonDemo/DictTest.py&#123;'Bob': 75, 'Michael': 95, 'Tracy': 100&#125;&#123;'Bob': 60, 'Michael': 95, 'Tracy': 100&#125;60None&#123;'Michael': 95, 'Tracy': 100&#125;('Michael', 95)&#123;'Tracy': 100&#125;&#123;&#125;&#123;'Bob': 75, 'Michael': 95, 'Tracy': 'hello'&#125;False[('Bob', 75), ('Michael', 95), ('Tracy', 'hello')]['Bob', 'Michael', 'Tracy'][75, 95, 'hello']Process finished with exit code 0 Set常用操作 123456789101112131415161718192021222324252627282930# coding=utf8\"\"\"Set测试\"\"\"# 新建一个set set会自动删除重复的元素set1 = set([1, 2, 3, 4, 1])print (set1)# 向set中添加元素set1.add(5)print (set1)# set不能直接删除元素 需要将set转换为list修改以后再转为setl = list(set1)l[1] = 100set2 = set(l)print (set2)# 删除元素set1.remove(5)print (set1)# 清空元素set1.clear()print (set1)E:\\python2.7\\python.exe E:/ideapython/PythonDemo/SetTest.pyset([1, 2, 3, 4])set([1, 2, 3, 4, 5])set([1, 3, 100, 5, 4])set([1, 2, 3, 4])set([])Process finished with exit code 0 循环语句 123456789101112131415161718192021222324252627282930# coding=utf8\"\"\"循环的测试\"\"\"# for循环l = ['ok', 'yes', 'no']for i in l: print(i)d = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 'hello'&#125;for k,v in d.items(): print (k,v)# while 循环sum = 0n = 10while n&gt;0 : sum+=n n-=1print (sum)E:\\python2.7\\python.exe E:/ideapython/PythonDemo/XunHuanTest.pyokyesno('Bob', 75)('Michael', 95)('Tracy', 'hello')55Process finished with exit code 0 判断语句 12345678910111213141516171819202122232425262728# coding=utf8\"\"\"判断测试\"\"\"age = 20if age &gt; 18 : print (\"已经成年了\")if age &gt;= 18 : print (\"已经成年了\")else: print (\"还未成年\")if age &lt; 18 : print (\"还未成年\")elif age &lt; 40 : print (\"还年轻\")else: print (\"老了\") E:\\python2.7\\python.exe E:/ideapython/PythonDemo/IfElseTest.py已经成年了已经成年了还年轻Process finished with exit code 0 函数 1234567891011121314151617181920212223242526272829303132333435363738# coding=utf8\"\"\"函数的使用\"\"\"'''1：使用def定义函数2：指定函数名称3：指定函数参数(如果有的情况下)4：指定函数体的执行逻辑5：return返回值(如果需要返回的情况)'''# 一般函数def fun(x): x += 1 return xres = fun(1)print (res)# 空函数def fun2 (): pass# 多返回值函数def funs(y): return y, y+1# 含有默认参数值的参数，如果不传入参数的话就会按默认值# 后面不可以跟不带默认参数值的参数def defalut(x=1): return x# 在参数前面加*表示该参数为可变参数，他需要放在最后一个def fun5(y,*x): return x,yres2 = fun5(1,2,3,4,5)print (res2)E:\\python2.7\\python.exe E:/ideapython/PythonDemo/DefTest.py2((2, 3, 4, 5), 1)Process finished with exit code 0 异常处理 1234567891011121314151617181920212223242526# coding=utf8\"\"\"异常处理\"\"\"def fun(): try: print (\"打开链接\") print (\"业务代码1\") print (\"业务代码2\") print (\"提交任务\") except Exception as e: print (\"exception %s\"%e) finally: print (\"关闭链接\")# Python脚本执行的入口 相当于Java中的main函数if __name__ == \"__main__\": fun() E:\\python2.7\\python.exe E:/ideapython/PythonDemo/ExceptionTest.py打开链接业务代码1业务代码2提交任务关闭链接Process finished with exit code 0 切片【List、tuple】 12345678910111213141516171819202122# coding=utf8\"\"\"切片功能\"\"\"def func(): li = [1, 2, 3, 4, 5, 6] # 指定索引左闭右开 print(li[2:5]) # 不指定索引 切开所有数据 print(li[::])if __name__ == \"__main__\": func()E:\\python2.7\\python.exe E:/ideapython/PythonDemo/QiePian.py[3, 4, 5][1, 2, 3, 4, 5, 6]Process finished with exit code 0 迭代【List、tuple、dict、set】 即for循环迭代取值代码参考循环中的for循环 面向对象 创建类 1234567891011121314151617181920212223242526# coding=utf8'''创建一个Person的实体类'''class Person: # 定义一个name属性 name = '' def __init__(self,_name): \"\"\" 构造函数 :param _name: \"\"\" self.name = _name def hello(self): print (\"hello %s\"%self.name)if __name__ == \"__main__\": # 实例化 不需要new p = Person(\"zs\") p.hello() E:\\python2.7\\python.exe E:/ideapython/PythonDemo/Person.pyhello zsProcess finished with exit code 0 Python模块 Python 模块(Module)，是一个 Python 文件，以 .py 结尾，包含了 Python 对象定义和Python语句。模块让你能够有逻辑地组织你的 Python 代码段，把相关的代码分配到一个模块里能让你的代码更好用，更易懂，模块能定义函数，类和变量，模块里也能包含可执行的代码。 Python本身就内置了很多很好用的模块，只要正常安装，这些模块就可以正常使用 在Python中有很多第三方的模块，通过管理工具pip使用 一般来说，第三方库都会在Python官方的pypi.python.org网站注册，要安装一个第三方库需要知道库的名称，就可以在官网搜索。类似于Java中的maven 本地模块的引用 引用其他包里的类 首先被引用的包里必须有init.py文件 然后使用指令 import 包地址 as 别名 也可以使用 from 包地址 import 需要使用的方法 注意：当一个文件被引用的时候会进行一次编译，生成一个.pyc的文件，主要是为了加快程序的运行速度 编译以及反编译 Python程序，是把原始程序代码放在.py文件里，而Python会在执行.py文件的时候将.py形式的程序编译成中间式文件（byte-compiled）的.pyc文件，这么做的目的就是为了加快下次执行文件的速度。 所以，在我们运行python文件的时候，就会自动首先查看是否具有.pyc文件，如果有的话，而且.py文件的修改时间和.pyc的修改时间一样，就会读取.pyc文件，否则，Python就会读原来的.py文件。 其实并不是所有的.py文件在与运行的时候都会产生.pyc文件，只有在import xxx.py文件的时候，才会生成相应的xxx.pyc文件","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"Python Python简介","slug":"Python-Python简介","permalink":"http://yoursite.com/tags/Python-Python简介/"}]},{"title":"Shell","slug":"2017-09-15-Shell","date":"2017-09-14T16:00:00.000Z","updated":"2019-09-05T07:39:47.049Z","comments":true,"path":"2017/09/15/2017-09-15-Shell/","link":"","permalink":"http://yoursite.com/2017/09/15/2017-09-15-Shell/","excerpt":"Shell","text":"Shell shell介绍 Shell是用户与Linux系统进行操作沟通的桥梁 shell的种类有很多 ，介绍的这种是bash 也就是Bourne Again Shell shell文件的后缀通常是 .sh shell脚本中的第一行通常是 #！bin/bash 脚本执行方式 a.sh 已经配置过环境变量 且脚本有足够的权限 bash a.sh 直接执行 bash -x a.sh单步执行 bash -n a.sh 语法检查 shell中的变量 变量不需要声明，初始化也不需要指定数据类型 变量的命名规范 变量名中只能含有数字、字母、下划线，不能以数字开头 严格区分大小写 变量的赋值使用=号，变量等号赋值之间不能有空格 显示变量 echo ${变量名} 本地变量 只对当前的shell进程有效 相当于是Java中的private 定义 ：aa=value 引用 : ${aa} 取消该变量 ： unset aa 环境变量 自定义的环境变量只对当前的shell进程以及他的子shell进程有效 对其他shell进程无效 定义 ： export aa=vlaue 如果想要对所有的shell进程都有效的话需要修改系统的环境变量 vi /etc/profile source /etc/profile 和windows中的环境变量类似 位置变量 ‘位置变量 $1$2$3$4 在函数执行的时候穿的变量 test.sh aa bb cc dd $0 表示的是脚本自己 $1 表示第一个参数 ….. 相当于Java中的args参数 特殊变量 $ ? 接收上一个命令返回的状态码 在0-255之间 $# 参数的个数 $* $@ 表示所有参数 $$ 获取当前shell脚本的进程号 引号 ‘’ 单引号不解析变量 “” 双引号解析变量 单引号包双引号不解析 双引号包单引号解析 shell中的循环以及判断 for循环 123456789101112131415第一种格式for (i=0;i&lt;10;i++)do .....done第二种格式for (i in 0 1 ... 10)do ......done第三种格式for (i in &#123;1..10&#125;)do ......done 条件测试 整型 gt 大于 lt 小于 ge 大于等于 le 小于等于 eq 等于 ne 不等于 字符串 = 等于 ！= 不等于 while循环 1234while [$aa -gt $bb ];do .....done if条件判断 12345678910111213141516171819202122232425单分支if [$aa -gt $bb ];then ......fi双分支if [$aa -gt $bb ];then ......else ......fi多分支if [$aa -gt $bb ];then ......elif [$aa -gt $bb ];then .......elif [$aa -gt $bb ];then .......else ......fi shell中的字符串 获取字符串的长度 1$&#123;#aa&#125; 字符串的截取 1$&#123;aa:offset:length&#125; 字符串的替换 /替换一个 //替换所有 12$&#123;aa/old/new&#125;$&#123;aa//old/new&#125; 取尾部指定个数的字符 不加负号是取除了前n个的所有 1$&#123;aa: -5&#125; 大小写转换 ^^转大写 ，，转小写 12$&#123;aa^^&#125;$&#123;aa,,&#125; shell扩展 算术运算 let var=算数表达式 let var=10+10 let var=$[算数表达式] let var2=$[var+10] let var=$((算数表达式)) date 输出当前时间 格式化输出 date +%Y-%m-%d date +%s 表示自1970-01-01 00:00:00以来的秒数 指定时间输出 date –date=’2009-01-01 11:11:11’ 指定时间输出 date –date=’1 days ago’ date -d’1 days ago’ 获取指定日期的前一天 date -d’20181212 1 days ago’ +%Y-%m-%d read 接收键盘的输入或者其他文件的描述的输入 read var read如果后面不指定变量，那么read命令会将接收到的数据放置在环境变量REPLY中 read -p “Enter your name:”var 加入提示文本 read -s -p “Enter your password:” pass 表示键盘输入时字符不显示 read-t 5 -p “enter your name:” var 指定键盘输入字符数量 后台模式运行脚本 在脚本后面加一个 &amp;字符 test.sh &amp; 后台运行 当用户注销或者网络断开的时候会受到hangup信号关闭所有子进程 nohup test.sh 忽略挂断信号 使用nohup test.sh &amp; 来启动一个需要一直执行的脚本 标准输入输出错误重定向 标准输入、输出、错误输出可以使用0、1、2表示 标准加重定向 &gt; 重定向到某一个文件中 &gt;&gt; 也是重定向但是文件信息是追加的 标准正确输出加重定向 ls 1&gt;a.txt 将ls命令的正确输出内容保存到a.txt当中 1可以省略 lk 2&gt;&gt;a.txt lk是一个错误指令 他会输出报错信息 将它的信息追加到a.txt之中 如果用 ls 2&gt;a.txt 或者 lk 1&gt;a.txt 都是无效的 将正确与错误的标准输出信息都存在同一个文件中 ls 1&gt;&gt;a.txt 2&gt;&amp;1 把信息重定向到一个无底洞里，相当于是直接把输出信息删除 ls &gt; /dev/null crontab定时器 定时执行指定任务 vi /etc/crontab 前面是修改执行时间 中间是用户名 后面是具体需要执行的命令 tail -f /var/log/cron 查看crontab的执行日志 service crond status ps 以及 jps ps -ef查看所有进程 jps查看Java进程 vi常用技巧 /String 查找某个字符 n下一个 ：Num 查找某一行 yy 复制当前行 num yy 从当前行开始复制num行 p 粘贴 dd 删除当前行 999 dd 删除当前跟后面的所有 G 跳转到最后一行 gg 跳转第一行 假死问题 在命令行中 按 Ctrl+S可能会造成假死状态 命令无法输入 按 Ctrl+q即可退出 awk awk是一个强大的文本分析工具，相对于 grep的查找，sed的编辑，awk在其对数据分析形成报告时尤为强大。 简单来说awk就是将文本逐行读入，以空格为默认分隔符将每行切片，切开的部分在进行分析处理 awk有三个版本 awk nawk gawk awk程序的报告生成能力通常用来从大文本文件中提取数据元素并将它们格式化成可读的报告。最完美的例子是格式化日志文件。awk程序允许从日志文件中只过滤出你想要看的数据 命令格式 awk [options] program file options 选项 program 程序 file 操作的文件 选项 描述 -F fs 指定行中的程序分割字段的分隔符 -f file 指定程序脚本文件 awk的基本特性之一即是处理文本文件的能力，他会自动为每个数据元素分配一个变量 $0 代表整个文本行 $1代表文本行中的第一个数据字段 $2代表文本行中的第二个数据字段 $n代表文本行中的第n个数据字段 注意：文本行中的文本是自动划分的 awk默认的分割符是任意的空白字符串例如：空格或者制表符 如果想要读取其他分隔字段，可以使用-F选项指定 实例：awk -F： ‘{print $2}’ test.txt 指定分隔符为： 如果awk只能进行一行命令的分析就没有那么大的用处了，所以awk给我们提供了有个方法，将多条编程语言放在一个文件中 然后使用-f指定这个文件执行 这个文件称为是awk脚本 实例：awk -F: -f script /etc/passwd awk中的各种begin，end begin：有时候需要在处理数据之前运行脚本，比如给数据添加一个开头 end：允许你在指定一个程序脚本，在awk处理完数据的时候执行它 awk的内置变量吧 FS：Field Seperator, 输入时的字段分隔符 awk ‘BEGIN{FS=”:”}{print $2,$5}’ test.txt RS：Record Seperator，输入行分隔符 OFS：Output Field Seperator, 输出时的字段分隔符; ORS:Outpput Row Seperator, 输出时的行分隔符； NF：Numbers of Field，字段数量 awk ‘BEGIN{FS=”:”}{print $1,NF}’ test.txt 整行的字段数量 NR：Numbers of Record, 行号；所有文件的一并计数； FNR：行号；各文件分别计数； awk中的匹配操作符 正则表达式需要放在/expr/中，/expr/必须出现在它要控制的程序脚本的左花括号前。 awk ‘/110.52.250.126/ {print $1}’ access_2013_05_30.log 匹配操作符允许将正则表达式限定在数据行中的特定数据字段。 awk ‘ ($1 ~ /110.52.250.126/) {print $1}’ access_2013_05_30.log awk ‘ ($1 !~ /110.52.250.126/) {print $1}’ access_2013_05_30.log 扩展 shell中的管道 command1 | command 2 他可以将command1输出的数据当做是2的输入数据进行处理 wc -l 统计行数 uniq -c在输出行的前面加上输出文件出现的次数 uniq -u 仅仅显示不重复的行 sort -nr n 依照数值的大小排序 r 反序 k 按照某一列进行排序 head -3 取出前三个 统计日志练习统计pv、uv 统计访问次数最多的三个IP 日志文件链接： 12345678910//清洗不符合规定的数据awk '($7 !~ /\\.css|\\.gif|\\.js|\\.png|\\.jpg/ )&#123;print $0&#125;' access_2013_05_30.log &gt;&gt;clean.log//我们粗略的认为清洗过后的条数就可以是pvwc -l clean.log 使用shell的方式统计awk 'BEGIN&#123;PV=0&#125; &#123;PV++&#125; END&#123;print PV&#125;' clean.log 使用awk的方式统计awk 'END&#123;print NR&#125;' clean.log 使用awk的方式统计//统计uvawk '&#123;print $1&#125;' clean.log |sort -n |uniq -u | wc -l//统计排名前三的IDawk '&#123;print $1&#125;' clean.log | sort -n | uniq -c | sort -nr -k 1 | head -3 sed sed -i ‘s/aaa/bbb/g’ test.conf i 表示对文件直接进行修改 s 表示操作是替换字符 aaa 表示的是源字符 bbb 表示的是替换的字符 g 表示对文件中所有该字符进行替换操作","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Linux","slug":"2017-08-21 -Linux","date":"2017-08-20T16:00:00.000Z","updated":"2019-09-05T07:39:16.558Z","comments":true,"path":"2017/08/21/2017-08-21 -Linux/","link":"","permalink":"http://yoursite.com/2017/08/21/2017-08-21 -Linux/","excerpt":"Linux","text":"Linux 简介 是一个免费开源的操作系统 组成部分 内核 shell 文件系统 应用程序 重要指令 常用指令 pwd 列出当前全路径 ls ll ll -a 列出目录下所有文件 加a会列出隐藏文件 touch 创建一个空文件 mkdir -p 创建目录 加p是递归创建目录 且若目录存在不报错 mv old new 重命名 连接 硬链接 ln 文件 连接 直接复制 软连接 ln -s 只是多加了一个快捷方式 cd 切换目录 rm -rf 删除文件 r 目录 f 强制删除 cp -r old new 复制文件 r文件夹 scp -rq 跨主机 q 不显示复制细节 chmod u+x xxx.sh 添加权限 777 最高权限 -r递归 cat -b 查看 -b带行号 more 分屏显示 tar zxcf zxvf 压缩、解压 z 是否同时具有 gzip 的属性？亦即是否需要用 gzip 压缩？ c 创建一个压缩文件的参数指令(create 的意思)； x 解开一个压缩文件的参数指令！ v 压缩的过程中显示文件！ f 使用档案名字，这个参数是最后一个参数，后面只能接档案名 重要指令 du 同价文件文件夹所占磁盘情况 a 文件全部以及子目录下得文档等所有文件 h 文件全部以及子目录的所有文件 ch 添加total sh 直接统计大小 vi dd 删除一行 yy p 复制粘贴 999 dd 删除所有 i 进入编辑模式 ：wq 保存退出 ：q! 不保存强制退出 /abc 查看目标所在位置 n下一个 | grep 管道加过滤 wc 统计 l 行数 w 单词数 c 字符数 啥也不加统计所有 echo 输出 双引号不解析 单引号解析 -e解析转义字符 export -p 列出当前所有环境变量 也可直接跟变量来临时设置环境变量 history 显示历史使用的命令 加数字显示条数 c 清除 w 保存至文件 ps -ef 查看正在运行的进程 netstat -anp 监听的端口号 kill 加进程ID -9 强制杀死 防火墙 临时 service iptables stop/start/status 关闭打开状态 永久 chkconfig iptables off/on 关闭打开 –list iptables 状态 top 显示各种资源占用率 P 按进程的CPU使用率排序 M 按进程的内存使用率排序 df -h 查看硬盘占用情况 who显示在线用户 uname -a 查看操作系统的版本 free 查看内存状态 clear 清屏 shutdown -h now 关机 reboot -h now 重启 exit quit 退出当前状态","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Maven 简介","slug":"2017-07-21 -maven的简介","date":"2017-07-20T16:00:00.000Z","updated":"2019-09-05T08:01:54.088Z","comments":true,"path":"2017/07/21/2017-07-21 -maven的简介/","link":"","permalink":"http://yoursite.com/2017/07/21/2017-07-21 -maven的简介/","excerpt":"Maven 简介","text":"Maven 简介 为什么需要maven 同样的代码要在不同的机器上运行他所需要的依赖可以放在maven仓库 项目组加入新成员可以快速的配置好环境 在开发其他项目的时候需要用到跟之前项目开发一样的jar包 maven是什么 maven是基于项目对象模型POM的软件项目管理工具 是可以跨平台的，主要服务基于Java平台的仙姑构建、依赖管理、项目信息管理等 构建的过程 清理 编译 测试 报告 打包 部署 maven的工程结构 src mian java – 存放Java的文件 源代码等 resource –存放资源文件 比如 spring，hibernate等的配置文件 test Java – 存放所有的.Java的测试文件，比如JUnit 测试类 resource –测试的资源文件夹 target —目标文件的输出位置比如jar包、war包等 pom.xml —maven的项目核心配置文件 maven常用命令 mvn compile 执行编译 会将生成文件存放在target目录中 mvn clean 删除target中的目录文件 mvn test 执行测试命令 执行后会在target目录中生成三个目录文件surefire、surefire-reports（测试报告）、test-classes（测试的字节码文件） mvn package 进行打包操作 操作后的文件存放在target目录之中 例如jar包war包 mvn install 将制定的jar包安装到本地仓库以便于其他工程的引用 mvn clean compile 清除测试类再执行compile执行编译操作 mvn clean test 先清除在进行test测试操作 mvn clean package 先执行clean清除在执行package打包 mvn clean install 先进行clean在执行install","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"}]}]}