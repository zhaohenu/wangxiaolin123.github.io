{"meta":{"title":"Zhao","subtitle":"waiting","description":"技术和生活，融为一体","author":"赵鑫涛","url":"http://yoursite.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-08-21T07:59:02.711Z","updated":"2019-08-21T06:37:13.678Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":"404 Not Found **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2019-08-24T05:33:04.340Z","updated":"2019-08-24T05:33:04.340Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"一个努力的程序猿，在线分享自己学习以及工作的心得，希望能与大家一起交流，共同进步！"},{"title":"projects","date":"2019-08-24T05:48:58.982Z","updated":"2019-08-24T05:48:58.982Z","comments":true,"path":"projects/index.html","permalink":"http://yoursite.com/projects/index.html","excerpt":"","text":"Spider简单爬虫 基于Java的简单爬虫程序详情请点击：https://github.com/zhaoxintaoaa/Spider"},{"title":"Friends","date":"2019-08-24T06:15:51.896Z","updated":"2019-08-24T06:15:51.896Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2019-08-21T08:53:32.192Z","updated":"2019-08-21T06:39:15.328Z","comments":true,"path":"blog/categories/index.html","permalink":"http://yoursite.com/blog/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-08-21T07:59:15.792Z","updated":"2019-08-21T06:40:20.856Z","comments":true,"path":"blog/tags/index.html","permalink":"http://yoursite.com/blog/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-21T11:33:39.255Z","updated":"2019-08-21T11:33:39.255Z","comments":true,"path":"blog/mylist/index.html","permalink":"http://yoursite.com/blog/mylist/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-21T11:36:09.246Z","updated":"2019-08-21T11:36:09.246Z","comments":true,"path":"blog/archives/index.html","permalink":"http://yoursite.com/blog/archives/index.html","excerpt":"","text":""}],"posts":[{"title":"Elasticsearch Head Plugin","slug":"Elasticsearch Head Plugin 详细安装教程","date":"2019-04-02T03:25:46.000Z","updated":"2019-08-22T12:32:51.470Z","comments":true,"path":"2019/04/02/Elasticsearch Head Plugin 详细安装教程/","link":"","permalink":"http://yoursite.com/2019/04/02/Elasticsearch Head Plugin 详细安装教程/","excerpt":"Elasticsearch Head Plugin 详细安装教程Elasticsearch Head Plugin站点插件可以以网页形式展现ES","text":"Elasticsearch Head Plugin 详细安装教程Elasticsearch Head Plugin站点插件可以以网页形式展现ES 注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt nodejs下载地址https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz 复制此链接就可以直接下或者https://nodejs.org/dist 使用这个两节找适合自己的版本 因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压 如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz 123解压：xz -d node-v10.15.3-linux-x64.tar.xztar -xvf node-v10.15.3-linux-x64.tar 创建软连接 12ln -s /data/soft/node-v10.15.3-linux-x64/bin/node /usr/bin/nodeln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm 设定nodejs安装软件的dialing服务器 1npm config set registry https://registry.npm.taobao.org 安装grunt 12npm install -g grunt npm install -g grunt-cli 创建软连接 1ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt /usr/bin/grunt 安装phantomjs 下载地址http://phantomjs.org/download.html 解压 12bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2tar -xvf phantomjs-2.1.1-linux-x86_64.tar 创建软连接 1ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/bin/phantomjs 安装依赖软件 1yum -y install wget fontconfig 安装head插件 下载地址：https://github.com/mobz/elasticsearch-head/archive/master.zip 解压 需要使用unzip命令 下载unzip 1yum install -y unzip 解压 1unzip elasticsearch-head-master.zip 然后cd进入到解压的文件中 安装两个插件 12npm audit fixnpm audit fix --force 执行安装命令安装head 1sudo npm install 启动之前修改Gruntfile.js文件，增加hostname参数 1234567vi Gruntfile.jsoptions: &#123; hostname: &apos;hadoop100&apos;, port: 9101, base: &apos;.&apos;, keepalive: true&#125; 启动服务 1grunt Server 启动服务后还需要修改Elasticsearch中的一些配置 123vi config/elasticsearch.ymlhttp.cors.enabled: true http.cors.allow-origin: &quot;*&quot; 修改完成后重启ES 进入网址http://hadoop100:9101/可以看到以下信息就说明插件安装成功了","categories":[{"name":"安装部署","slug":"安装部署","permalink":"http://yoursite.com/categories/安装部署/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch的安装部署","slug":"2019-04-2-Elasticsearch的安装部署","date":"2019-04-02T02:35:11.000Z","updated":"2019-08-22T12:32:58.445Z","comments":true,"path":"2019/04/02/2019-04-2-Elasticsearch的安装部署/","link":"","permalink":"http://yoursite.com/2019/04/02/2019-04-2-Elasticsearch的安装部署/","excerpt":"Elasticsearch安装部署 安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了）","text":"Elasticsearch安装部署 安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了） 下载Elasticsearch 网址：https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3 选择合适的版本下载就可以 下载完成以后上传到Linux系统中，解压 进入到解压后的文件中尝试进行开启 bin/elasticsearch （-d 后台运行） 注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off) 会发现执行报错 就是不可以在root用户下打开，选择一个其他用户就可以了 修改配置变量 修改Elasticsearch中config变量 （vi config/elasticsearch.yml) 增加两行代码 12bootstrap.system_call_filter: false network.host: 192.168.32.110 //后面的的IP设置你自己的本地IP就可以 修改Linux的配置变量 12345678910vi /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096vi /etc/sysctl.confvm.max_map_count=262144vi /etc/security/limits.d/90-nproc.conf 把1024修改为4096* soft nproc 4096 修改完配置以后重启操作系统在启动就可以啦 启动之后出现以下情况 没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了 然后可以使用web界面查看 在浏览器输入 http://yourip:9200 就可以查看了","categories":[{"name":"安装部署","slug":"安装部署","permalink":"http://yoursite.com/categories/安装部署/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Storm高级以及优化","slug":"Storm高级","date":"2019-04-01T10:32:45.000Z","updated":"2019-08-21T11:45:50.545Z","comments":true,"path":"2019/04/01/Storm高级/","link":"","permalink":"http://yoursite.com/2019/04/01/Storm高级/","excerpt":"Storm高级Storm核心之流分组 stream grouping 分类","text":"Storm高级Storm核心之流分组 stream grouping 分类 Shuffle Grouping：随机分组。将stream中的tuple缓存后随机发放给所有bolt，可以使每个bolt中的数据量大致相等（可以较好的实现负载均衡） Fields Grouping：按字段分组，例如按groupID字段进行分组，将同一个分组的tuple分到统一任务中 All Grouping:广播发送，每一个tuple都会发送到所有任务中，所以每一个bolt都会有所有的tuple Global Grouping：全局分组，这个tuple会被分配到storm中的某一个bolt,具体一点就是分配到ID值最小的一个bolt之中 Non Grouping：随机分派，效果和shuffle一样 Direct Grouping：直接分组，将tuple发送给制定好的任务中 localOrShuffleGrouping：指如果目标Bolt 中的一个或者多个Task 和当前产生数据的Task在同一个Worker 进程里面，那么就走内部的线程间通信，将Tuple 直接发给在当前Worker进程的目的Task。否则，同shuffleGrouping。 Storm可靠性剖析Storm可能出现的问题 worker进程死掉 supervisor进程死掉 nimbus进程死掉 节点宕机 解决方案 (acker机制)ack/fail消息确认机制(确保一个tuple被完全处理) 在spout中发射tuple的时候需要同时发送messageid，这样才相当于开启了消息确认机制 如果你的topology里面的tuple比较多的话，那么把acker的数量设置多一点,效率会高一点。 通过config.setNumAckers(num)来设置一个topology里面的acker的数量，默认值是1。 注意：acker用了特殊的算法，使得对于追踪每个spout tuple的状态所需要的内存量是恒定的（20 bytes) 注意：如果一个tuple在指定的timeout(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS默认值为30秒)时间内没有被成功处理，那么这个tuple会被认为处理失败了。 Storm定时器分析 可以指定每隔一段时间将数据整合一次存入数据库 在main中设置conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, 60);// 设置本Bolt定时发射数据 在bolt中使用下面代码判断是否是触发用的bolt tuple.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID) StormUI的详解 deactive：未激活(暂停) emitted:emitted tuple数 与emitted的区别：如果一个task，emitted一个tuple到2个task中，则transferred tuple数是emitted tuple数的两倍 completelatency: spout emitting 一个tuple到spout ack这个tuple的平均时间(可以认为是tuple以及该tuple树的整个处理时间) processlatency: bolt收到一个tuple到bolt ack这个tuple的平均时间，如果没有启动acker机制，那么值为0 execute latency：bolt处理一个tuple的平均时间，不包含acker操作，单位是毫秒(也就是bolt执行 execute 方法的平均时间) capacity：这个值越接近1，说明bolt或者spout基本一直在调用execute方法，说明并行度不够，需要扩展这个组件的executor数量。(调整组件并行度的依据) 总结：execute latency和proces latnecy是处理消息的时效性，而capacity则表示处理能力是否已经饱和，从这3个参数可以知道topology的瓶颈所在。 Storm的优化并行度的优化 worker为storm提供工作进程，程序的并行度可以设置（包括spout和bolt的并行度，如果有acker的话还包括acker的并行度），并行度即为executor的数目。 一般情况下worker与executor的比例是一比十到十五，也可以根据实际需要修改。 worker的优化 CPU 16核，建议配置20个worker。CPU 24或32核，30个worker 默认情况下，Storm启动worker进程时，JVM的最大内存是768M，可以通过在Strom的配置文件storm.yaml中设置worker的启动参数worker.childopts: “-Xmx2048m” 一个topology使用的worker数量，12个是比较合理的，这个时候吞吐量和整体性能最优。如果多增加worker进程的话，会将一些原本线程间的内存通信变为进程间的网络通信。 acker优化 如果可靠性对你来说不是那么重要，那么你可以通过不跟踪这些tuple树来获取更好的性能。不去跟踪消息的话会使得系统里面的消息数量减少一半，因为对于每一个tuple都要发送一个ack消息。 三种去掉可靠性的方法 第一是把config.setNumAckers(0)设置为0，在这种情况下，storm会在spout发射一个tuple之后马上调用spout的ack方法。也就是说这个tuple树不会被跟踪。 第二个方法是在tuple层面去掉可靠性。你可以在发射tuple的时候不指定messageid来达到不跟踪spout中tuple的目的。 最后一个方法是如果你对于一个tuple树里面的某一部分到底成不成功不是很关心，那么可以在发射这些tuple的时候unanchor它们(anchor是锚定的意思，unanchor表示不把当前这个tuple包含到tuple树中，也就是说不跟踪这个消息了)。这样这些tuple就不在tuple树里面， 也就不会被跟踪了。 雪崩问题的出现原因以及解决方法 原因：spout发送的速度大于bolt接收的速度，导致数据堆积，不断消耗内存，最终系统崩溃，并引起数据链上多节点down掉。 解决方案 增加bolt的并行度 增加它接收的速度 可以通过topology.max.spout.pending来控制spout发送消息的速度，通过代码这样设置config.setMaxSpoutPending(num); 注意：这个参数表示，当下游的bolt还有topology.max.spout.pending个 tuple 没有消费完时，spout会停止调用nexttuple方法发射数据。等待下游bolt去消费，当tuple的个数少于topology.max.spout.pending个数时，spout 会继续发射数据(这个属性只对可靠消息处理有用，也就是说需要启用acker消息确认机制，在spout中emit数据的时候需要带有messageid)","categories":[],"tags":[{"name":"storm storm优化","slug":"storm-storm优化","permalink":"http://yoursite.com/tags/storm-storm优化/"}]},{"title":"Elasticsearch简介","slug":"Elasticsearch简介","date":"2019-04-01T05:10:12.000Z","updated":"2019-08-22T12:32:47.857Z","comments":true,"path":"2019/04/01/Elasticsearch简介/","link":"","permalink":"http://yoursite.com/2019/04/01/Elasticsearch简介/","excerpt":"ElasticsearchElasticsearch简介​ Elasticsearch是一个实时分布式搜索和分析引擎。它对Lucene进行了封装。能够满足实时搜索的稳定、可靠、快速等。基于REST接口。","text":"ElasticsearchElasticsearch简介​ Elasticsearch是一个实时分布式搜索和分析引擎。它对Lucene进行了封装。能够满足实时搜索的稳定、可靠、快速等。基于REST接口。 ES与MySQL的对比 Elasticsearch MySQL index 索引库 database 数据库 type 类型 table 类型 document 文档 row 行 field 字段 column 列 Elasticsearch安装部署 安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了） 下载Elasticsearch 网址：https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3 选择合适的版本下载就可以 下载完成以后上传到Linux系统中，解压 进入到解压后的文件中尝试进行开启 bin/elasticsearch （-d 后台运行） 注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off) 会发现执行报错 就是不可以在root用户下打开，选择一个其他用户就可以了 修改配置变量 修改Elasticsearch中config变量 （vi config/elasticsearch.yml) 增加两行代码 12bootstrap.system_call_filter: false network.host: 192.168.32.110 //后面的的IP设置你自己的本地IP就可以 修改Linux的配置变量 12345678910vi /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096vi /etc/sysctl.confvm.max_map_count=262144vi /etc/security/limits.d/90-nproc.conf 把1024修改为4096* soft nproc 4096 修改完配置以后重启操作系统在启动就可以啦 启动之后出现以下情况 没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了 然后可以使用web界面查看 在浏览器输入 http://yourip:9200 就可以查看了 简单基本操作CURL简介 curl起始就是一个可以在命令行下访问URL的工具 curl可以利用URL语法在命令行的方式下操作开源的文件 这样即可以方便我们其他不同部门对我们数据库的操作，也方便我们管理数据库，方便管理其他用户的权限 CURL的简单操作 -x 是指定http请求的方法 他的类型有很多种包括 GET POST PUT DELETE 查询、修改、增加、删除等很多操作 -d 是指需要传递的参数 首先我们先创建一个简单的索引 curl -XPUT ‘http://localhost:9200/test/&#39; localhost一定要换成你之前设置的IP 这样它就为我们创建了test索引库 然后我们可以创建一个索引并为创建的索引添加一些内容然后进行一些列查询 123456curl -H &quot;Content-Type: application/json&quot; //—H指定添加内容的类型为json类型-XPOST http://localhost:9200/test/emp/1 //1是指定索引的IP 不加系统也会自动生成-d &apos;&#123;&quot;name&quot; : &quot;tom&quot;,&quot;age&quot; : 25&#125;&apos; //加入索引中的具体内容 查询我们刚刚创建的索引 1curl -XGET http://localhost:9200/test/emp/1?pretty 检索索引中的一部分内容 1curl -XGET &apos;http://localhost:9200/test/emp/1?_source=name&amp;pretty&apos; 查询指定索引库指定类型的所有数据 12curl -XGET http://hadoop110:9200/test/emp/_search?pretty 查看tem类型下的所有数据 对ES进行更新操作，ES中可以使用put或者post两种方式进行更新操作 执行更新操作的时候ES的操作细节 首先将旧的文件标记为删除状态 添加新的文件 旧文件不会立即消失但是我们看不见 ES在后续你添加更多文件的时候在后台清理掉标记为删除状态的文件 执行局部更新的操作 1curl -H &quot;Content-Type: application/json&quot; -XPOST http://hadoop110:9200/test/emp/1/_update -d &apos;&#123;&quot;doc&quot;:&#123;&quot;age&quot;:20&#125;&#125;&apos; 我们接着进行一次查询看数据是否已经更新 可以看到年龄已将改成20了 所以说明更新操作成功了 我们可以根据这个操作做很多事情 对ES进行删除操作 删除我们之前创建的索引 1curl -XDELETE http://hadoop110:9200/test/emp/1 删除以后我们在进行get获取操作就会报错说明我们的删除操作已经执行成功了 如果删除文档存在 则会返回：200 ok的状态码，found属性值为true，_version属性的值+1 如果想要删除的文件不存在就会返回：404 NotFound的状态码，found属性值为false，但是_version属性的值依然会+1，这个就是内部管理的一部分，它保证了我们在多个节点间的不同操作的顺序都被正确标记了 对ES进行批量操作 包括很多步的增删改查等 批量操作就是bulk API帮助我们同时执行多个操作 语法的格式： 123456action：index/create/update/delete //需要执行的操作类型metadata：_index,_type,_id //指定需要操作的索引的索引库、类型、ID等request body：_source(删除操作不需要) &#123; action: &#123; metadata &#125;&#125; //具体要执行的操作&#123; request body &#125;......... create与index的区别 在创建数据时，如果数据已存在 create会返回创建失败，文件已存在，但是index会执行成功 使用方法： 我们创建一个文件保存我们需要执行的操作 12345vi requests&#123; &quot;index&quot; : &#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;emp&quot;,&quot;_id&quot;:&quot;21&quot;&#125;&#125;&#123; &quot;name&quot; : &quot;test21&quot;&#125;执行：curl -H &quot;Content-Type: application/json&quot; -XPUT localhost:9200/test/emp/_bulk --data-binary @requests 出现下面结果表示执行成功了 我们可以放多条指令进去 同时执行多条指令但是要保证中间格式不出错 插件的介绍Elasticsearch Head Plugin站点插件可以以网页形式展现ES 注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt nodejs下载地址https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz 复制此链接就可以直接下或者https://nodejs.org/dist 使用这个两节找适合自己的版本 因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压 如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz 123解压：xz -d node-v10.15.3-linux-x64.tar.xztar -xvf node-v10.15.3-linux-x64.tar 创建软连接 12ln -s /data/soft/node-v10.15.3-linux-x64/bin/node /usr/bin/nodeln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm 设定nodejs安装软件的dialing服务器 1npm config set registry https://registry.npm.taobao.org 安装grunt 12npm install -g grunt npm install -g grunt-cli 创建软连接 1ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt /usr/bin/grunt 安装phantomjs 下载地址http://phantomjs.org/download.html 解压 12bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2tar -xvf phantomjs-2.1.1-linux-x86_64.tar 创建软连接 1ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/bin/phantomjs 安装依赖软件 1yum -y install wget fontconfig 安装head插件 下载地址：https://github.com/mobz/elasticsearch-head/archive/master.zip 解压 需要使用unzip命令 下载unzip 1yum install -y unzip 解压 1unzip elasticsearch-head-master.zip 然后cd进入到解压的文件中 安装两个插件 12npm audit fixnpm audit fix --force 执行安装命令安装head 1sudo npm install 启动之前修改Gruntfile.js文件，增加hostname参数 1234567vi Gruntfile.jsoptions: &#123; hostname: &apos;hadoop100&apos;, port: 9101, base: &apos;.&apos;, keepalive: true&#125; 启动服务 1grunt Server 启动服务后还需要修改Elasticsearch中的一些配置 123vi config/elasticsearch.ymlhttp.cors.enabled: true http.cors.allow-origin: &quot;*&quot; 修改完成后重启ES 进入网址http://hadoop100:9101/可以看到以下信息就说明插件安装成功了 配置参数详解 配置文件elasticsearch.yml ES已经为大多数的参数设置了合理的默认值 我们只需要在有特殊需求的时候进行修改 书写规范 属性顶格写，不能有空格 缩进一定要是用空格而不能使用制表符 属性与属性值之间必须有一个空格 常见的配置文件以及其含义 cluster.name: 集群名称 node.name 节点名称 path.data: /path/to/data es的数据存储目录 path.logs: /path/to/logs es的日志存储目录 bootstrap.memory_lock: true 锁定物理内存地址，防止elasticsearch内存被交换出去,也就是避免es使用swap交换分区中的内存 network.host: 192.168.0.1 为es设置ip绑定 http.port: 9200 为es设置自定义端口，默认是9200 discovery.zen.ping.unicast.hosts: [“host1”, “host2”] 当启动新节点时，通过这个ip列表进行节点发现，组建集群 discovery.zen.minimum_master_nodes: 通过配置这个参数来防止集群脑裂现象 (集群总节点数量/2)+1 gateway.recover_after_nodes: 3 一个集群中的N个节点启动后,才允许进行数据恢复处理，默认是1 action.destructive_requires_name: true 设置是否可以通过正则或者_all删除或者关闭索引库 核心概念 cluster 代表的是一个集群，集群中有很多节点，其中有一个主节点，这个主节点通过选举产生，主从节点时对于集群内部而言的。es有一个概念叫去中心化，就是说没有中心节点，这个是对于外部来说的，在外部来看，集群就是一个整体，我们两节集群中的任何一个节点与集群通信跟直接与集群通信是等价的。 主节点的主要职责就是负责管理集群的状态，包括管理分片以及副本的状态，以及节点的删除、新节点的发现等 注意：主节点不负责对进群的增删改查处理，只负责管理集群状态 shards 代表的是索引分片，ES将一个完整的索引分成多个分片，这样的好处是可以把一个大的索引分成多个分片后分布到不同的节点上，构成分布式搜索。提高性能和吞吐量 分片的的数量只能在创建索引库的时候指定，索引库创建以后不可以更改 索引库默认是5个分片 每个分片最多存储2,147,483,519条数据 1curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_shards&quot;:3&#125;&#125;&apos; replicas 代表的是分片的副本，es给分片设置副本是为了提高系统的容错性，当某个节点的某个分片损坏或者丢失了可以从副本中恢复。 提高es的查询效率，es会自动搜索并请求进行负载均衡 默认每个分区只有一个副本，主副本不会存在于一个节点之上，副本数量可以在创建索引库的时候进行设置吧 1curl -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_replicas&quot;:2&#125;&#125;&apos; recovery 代表数据的恢复或者数据的重新分布 es在所有节点的加入或者退出后会根据机器的负载对索引分片进行重新分配，挂掉的节点重启时也会进行数据恢复 ElasticsearchJavaAPI操作使用Java对ES进行操作 添加maven依赖 可以maven仓库中寻找适合你的版本 12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.4.3&lt;/version&gt;&lt;/dependency&gt; Java中对ES的简单增删改查操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package EsTest;import org.elasticsearch.action.delete.DeleteResponse;import org.elasticsearch.action.get.GetResponse;import org.elasticsearch.action.index.IndexResponse;import org.elasticsearch.action.update.UpdateResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.TransportAddress;import org.elasticsearch.common.xcontent.XContentType;import org.elasticsearch.transport.client.PreBuiltTransportClient;import java.net.InetAddress;import java.util.HashMap;/** * Es简单操作测试 */public class EsDemo1 &#123; public static void main(String[] args) throws Exception&#123; //给集群添加自动嗅探的功能 Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;) //集群名称 .put(&quot;client.transport.sniff&quot;, true) //开启自动嗅探功能，可以自动识别集群内的其他节点信息 .build(); //创建连接 TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) //可添加多个节点 //.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop100&quot;), 9300)) .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300)); //获取节点的信息 int size = client.connectedNodes().size(); //System.out.println(size); String index = &quot;test&quot;; String type = &quot;emp&quot;; //添加数据 使用json字符串 String json = &quot;&#123;\\&quot;name\\&quot;:\\&quot;jack\\&quot;,\\&quot;age\\&quot;:10&#125;&quot;; IndexResponse res = client.prepareIndex(index, type, &quot;1&quot;) .setSource(json, XContentType.JSON).get(); //System.out.println(res.toString()); //添加数据 使用map结构 HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;name&quot;,&quot;zs&quot;); map.put(&quot;age&quot;,21); IndexResponse res2 = client.prepareIndex(index, type, &quot;101&quot;) .setSource(map) .execute() .actionGet(); //更新操作 update UpdateResponse updateResponse = client.prepareUpdate(index, type, &quot;101&quot;).setDoc(&quot;&#123;\\&quot;age\\&quot;:18&#125;&quot;, XContentType.JSON).get(); //根据ID进行数据查询 GetResponse get1 = client.prepareGet(index, type, &quot;101&quot;).get(); System.out.println(get1.getSourceAsString()); //删除操作 delete DeleteResponse deleteResponse = client.prepareDelete(index, type, &quot;101&quot;).get(); System.out.println(deleteResponse.toString()); &#125;&#125;","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Strome 基础","slug":"Storm的详细分析","date":"2019-03-21T05:46:56.000Z","updated":"2019-08-21T11:45:52.807Z","comments":true,"path":"2019/03/21/Storm的详细分析/","link":"","permalink":"http://yoursite.com/2019/03/21/Storm的详细分析/","excerpt":"Storm的详细分析Storm人的概述 Storm是Twitter开源的一个实时处理框架 Storm能够实现高频数据和大规模数据的实时处理","text":"Storm的详细分析Storm人的概述 Storm是Twitter开源的一个实时处理框架 Storm能够实现高频数据和大规模数据的实时处理 Storm与MapReduce的区别Storm type MapReduce Storm 数据来源 hdfs上TB级别历史数据 实时新增的某一条数据 处理过程 map阶段和reduce阶段 可以有很多阶段包含spout以及bolt 是否会结束 执行完结束 不会结束 处理速度 主要以执行TB级别数据速度较慢 只处理新增数据速度很快 适用场景 处理批数据不讲时效性 处理新增数据将时效性 Spark Streaming与Storm的区别 type Spark Streaming Storm 计算模型 是近实时处理框架 全实时处理框架 延迟度 最高支持秒级别的延迟 可以支持毫秒级别的延迟 吞吐量 因为是批处理所以吞吐量高 吞吐量相对来说较低 动态调整并行度 不支持 支持 事务机制 支持但是不够完善 支持且完善 Storm各个组件解释 Topology：用于封装一个实时计算应用程序的逻辑 Stream：消息流，是一个没有边界的tuple序列，这些tuple会以分布式的方式进行创建以及处理 Spout：消息源，消息的生产者，会从外部源获取消息然后向Topology发出：tuple Bolt：消息处理者，消息的处理逻辑被封装到bolt之中，处理输入的数据然后产生新的输出数据流 Storm的设计思想 是对stream流的一个抽象即一个不间断的连续tuple 将流中的元素抽象为一个tuple，一个tuple就是一个值列表value list，list中的每个value都有一个name，并且这个value可以是很多数据类型例如基本类型、字符类型等 每一个stream流都有一个数据源，称为Spout stream从spout中获取不间断数据tuple需要经过处理。处理的过程就是stream流转换的过程称为bolt，bolt可以消费任意数量的流，它是将stream汇总的tuple挨个实时进行处理转换成一个新的stream流经过多个bolt处理就可以得到目标数据 spout+tuple+bolt这个过程可以称为是Topology拓扑。Topology是Storm中最高的一个抽象概念他可以被提交到集群中执行 Topology的每个节点都要指定他所发射数据的name，其他节点只需要订阅该name就可以接收数据进行处理 Topology的整个流程 如果将stream比作是一列火车的话 spout就是这列火车的始发站每一节车厢就是一个tuple乘客就是tuple中的values 中间的站点就相当于是bolt进行处理上下乘客终点站就相当于stream的目标数据 Storm的整体架构图 Storm的简单实例开发 需求：一个源源不断的数据1，2，3，4……求每出现一个数字就要计算出现的所有数字的和 开发过程 在IDE中创建maven工程 在pom中添加、Storm依赖 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;1.0.6&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package Storme;import java.util.Map;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.generated.StormTopology;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.apache.storm.utils.Utils;/** * 需求：实现数字累加求和 * 分析： * 需要有一个spout负责源源不断的产生从1开始的递增数字 * 还需要有一个bolt负责对spout产生的数据进行累加求和，并且把结果打印到控制台 * 最后把这个spout和bolt组装成一个topology * */public class WordCount &#123; /** * 实现自己的数据源spout， * 该spout负责源源不断产生从1开始的递增数字 * */ public static class MySpout extends BaseRichSpout&#123; private Map conf;//这里面存储配置信息 private TopologyContext context;//代表上下文 private SpoutOutputCollector collector;//收集器，主要负责向外面发射数据 /** * 是一个初始化的方法，这个方法在本实例运行的之后，首先被调用，仅且仅被调用一次 * 所以这个方法内一般放一些初始化的代码 * 例子：针对操作mysql数据的案例，使用jdbc获取数据库连接的代码需要放到这里面实现 */ public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; this.conf = conf; this.context = context; this.collector = collector; //System.err.println(&quot;spout------&quot;+conf.get(&quot;name&quot;)); &#125; /** * 这个方法会被循环调用 */ int i = 1; public void nextTuple() &#123; // 注意：针对需要发射的数据，需要封装成tuple，可以使用storm中的values对象快速封装tuple System.out.println(&quot;spout:&quot;+i); this.collector.emit(new Values(i++)); // 让线程每发射一条数据，休息1秒 Utils.sleep(1000); &#125; /** * 声明输出字段 * 定义两个组件之间数据传输的一个规则 * 注意：只要这个组件(spout/spout)向外发射了数据，那么这个declareOutputFields就需要实现 */ public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; // 注意：Fields中的字段列表和Values中的数据列表是一一对应的 declarer.declare(new Fields(&quot;num&quot;)); &#125; &#125; /** * 聚合的Bolt，负责把Spout发射出来的数据进行累加求和，并且打印到控制台 * */ public static class SumBolt extends BaseRichBolt&#123; private Map stormConf; private TopologyContext context; private OutputCollector collector; /** * prepare是一个初始化方法，只会执行一次，这里面也是可以放一些初始化的代码 */ public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.stormConf = stormConf; this.context = context; this.collector = collector; //System.err.println(&quot;bolt------&quot;+stormConf.get(&quot;name&quot;)); &#125; int sum = 0; /** * 这个方法也是会被循环调用 * 主要上一个组件向外发射了数据，那么这个方法就会被调用一次 */ public void execute(Tuple input) &#123; //input.getInteger(0);// 通过角标获取数据 Integer num = input.getIntegerByField(&quot;num&quot;); sum += num; System.out.println(&quot;和为：&quot;+sum); &#125; /** * 注意：这个方法在这里就不需要实现了，因为这个bolt没有向下一个组件发射数据 */ public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125; &#125; public static void main(String[] args) &#123; TopologyBuilder builder = new TopologyBuilder(); // 组装spout builder.setSpout(&quot;spoutid&quot;, new MySpout()); // 组装bolt,并且告诉bolt接收哪个组件的数据 builder.setBolt(&quot;bolt-1&quot;, new SumBolt()).shuffleGrouping(&quot;spoutid&quot;); StormTopology createTopology = builder.createTopology(); // 通过代码创建一个本地集群 LocalCluster localCluster = new LocalCluster(); String topologyName = WordCount.class.getSimpleName(); Config config = new Config(); config.put(&quot;name&quot;, &quot;zs&quot;); // 把代码提交到本地集群中运行 localCluster.submitTopology(topologyName, config, createTopology); &#125;&#125; Storm核心之并行度组件解释 worker：worker是一个进程，每一个worker进程里面都执行的是一个Topology的任务（不会出现一个worker执行多个Topology的任务）。一个worker中会启动一个或多个executor线程来执行Topology的spout或者bolt组件。一个Topology会使用一个或者多个worker来执行任务 executor：是worker进程内部启动的独立线程，每一个executor会产生一个或者多个task（storm默认是一个task即一个spout或者bolt有一个task，如果有多个task，executor会循环调用所有task中的实例） task：是最终运行spout或者bolt中具体代码的执行单元。Topology启动（spout或者bolt）的task的数目是不变的，但是executor线程的数量可以动态进行调整（例如：1个executor线程可以执行该(spout或bolt)的1个或多个task实例）。这意味着，对于1个(spout或bolt)存在这样的条件：#threads&lt;=#tasks（即：线程数小于等于task数目）。默认情况下task的数目等于executor线程数目，即1个executor线程只运行1个task。 默认情况下，一个supervisor节点最多可以启动4个worker进程，每一个topology默认占用一个worker进程，每个spout或者bolt会占用1个executor，每个executor启动1个task。 提高Storm组件的并行度 worker（slot）【间接】 默认一个节点最多可以启动四个worker进程，可以修改进程数量strom-core.jar/defaults.yaml/supervisor.slots.ports 默认一个Topology只有一个worker进程，可以通过代码指定一个Topology使用多个worker进程config.setNumWorkers(workersnum) 注意：如果worker使用完在提交Topology就不会执行，会处于等待状态。worker之是通过Netty通信的 executor【直接】 默认情况下一个executor只会运行一个task，可以直接通过代码修改增加task数量，会直接提高Storm组件的并行度 builder.setSpout(id, spout,parallelism_hint); builder.setBolt(id, bolt,parallelism_hint); task【间接】 通过boltDeclarer.setNumTasks(num);来设置实例的个数 executor的数量会小于等于task的数量(为了rebalance) 弹性计算rebalance 前提是Topology中的task数量要大于executor线程数 通过shell调整 storm rebalance mytopology -w 10 -n 5 -e blue-spout=3 -e yellow-bolt=10 注意：acker的树木运行时是不会变化的，所以多指定几个worker进程，acker的数量也不会增加 -w：表示超时时间，Rebalance会在一个超时时间内注销掉Topology，然后在集群中重新分配worker -n：表示的是worker的数量 -e：调整组件的并行度 注：-n 以及 -e 都可以单独使用或者组合起来使用 通过UI界面进行调整，不建议使用所以就不具体解释使用方法了 并行度设置多少合适 单spout每秒大概可以发送500个tuple 单bolt每秒大概可以接收2000个tuple 单acker每秒大概可以接收6000个tuple 根据需要进行调整","categories":[],"tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"Maven 简介","slug":"maven的简介","date":"2019-01-20T16:00:00.000Z","updated":"2019-08-21T11:43:29.612Z","comments":true,"path":"2019/01/21/maven的简介/","link":"","permalink":"http://yoursite.com/2019/01/21/maven的简介/","excerpt":"Maven 简介为什么需要maven 同样的代码要在不同的机器上运行他所需要的依赖可以放在maven仓库 项目组加入新成员可以快速的配置好环境 在开发其他项目的时候需要用到跟之前项目开发一样的jar包","text":"Maven 简介为什么需要maven 同样的代码要在不同的机器上运行他所需要的依赖可以放在maven仓库 项目组加入新成员可以快速的配置好环境 在开发其他项目的时候需要用到跟之前项目开发一样的jar包 maven是什么 maven是基于项目对象模型POM的软件项目管理工具 是可以跨平台的，主要服务基于Java平台的仙姑构建、依赖管理、项目信息管理等 构建的过程 清理 编译 测试 报告 打包 部署 maven的工程结构 src mian java – 存放Java的文件 源代码等 resource –存放资源文件 比如 spring，hibernate等的配置文件 test Java – 存放所有的.Java的测试文件，比如JUnit 测试类 resource –测试的资源文件夹 target —目标文件的输出位置比如jar包、war包等 pom.xml —maven的项目核心配置文件 maven常用命令 mvn compile 执行编译 会将生成文件存放在target目录中 mvn clean 删除target中的目录文件 mvn test 执行测试命令 执行后会在target目录中生成三个目录文件surefire、surefire-reports（测试报告）、test-classes（测试的字节码文件） mvn package 进行打包操作 操作后的文件存放在target目录之中 例如jar包war包 mvn install 将制定的jar包安装到本地仓库以便于其他工程的引用 mvn clean compile 清除测试类再执行compile执行编译操作 mvn clean test 先清除在进行test测试操作 mvn clean package 先执行clean清除在执行package打包 mvn clean install 先进行clean在执行install","categories":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/categories/基础/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"}]}]}