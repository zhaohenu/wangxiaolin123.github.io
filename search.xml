<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Elasticsearch问题以及调优</title>
      <link href="/2019/07/03/Elasticsearch%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
      <url>/2019/07/03/Elasticsearch%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="Elasticsearch问题以及调优"><a href="#Elasticsearch问题以及调优" class="headerlink" title="Elasticsearch问题以及调优"></a>Elasticsearch问题以及调优</h2><h3 id="Elasticsearch脑裂问题分析"><a href="#Elasticsearch脑裂问题分析" class="headerlink" title="Elasticsearch脑裂问题分析"></a>Elasticsearch脑裂问题分析</h3><ul><li><p>脑裂问题的图解</p><ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566822492817.png" alt="1566822492817"></li></ul><a id="more"></a><ul><li>脑裂问题就是在集群环境之中，由于节点之间的通信问题导致节点对集群的状态理解不同，导致es有些查询非常缓慢甚至查询失败</li><li>最好的解决办法就是重启集群，详细的问题分析可以查看我的博客《Elasticsearch脑裂问题详细分析及解决方案》</li></ul></li></ul><h3 id="Elasticsearch索引模板以及索引名"><a href="#Elasticsearch索引模板以及索引名" class="headerlink" title="Elasticsearch索引模板以及索引名"></a>Elasticsearch索引模板以及索引名</h3><h4 id="索引模板index-template"><a href="#索引模板index-template" class="headerlink" title="索引模板index template"></a>索引模板index template</h4><ul><li><p>在我们的工作中，针对一个大批量的数据存储时需要使用多个索引库，如果我们手工去为每个索引库配置信息就很麻烦，所以就有了索引模板，创建一个模板，制定好配置信息，如果我们闯进的索引库匹配到了模板就会使用模板中的配置信息</p></li><li><p>创建模板</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPUT localhost:9200/_template/template_1 -d &apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;template&quot; : &quot;*&quot;,</span><br><span class="line">    &quot;order&quot; : 0,</span><br><span class="line">    &quot;settings&quot; : &#123;</span><br><span class="line">        &quot;number_of_shards&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;mappings&quot; : &#123;</span><br><span class="line">        &quot;type1&quot; : &#123;</span><br><span class="line">            &quot;_source&quot; : &#123; &quot;enabled&quot; : false &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">其中的order值是用来进行模板之间的优先级排序的，如果一个索引匹配到多个模板则比较模板的order值，选取最大order值的模板进行配置信息匹配</span><br></pre></td></tr></table></figure></li></ul></li><li><p>查看模板信息  curl -XGET localhost:9200/_template/temp*?pretty</p></li><li><p>删除模板  curl -XDELETE localhost:9200/_template/temp_1</p></li></ul><h4 id="索引别名index-alias"><a href="#索引别名index-alias" class="headerlink" title="索引别名index alias"></a>索引别名index alias</h4><ul><li><p>索引别名就是为索引起一个或者多个别名方便引用</p></li><li><p>公司使用es收集应用的日志，每个星期创建一个索引库，这样时间长了就会创建很多个索引库，操作和管理非常不方便</p></li><li><p>由于新增的索引只会操作最新的这一周的索引库，所以我们就可以创建两个别名</p></li><li><p>curr_week :此别名执行这个星期的索引库，新增的数据操作这个索引库</p></li><li><p>last_3_month:这个别名指向的是近三个月的索引库，因为我们需要查询近三个月的数据</p></li><li><p>后期只需要修改两个别名与索引库的指向关系即可，应用层的代码不许要更新、</p></li><li><p>还需要将三个月之前的索引库close掉，将一年前的索引库删除</p></li><li><p>es默认对查询分片的数量是有限制的，默认是1000个，使用通配符查询多个索引库的时候会出问题，正好可以使用别名解决</p></li><li><p>增加索引别名  可同时增减多个</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;actions&quot; : [</span><br><span class="line">        &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;,</span><br><span class="line">        &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test2&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;</span><br><span class="line"></span><br><span class="line">    ]</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>删除索引别名</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;actions&quot; : [</span><br><span class="line">        &#123; &quot;remove&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Elasticsearch参数调优"><a href="#Elasticsearch参数调优" class="headerlink" title="Elasticsearch参数调优"></a>Elasticsearch参数调优</h3><ul><li><p>解决es启动警告信息</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br><span class="line">增加以下内容</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line">vi /etc/security/limits.d/90-nproc.conf </span><br><span class="line">把1024修改为4096</span><br><span class="line">*          soft    nproc     4096</span><br></pre></td></tr></table></figure></li></ul></li><li><p>修改配置文件调整es的jvm内存大小</p><ul><li>修改bin/elasticsearch.in.sh中ES_MIN_MEM和ES_MAX_MEM的大小，建议设置一样大，避免频繁的分配内存，根据服务器内存大小，一般分配60%左右(默认256M)</li><li>注意内存最大不要超过32G 一旦你越过这个神奇的32GB边界，指针会切换回普通对象指针.。每个指针的大小增加，使用更多的CPU内存带宽。事实上，你使用40~50G的内存和使用32G的内存效果是一样的。</li></ul></li><li><p>设置memory_lock来锁定进程的物理内存地址</p><ul><li><p>避免交换（swapped）来提高性能</p><p>修改文件conf/elasticsearch.yml</p><p>bootstrap.memory_lock: true</p><p>需要根据es启动日志修改/etc/security/limits.conf文件(重启系统)</p></li></ul></li><li><p>改变分片的数量</p><ul><li>分片多的话，可以提升建立索引的能力，5-20个比较合适。</li><li>如果分片的数量过多或者过少都会导致检索比较慢</li><li>分片过多会导致查询的额时候打开较多的文件，而分片数过少会导至单个分片索引过大，所以检索速度也会慢。</li><li>建议单个分片存储20G左右的索引数据【最高也不要超过50G，否则性能会很差】，所以，分片数量=数据总量/20G</li><li>副本多的话，可以提升搜索的能力，但是如果设置很多副本的话也会对服务器造成额外的压力，因为主分片需要给所有副本同步数据。所以建议最多设置1-2个即可。</li></ul></li><li><p>针对不使用的index，建议close，减少内存占用。因为只要索引处于open状态，索引库中的segement就会占用内存，close之后就只会占用磁盘空间了。</p><ul><li>curl -XPOST ‘localhost:9200/test/_close’</li></ul></li><li><p>要定时对索引进行合并优化，不然segment越多，占用的segment memory越多，查询的性能也越差</p><ul><li><p>索引量不是很大的话可以将segment设为1</p></li><li><p>在es2.1.0以前调用_optimize接口，后期改为_forcemerge接口</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;http://localhost:9200/test/_forcemerge?max_num_segments=1&apos;</span><br><span class="line">client.admin().indices().prepareForceMerge(&quot;test&quot;).setMaxNumSegments(1).get();</span><br><span class="line">注意：索引合并是针对分片的。segment设置为1，则每个分片都有一个索引片段。</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>删除文档：在es中删除文档，数据不会马上在硬盘上除去，而是在es索引中产生一个.del的文件，而在检索过程中这部分数据也会参与检索，es在检索过程会判断是否删除了，如果删除了在过滤掉。这样也会降低检索效率。所以可以执行清除删除文档</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">shell:</span><br><span class="line">curl -XPOST &apos;http://localhost:9200/test/_forcemerge?only_expunge_deletes=true&apos;</span><br><span class="line">java:</span><br><span class="line">client.admin().indices().prepareForceMerge(&quot;test&quot;).setOnlyExpungeDeletes(true).get();</span><br></pre></td></tr></table></figure></li></ul></li><li><p>如果在项目开始的时候需要批量入库大量数据的话，建议将副本数设置为0</p><ul><li>因为es在索引数据的时候，如果有副本存在，数据也会马上同步到副本中，这样会对es增加压力。可以等索引完成后将副本按需要改回来。这样可以提高索引效率</li></ul></li><li><p>Elasticsearch在建立索引时，根据id或(id,类型)进行hash，得到hash值之后再与该索引的分片数量取模，取模的值即为存入的分片编号</p><ul><li><p>可以指定把数据存储到某一个分片中，通过routing参数  可以显著提高性能</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/yehua/emp?routing=rout_param&apos; -d &apos;&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:20&#125;&apos;</span><br><span class="line">routing(路由参数)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 调优 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch脑裂问题详细分析以及解决方案</title>
      <link href="/2019/07/02/Elasticsearch%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
      <url>/2019/07/02/Elasticsearch%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<h2 id="Elasticsearch脑裂问题详细分析以及解决方案"><a href="#Elasticsearch脑裂问题详细分析以及解决方案" class="headerlink" title="Elasticsearch脑裂问题详细分析以及解决方案"></a>Elasticsearch脑裂问题详细分析以及解决方案</h2><h3 id="什么是脑裂问题"><a href="#什么是脑裂问题" class="headerlink" title="什么是脑裂问题"></a>什么是脑裂问题</h3><ul><li>脑裂问题其实就是同一个集群的不同节点对于整个几位群的状态有不同的理解，导致操作错乱，类似于精神分裂</li></ul><h3 id="怎么发现集群产生脑裂问题"><a href="#怎么发现集群产生脑裂问题" class="headerlink" title="怎么发现集群产生脑裂问题"></a>怎么发现集群产生脑裂问题</h3><a id="more"></a><ul><li>Elasticsearch出现查询非常缓慢的情况</li><li>通过命令查看集群的状态<ul><li>curl -XGET ‘<a href="http://localhost:9200/_cluster/health&#39;" target="_blank" rel="noopener">http://localhost:9200/_cluster/health&#39;</a></li></ul></li><li>发现集群状态为red，且集群数量明显错误，再向不同的节点查询集群状态的时候，总体状态都是red，但是返回的集群数量却不太一样</li><li>正常情况下，访问每一个节点，对集群中的状态返回应该是一致的。不一致的信息表示集群中不同节点对master节点的选择出现了问题。导致集群不能正常工作</li></ul><h3 id="产生脑裂问题的原因"><a href="#产生脑裂问题的原因" class="headerlink" title="产生脑裂问题的原因"></a>产生脑裂问题的原因</h3><ul><li>网络<ul><li>由于某些节点之间的网络通信出现问题，导致一些节点认为master节点已经挂了，所以有重新选举了新的master节点，从而导致集群信息混乱，可以检查Ganglia集群监控，来查看是否是网络原因</li></ul></li><li>节点负载过大：由于master节点与data节点都是混在一起的，有可能master节点的负载过大，导致对应的es实例停止响应，这时一部分节点会一位master节点已经挂掉从而重新选举，导致多master节点运行。同时由于data节点上ES进程占用的内存较大，较大规模的内存回收操作也能造成ES进程失去响应。所以，这个原因的可能性应该是最大的。</li></ul><h3 id="如何解决脑裂问题"><a href="#如何解决脑裂问题" class="headerlink" title="如何解决脑裂问题"></a>如何解决脑裂问题</h3><ul><li><p>对于网络问题，只能进行网络修复，在重启集群</p></li><li><p>对于负载的问题</p><ul><li><p>一个直观的解决方案就是将master节点与data节点分离，准备几台机器加入集群中，这几台机器只能充当master节点，不可担任存储和搜索的角色</p><ul><li><p>配置信息</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node.master: true</span><br><span class="line">node.data: false</span><br><span class="line">其他节点  只能充当data不能充当master</span><br><span class="line">node.master: false</span><br><span class="line">node.data: true</span><br></pre></td></tr></table></figure></li></ul></li><li><p>还有两个参数的修改可以减少脑裂问题的出现</p><ul><li>discovery.zen.ping_timeout（默认值是3秒）：默认情况下，一个节点会认为，如果master节点在3秒之内没有应答，那么这个节点就是死掉了，而增加这个值，会增加节点等待响应的时间，从一定程度上会减少误判。</li><li>discovery.zen.minimum_master_nodes（默认是1）：这个参数控制的是，一个节点需要看到的具有master节点资格的最小数量，然后才能在集群中做操作。官方的推荐值是(N/2)+1，其中N是具有master资格的节点的数量</li></ul></li><li><p>如果脑裂问题已经发生该如何解决</p><ul><li>当脑裂发生后，唯一的修复办法是解决这个问题并重启集群。 当elasticsearch集群启动时，会选出一个主节点（一般是启动的第一个节点被选为主）。由于索引的两份拷贝已经不一样了，elasticsearch会认为选出来的主保留的分片是“主拷贝”并将这份拷贝推送给集群中的其他节点。这很严重。让我们设想下你是用的是node客户端并且一个节点保留了索引中的正确数据。但如果是另外的一个节点先启动并被选为主，它会将一份过期的索引数据推送给另一个节点，覆盖它，导致丢失了有效数据。</li><li>所以怎么从脑裂中恢复？第一个建议是给所有数据重新索引。第二，如果脑裂发生了，要十分小心的重启你的集群。停掉所有节点并决定哪一个节点第一个启动。 如果需要，单独启动每个节点并分析它保存的数据。如果不是有效的，关掉它，并删除它数据目录的内容（删前先做个备份）。如果你找到了你想要保存数据的节点，启动它并且检查日志确保它被选为主节点。这之后你可以安全的启动你集群里的其他节点了。</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 问题分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch脑裂问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch高级</title>
      <link href="/2019/07/01/Elasticsearch%E9%AB%98%E7%BA%A7%E4%BA%8C/"/>
      <url>/2019/07/01/Elasticsearch%E9%AB%98%E7%BA%A7%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="Elasticsearch高级二"><a href="#Elasticsearch高级二" class="headerlink" title="Elasticsearch高级二"></a>Elasticsearch高级二</h2><h3 id="Elasticsearch查询详解"><a href="#Elasticsearch查询详解" class="headerlink" title="Elasticsearch查询详解"></a>Elasticsearch查询详解</h3><h4 id="查询Query"><a href="#查询Query" class="headerlink" title="查询Query"></a>查询Query</h4><a id="more"></a><ul><li><p>代码</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public class EsDemo2 &#123;</span><br><span class="line">    static String index = &quot;test&quot;;</span><br><span class="line">    static String type = &quot;emp&quot;;</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, true)</span><br><span class="line">                .build();</span><br><span class="line">        TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)</span><br><span class="line">                .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300));</span><br><span class="line">        testSearch(client);</span><br><span class="line">    &#125;</span><br><span class="line">     public static void testSearch(TransportClient client)&#123;</span><br><span class="line">        SearchResponse searchResponse = client.prepareSearch(index)</span><br><span class="line">                .setQuery(QueryBuilders.&lt; !-- matchAllQuery() 具体的查询方法 -- &gt;)</span><br><span class="line">                .get();</span><br><span class="line">        SearchHits hits = searchResponse.getHits();</span><br><span class="line">        //获取总条数</span><br><span class="line">        long totalHits = hits.getTotalHits();</span><br><span class="line">        System.out.println(&quot;数据的总条数&quot;+totalHits);</span><br><span class="line">        //打印所有数据内容</span><br><span class="line">        SearchHit[] hits1 = hits.getHits();</span><br><span class="line">        for (SearchHit hit:hits1) &#123;</span><br><span class="line">            System.out.println(hit.getSourceAsString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>matchAllQuery()  查询所有数据</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">结果：</span><br><span class="line">数据的总条数6</span><br><span class="line">&#123;&quot;name&quot;:&quot;jessic&quot;,&quot;age&quot;:18&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>matchQuery((“name”,”zs”))  根据指定列进行模糊查询 不支持通配符</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据的总条数2</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>multiMatchQuery(“zs”,”name”,”city”)  在多个列中进行模糊查询 查询的列不存在不会报错</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据的总条数2</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>queryStringQuery(“name:z*”) Lucene提供的方法支持对某一列查询的时候使用通配符</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">数据的总条数2</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>boolQuery()</p><p>.should(QueryBuilders.matchQuery(“name”,”zs”).boost(10.0f))        .should(QueryBuilders.matchQuery(“age”,19).boost(1.0f))</p><p>根据不同的条件进行多次查询  可以根据boost值来设置两条语句的结果先后顺序</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据的总条数4</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:15&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>termQuery(“name”,”abc xyz”)  查询的时候不会进行分词的精确查询</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据的总条数0</span><br></pre></td></tr></table></figure></li><li><p>在默认情况之下es会将所有词进行分词索引，这是你试试用及精确查询时查不到的</p></li><li><p>解决方案</p><ul><li><p>在创建索引的时候将不需要进行分词的特殊索引指定不分词</p></li><li><p>根据Lucene提供的方法直接查询可以对已经分词的索引进行精确查询</p></li><li><p>queryStringQuery(“name:&quot;abc xyz&quot;“)</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据的总条数1</span><br><span class="line">&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>matchQuery(“name”,”abc xyz”).operator(Operator.AND)  也可以对已经分词的索引进行精确查询</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据的总条数1</span><br><span class="line">&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="其他查询"><a href="#其他查询" class="headerlink" title="其他查询"></a>其他查询</h4><ul><li><p>from、size  分页</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.setFrom(2)</span><br><span class="line">.setSize(3)</span><br><span class="line">&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>sort 排序</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">.setQuery(QueryBuilders.matchAllQuery()) //取出所有数据</span><br><span class="line">.addSort(&quot;age&quot;, SortOrder.DESC)  //按照年龄降序排序</span><br><span class="line"></span><br><span class="line">&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:19&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;jessic&quot;,&quot;age&quot;:18&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;abc xyz&quot;,&quot;age&quot;:15&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>filter 过滤</p><ul><li><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.setPostFilter(QueryBuilders.rangeQuery(<span class="string">"age"</span>).from(<span class="string">"16"</span>).to(<span class="number">20</span>))</span><br><span class="line">    过滤出来年龄在<span class="number">16</span>到<span class="number">20</span>之间的</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"jessic"</span>,<span class="string">"age"</span>:<span class="number">18</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"zs"</span>,<span class="string">"age"</span>:<span class="number">16</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"jack"</span>,<span class="string">"age"</span>:<span class="number">19</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"lili"</span>,<span class="string">"age"</span>:<span class="number">16</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"tom"</span>,<span class="string">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>highlight  高亮  将搜索出的结果加上高亮的效果</p></li><li><p>按查询匹配度排序</p><ul><li>.setExplain(true)</li></ul></li></ul><h4 id="两个简单练习"><a href="#两个简单练习" class="headerlink" title="两个简单练习"></a>两个简单练习</h4><ul><li><p>聚合分组求count</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">    * 统计测试</span><br><span class="line">    * 聚合分组求count</span><br><span class="line">    */</span><br><span class="line">   public static void testAggregation(TransportClient client)&#123;</span><br><span class="line">       SearchResponse searchResponse = client.prepareSearch(index)</span><br><span class="line">               //.setTypes(type)</span><br><span class="line">               .setQuery(QueryBuilders.matchAllQuery())</span><br><span class="line">               .addAggregation(AggregationBuilders.terms(&quot;term_age&quot;).field(&quot;age&quot;))</span><br><span class="line">               .get();</span><br><span class="line">       Terms term_age = searchResponse.getAggregations().get(&quot;term_age&quot;);</span><br><span class="line">       List&lt;? extends Terms.Bucket&gt; buckets = term_age.getBuckets();</span><br><span class="line">       for (Terms.Bucket bk:buckets) &#123;</span><br><span class="line">           System.out.println(bk.getKey()+&quot;----------&quot;+bk.getDocCount());</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>聚合分组求sum</p><ul><li><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 统计测试</span></span><br><span class="line"><span class="comment"> * 聚合分组求sum</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">testAggregation2</span><span class="params">(TransportClient client)</span></span>&#123;</span><br><span class="line">    SearchResponse searchResponse = client.prepareSearch(index)</span><br><span class="line">            .setQuery(QueryBuilders.matchAllQuery())</span><br><span class="line">            .addAggregation(AggregationBuilders.terms(<span class="string">"term_name"</span>).field(<span class="string">"name.keyword"</span>)  <span class="comment">//name是text类型不支持分组，所以取他的keyword</span></span><br><span class="line">            .subAggregation(AggregationBuilders.sum(<span class="string">"sum_score"</span>).field(<span class="string">"score"</span>)))</span><br><span class="line">            .get();</span><br><span class="line">    Terms term_name = searchResponse.getAggregations().get(<span class="string">"term_name"</span>);</span><br><span class="line">    List&lt;? extends Terms.Bucket&gt; buckets = term_name.getBuckets();</span><br><span class="line">    <span class="keyword">for</span> (Terms.Bucket bk:buckets) &#123;</span><br><span class="line">        Sum sumScore = bk.getAggregations().get(<span class="string">"sum_score"</span>);</span><br><span class="line">        System.out.println(bk.getKey()+<span class="string">"----------"</span>+sumScore.getValue());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Elasticsearch中的setting以及mapping详解"><a href="#Elasticsearch中的setting以及mapping详解" class="headerlink" title="Elasticsearch中的setting以及mapping详解"></a>Elasticsearch中的setting以及mapping详解</h3><ul><li><p>setting是修改索引库默认的配置</p><ul><li><p>查看页面的setting信息</p><ul><li>curl -XGET <a href="http://localhost:9200/test/_settings?pretty" target="_blank" rel="noopener">http://localhost:9200/test/_settings?pretty</a></li></ul></li><li><p>修改已经存在的索引库信息</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test/_settings&apos; -d&apos;&#123;&quot;index&quot;:&#123;&quot;number_of_replicas&quot;:1&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><ul><li><p>修改不存在的索引库的信息</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test1/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_shards&quot;:3,&quot;number_of_replicas&quot;:0&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>mapping 是对索引库中的索引的名称以及数据类型进行定义，类似于MySQL的表名以及表的结构信息。但是es的mapping比较灵活，可以动态识别各字段的信息，一般不需要自定义mapping</p><ul><li><p>查看索引库mapping信息</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://localhost:9200/test/emp/_mapping?pretty</span><br></pre></td></tr></table></figure></li></ul></li><li><p>操作已经存在的索引  指定分词器</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPOST http://localhost:9200/test/emp/_mapping -d&apos;&#123;&quot;properties&quot;:&#123;&quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><ul><li><p>操作不存在的索引</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test2&apos; -d&apos;&#123;&quot;mappings&quot;:&#123;&quot;emp&quot;:&#123;&quot;properties&quot;:&#123;&quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;&#125;&#125;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Elasticsearch的分片查询方式"><a href="#Elasticsearch的分片查询方式" class="headerlink" title="Elasticsearch的分片查询方式"></a>Elasticsearch的分片查询方式</h3><ul><li>默认的是randomize across shards   表示随机选取，即随机从分片中取数据</li><li>_local :表示执行查询操作的时候回优先在本地节点中的分片中进行查询，没有的话在去其他节点</li><li>_only_local:表示只在本地分片中查询</li><li>_primary:表示只在主分片中查询</li><li>_primary_first:表示优先在主分片中查询，如果主分片出现问题数据丢失或者其他就会去副分片中查询</li><li>_replica_first:表示优先在副分片上查询，有问题了再去主分片查询</li><li>_only_node:在指定ID的节点上查询，只有该节点上有相关分片就回进行查询，可能导致查询结果不够完整</li><li>_only_nodes:指定ID的节点是多个</li><li>_prefer_node:优先在指定ID的节点查询 查不到再去其他节点</li><li>_shards:查询指定分片的信息 可以实现急速查询  但需要指定索引所在分片的信息</li></ul>]]></content>
      
      
      <categories>
          
          <category> 提高 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELK的简单部署以及使用</title>
      <link href="/2019/06/06/ELK%E7%AE%80%E5%8D%95%E9%83%A8%E7%BD%B2%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/06/06/ELK%E7%AE%80%E5%8D%95%E9%83%A8%E7%BD%B2%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="ELK简单部署以及使用"><a href="#ELK简单部署以及使用" class="headerlink" title="ELK简单部署以及使用"></a>ELK简单部署以及使用</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li>此项目是使用filebeat轻量化日志采集工具，将日志采集到kafka，在使用logstash工具将日志采集到Elasticsearch中，使用kibana工具在web界面上进行各种搜索查看建立图标等操作。默认kafka以及Elasticsearch已经装好了。</li></ul><a id="more"></a><h3 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h3><ul><li>进入到官网<a href="https://www.elastic.co/" target="_blank" rel="noopener">https://www.elastic.co</a><ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566796617980.png" alt="1566796617980"></li></ul></li><li>找到产品<ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566796654407.png" alt="1566796654407"></li></ul></li><li>点击下载<ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566796697107.png" alt="1566796697107"></li></ul></li><li>选择需要下载的工具点击下载<ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566796751070.png" alt="1566796751070"></li><li>在past releases中可以选择其他版本点击下载就可以</li></ul></li><li>下载好安装包以后进行解压、配置环境变量</li></ul><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><ul><li><p>filebeat变量配置</p><ul><li><p>首先进入到filebeat解压后的文件中</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd filebeat-6.4.3-linux-x86_64</span><br></pre></td></tr></table></figure></li></ul></li><li><p>修改filebeat.yml中的配置</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi filebeat.yml</span><br></pre></td></tr></table></figure></li></ul></li><li><p>修改成以下值</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">filebeat.inputs:</span><br><span class="line">- type: log</span><br><span class="line">enabled: true</span><br><span class="line"> paths:</span><br><span class="line">    - /data/filebeattest/logs/*.log  //需要采集的日志地址</span><br><span class="line">output.kafka:</span><br><span class="line">  hosts: [&quot;hadoop110:9092&quot;]   //kafka的地址 如果使用hadoop这种类型的名字需要在本节点上配置 hosts文件</span><br><span class="line">  topic: &apos;filelog&apos;   //kafka中的topic</span><br><span class="line">  partition.round_robin:</span><br><span class="line">    reachable_only: false</span><br><span class="line"></span><br><span class="line">  required_acks: 1</span><br><span class="line">  compression: gzip</span><br><span class="line">  max_message_bytes: 1000000</span><br></pre></td></tr></table></figure></li></ul></li><li><p>启动filebeat的指令</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filebeat -c filebeat.yml</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>logstash变量配置</p><ul><li><p>进入到logstash的解压文件中</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd logstash-6.4.3</span><br></pre></td></tr></table></figure></li></ul></li><li><p>创建一个配置文件的文件夹conf 并在文件夹里创建一个配置文件kafka-elasticsearch.conf</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir conf</span><br><span class="line">cd conf</span><br><span class="line">touch kafka-elasticsearch.conf</span><br></pre></td></tr></table></figure></li></ul></li><li><p>给配置文件加上配置信息</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vi kafka-elasticsearch.conf</span><br><span class="line">input &#123;</span><br><span class="line">  kafka &#123;</span><br><span class="line">     topics =&gt; [&quot;filelog&quot;]</span><br><span class="line">     bootstrap_servers =&gt; &quot;hadoop110:9092&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line"></span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">       hosts =&gt; [&quot;hadoop110:9200&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>启动logstash</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logstash  -f  conf/kafka-elasticsearch.conf</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>配置kibana</p><ul><li><p>进入到kibana的解压文件中</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd kibana-6.4.3-linux-x86_64</span><br></pre></td></tr></table></figure></li></ul></li><li><p>修改一些需要的变量配置</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd confog</span><br><span class="line">vi kibana.yml</span><br><span class="line">server.port: 5601</span><br><span class="line">elasticsearch.url: &quot;http://hadoop110:9200&quot;</span><br><span class="line">根据实际需要修改</span><br></pre></td></tr></table></figure></li></ul></li><li><p>启动kibana</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kibana</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h3 id="启动检查效果"><a href="#启动检查效果" class="headerlink" title="启动检查效果"></a>启动检查效果</h3><ul><li>进入到kibana的web界面查看效果<ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566799108735.png" alt="1566799108735"></li><li>看到我们创建的索引已经存在了就说明我们的真个过程已经成功了</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch高级</title>
      <link href="/2019/05/11/Elasticsearch%E5%9F%BA%E7%A1%80/"/>
      <url>/2019/05/11/Elasticsearch%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h2 id="Elasticsearch高级"><a href="#Elasticsearch高级" class="headerlink" title="Elasticsearch高级"></a>Elasticsearch高级</h2><h3 id="Elasticsearch批量操作的查询类型"><a href="#Elasticsearch批量操作的查询类型" class="headerlink" title="Elasticsearch批量操作的查询类型"></a>Elasticsearch批量操作的查询类型</h3><h5 id="Bulk批量查询的Java实现"><a href="#Bulk批量查询的Java实现" class="headerlink" title="Bulk批量查询的Java实现"></a>Bulk批量查询的Java实现</h5><a id="more"></a><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">package EsTest;</span><br><span class="line"></span><br><span class="line">import org.elasticsearch.action.bulk.BulkItemResponse;</span><br><span class="line">import org.elasticsearch.action.bulk.BulkRequestBuilder;</span><br><span class="line">import org.elasticsearch.action.bulk.BulkResponse;</span><br><span class="line">import org.elasticsearch.action.delete.DeleteRequest;</span><br><span class="line">import org.elasticsearch.action.index.IndexRequest;</span><br><span class="line">import org.elasticsearch.client.transport.TransportClient;</span><br><span class="line">import org.elasticsearch.common.settings.Settings;</span><br><span class="line">import org.elasticsearch.common.transport.TransportAddress;</span><br><span class="line">import org.elasticsearch.transport.client.PreBuiltTransportClient;</span><br><span class="line"></span><br><span class="line">import java.net.InetAddress;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Es简单操作测试</span><br><span class="line"> */</span><br><span class="line">public class EsDemo2 &#123;</span><br><span class="line">    static String index = &quot;test&quot;;</span><br><span class="line">    static String type = &quot;emp&quot;;</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, true)</span><br><span class="line">                .build();</span><br><span class="line">        TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)</span><br><span class="line">                .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300));</span><br><span class="line">        testBulk(client);</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * bulk批量查询测试</span><br><span class="line">     */</span><br><span class="line">    public static  void testBulk(TransportClient client)&#123;</span><br><span class="line">        BulkRequestBuilder bulkRequestBuilder = client.prepareBulk();</span><br><span class="line">        //创建请求</span><br><span class="line">        IndexRequest indexRequest = new IndexRequest(index, type)</span><br><span class="line">                                    .source(&quot;&#123;\&quot;name\&quot;:\&quot;zs1\&quot;,\&quot;age\&quot;:25&#125;&quot;);</span><br><span class="line">        //删除请求</span><br><span class="line">        DeleteRequest deleteRequest = new DeleteRequest(index, type, &quot;21&quot;,,XContentType.JSON);</span><br><span class="line">        //将操作整合到buider中</span><br><span class="line">        bulkRequestBuilder.add(indexRequest);</span><br><span class="line">        bulkRequestBuilder.add(deleteRequest);</span><br><span class="line">        //执行批量操作</span><br><span class="line">        BulkResponse bulkItemResponses = bulkRequestBuilder.get();</span><br><span class="line">        //查看执行过程中失败的信息</span><br><span class="line">        if(bulkItemResponses.hasFailures())&#123;</span><br><span class="line">            BulkItemResponse[] items = bulkItemResponses.getItems();</span><br><span class="line">            for (BulkItemResponse item: items) &#123;</span><br><span class="line">                System.out.println(item.getFailureMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            System.out.println(&quot;所有bulk指令都执行成功了&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="SearchType的详解"><a href="#SearchType的详解" class="headerlink" title="SearchType的详解"></a>SearchType的详解</h4><ul><li><p>query and fetch :当客户机向es集群中的某一个节点发送请求时，这个节点会将请求复制到每一个节点上，然后每一个节点会将所请求的数据返回到查询节点上，然后由查询节点返回到客户机上，这样的优点就是速度快，缺点是不准确，客户想要10条数据，集群返回的是10*n条数据，n是集群的节点数</p></li><li><p>query then fetch:当客户机向集群发送请求时，集群中接收请求的节点也会将查询请求发送到每一个节点之上，但是每个节点只返回查询结果的ID等值给主节点，主节点将受到的数据在进行排序取出所需要的条数，然后根据其ID等到相应节点上取的数据，在将数据返回至客户机。优点是可以准确返回需要条数的请求，且结果相对来说准确，缺点是查询速度慢，是es的默认查询类型</p></li><li><p>DFS D是Distributed，F是frequency的缩写，S是Scatter的缩写，整个单词可能是分布式词频率和文档频率散发的缩写</p><ul><li>dfs简称是初始化散发</li><li>官方解释是初始化散发其实就是在进行真正的查询之前，先把各个分片的词频率和文档频率收集一下，然后进行词搜索的时候，各分片依据全局的词频率和文档频率进行搜索和排名。</li><li>通俗一点来说就是统计所有节点的搜索排名的算法，总结到一起可以对整个文档进行精确的算法排名</li></ul></li><li><p>dfs query and fetch：就是加了dfs的query and fetch依然是速度快，但是结果条数多</p></li><li><p>dfs query then fetch:执行过程：首先，从各个节点的搜索排序算法即词频率文档频率等，然后根据整合好的算法在每个节点上取出相应数据的ID等信息，在主节点上再次通过该算法获取准确的数据信息，在通过他们的ID等信息去各个节点获取具体数据返回至客户机上。优点是查询准确率高，但是查询速度慢</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566654050267.png" alt="1566654050267"></p></li><li><p>代码写法</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Es简单操作测试</span><br><span class="line"> */</span><br><span class="line">public class EsDemo2 &#123;</span><br><span class="line">    static String index = &quot;test&quot;;</span><br><span class="line">    static String type = &quot;emp&quot;;</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, true)</span><br><span class="line">                .build();</span><br><span class="line">        TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)</span><br><span class="line">                .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300));</span><br><span class="line">        //testBulk(client);</span><br><span class="line">        testSeType(client);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 查询类型的测试</span><br><span class="line">     */</span><br><span class="line">    public  static void testSeType(TransportClient client)&#123;</span><br><span class="line">        SearchResponse searchResponse = client.prepareSearch(index)  //索引库信息</span><br><span class="line">                .setQuery(QueryBuilders.matchAllQuery())  //查询规则 所有队列</span><br><span class="line">                .setPreference(&quot;_shards:1&quot;)  //指点分片</span><br><span class="line">                .setSearchType(SearchType.QUERY_THEN_FETCH)  //类型可以自己指定</span><br><span class="line">                .get();</span><br><span class="line">        SearchHits hits = searchResponse.getHits();</span><br><span class="line">        //获取总条数</span><br><span class="line">        long totalHits = hits.getTotalHits();</span><br><span class="line">        System.out.println(&quot;数据的总条数&quot;+totalHits);</span><br><span class="line">        //打印所有数据内容</span><br><span class="line">        SearchHit[] hits1 = hits.getHits();</span><br><span class="line">        for (SearchHit hit:hits1) &#123;</span><br><span class="line">            System.out.println(hit.getSourceAsString());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Elasticsearch分词详解"><a href="#Elasticsearch分词详解" class="headerlink" title="Elasticsearch分词详解"></a>Elasticsearch分词详解</h3><h4 id="es索引建立和搜索过程图解"><a href="#es索引建立和搜索过程图解" class="headerlink" title="es索引建立和搜索过程图解"></a>es索引建立和搜索过程图解</h4><ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566654133368.png" alt="sss"></li></ul><h4 id="倒排索引介绍"><a href="#倒排索引介绍" class="headerlink" title="倒排索引介绍"></a>倒排索引介绍</h4><ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566654223385.png" alt="1566654223385"></li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566654235494.png" alt="1566654235494"></li><li>表中的各个单词表示文档意思<ul><li>单词ID：记录每个单词的编号</li><li>单词：单词ID对应的单词</li><li>文档频率：单词在几个文档中出现过</li><li>倒排列表：单词出现的文档信息以及单词出现位置信息</li><li>DocID：单词出现的文档的ID</li><li>TF：单词在该文档出现的次数</li><li>POS：单词在文档中出现的位置</li></ul></li></ul><h4 id="分词器Analyzer的介绍"><a href="#分词器Analyzer的介绍" class="headerlink" title="分词器Analyzer的介绍"></a>分词器Analyzer的介绍</h4><ul><li><p>分词器就是将数据按以此为单位分开</p></li><li><p>分词器的作用</p><ul><li>是吧文本中的词按照一定的规则进行切分。</li><li>分词器所对应的的类是Analyzer是一个抽象类，具体的实现方法要靠他的子类，所以对于不同的语言就可以提供不同的分词器</li><li>在创建索引以及搜索的时候都会用到分词器，而且这两个过程所用到的分析器必须是同一种分词器</li></ul></li><li><p>分词器的工作流程</p><ul><li>切分关键词</li><li>取出停用词</li><li>对于英文字母，将所有字母转换为小写</li></ul></li><li><p>停用词的介绍</p><ul><li>有些词在文本中出现的概率很高但是对于文本所携带的信息并没有什么影响</li><li>英文中的<ul><li>a,an,the,of 等   <ul><li><a href="http://www.ranks.nl/stopwords" target="_blank" rel="noopener">http://www.ranks.nl/stopwords</a></li></ul></li></ul></li><li>中文的<ul><li>的，了，是，着，以及各种标点符号等等<ul><li><a href="http://www.ranks.nl/stopwords/chinese-stopwords" target="_blank" rel="noopener">http://www.ranks.nl/stopwords/chinese-stopwords</a></li></ul></li></ul></li><li>文本经过分词的过程以后，这种停用词一般都会被过滤掉，不会被索引</li><li>如果搜索的词含有停用词一本也会被过滤掉</li><li>过滤掉停用词可以加快建立索引，减小索引库的文件的大小</li></ul></li><li><p>几个重要的分词器介绍</p><ul><li><table><thead><tr><th align="center">分词器</th><th align="center">分词方式</th></tr></thead><tbody><tr><td align="center">StandardAnalyzer</td><td align="center">单词分词器</td></tr><tr><td align="center">ChineseAnalyzer</td><td align="center">单字分词器</td></tr><tr><td align="center">CJKAnalyzer</td><td align="center">二分法分词器</td></tr><tr><td align="center">IKAnalyzer</td><td align="center">词库分词器</td></tr></tbody></table></li><li><p>单字分词以及单词分词的意思是一样的</p><ul><li>“我们是中国人”效果：“我”“们”“是”“中”“国”“人”</li></ul></li><li><p>二分法分词器：按两个字的方式分词</p><ul><li>“我们是中国人”，效果：“我们”、“们是”、“是中”、“中国”、“国人”</li></ul></li><li><p>词库分词器</p><ul><li>按某种算法造词，然后将词存入到词库，把搜索内容匹配到词库的词然后进行拆分。</li></ul></li></ul></li></ul><h3 id="Elasticsearch分词插件介绍以及使用es-ik"><a href="#Elasticsearch分词插件介绍以及使用es-ik" class="headerlink" title="Elasticsearch分词插件介绍以及使用es-ik"></a>Elasticsearch分词插件介绍以及使用es-ik</h3><ul><li>官方默认的分词插件对中文的支持不是很好，所以我们需要采用第三方的词库来进行分词，IK就是一个分成不错的分词工具</li><li>下载地址<ul><li><a href="https://github.com/medcl/elasticsearch-analysis-ik/releases" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik/releases</a></li></ul></li><li>将下载好的文件放在ES_HOME/plugins/ik目录下解压</li><li>解压后就可以使用</li><li>自己添加词库<ul><li>进入到config文件中</li><li>创建一个自己存放自己词库的文件夹</li><li>在文件夹中创建dic文件将自己的词库内容放到所创建的文件中</li><li>修改IKAnalyzer.cfg.xml文件信息</li><li>将自己创建的词库加进去</li><li>重启es就可以使用自己创建的词库了</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566655870803.png" alt="1566655870803"></li></ul></li><li>实现热词的自动更新不需要重启es<ul><li>在一台服务器上部署一个Tomcat</li><li>在Tomcat中的webapp/ROOT 创建hot.doc热词词库</li><li>通过访问网络端口确定这个热词库可以访问</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566656094187.png" alt="1566656094187"></li><li>修改IK的配置<ul><li><entry key="remote_ext_dict"><a href="http://192.168.80.100:8080/hot.dic" target="_blank" rel="noopener">http://192.168.80.100:8080/hot.dic</a></entry></li></ul></li><li>重启es之后就可以在hot.dic文件中动态添加热词，IK会定时从端口中访问该文件然后进行更新热词</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 提高 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch IK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch Head Plugin</title>
      <link href="/2019/04/02/Elasticsearch%20Head%20Plugin%20%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
      <url>/2019/04/02/Elasticsearch%20Head%20Plugin%20%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Elasticsearch-Head-Plugin-详细安装教程"><a href="#Elasticsearch-Head-Plugin-详细安装教程" class="headerlink" title="Elasticsearch Head Plugin 详细安装教程"></a>Elasticsearch Head Plugin 详细安装教程</h2><h4 id="Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES"><a href="#Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES" class="headerlink" title="Elasticsearch Head Plugin站点插件可以以网页形式展现ES"></a>Elasticsearch Head Plugin站点插件可以以网页形式展现ES</h4><a id="more"></a><ul><li><p>注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt</p><ul><li><p>nodejs下载地址<a href="https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz" target="_blank" rel="noopener">https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz</a></p><ul><li>复制此链接就可以直接下或者<a href="https://nodejs.org/dist" target="_blank" rel="noopener">https://nodejs.org/dist</a> 使用这个两节找适合自己的版本</li></ul></li><li><p>因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压  如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">解压：</span><br><span class="line">xz -d node-v10.15.3-linux-x64.tar.xz</span><br><span class="line">tar -xvf node-v10.15.3-linux-x64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/node  /usr/bin/node</span><br><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm</span><br></pre></td></tr></table></figure></li><li><p>设定nodejs安装软件的dialing服务器</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li><li><p>安装grunt</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g grunt </span><br><span class="line">npm install -g grunt-cli</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt  /usr/bin/grunt</span><br></pre></td></tr></table></figure></li><li><p>安装phantomjs</p><ul><li>下载地址<a href="http://phantomjs.org/download.html" target="_blank" rel="noopener">http://phantomjs.org/download.html</a></li></ul></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2</span><br><span class="line">tar -xvf phantomjs-2.1.1-linux-x86_64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs  /usr/bin/phantomjs</span><br></pre></td></tr></table></figure></li><li><p>安装依赖软件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install wget fontconfig</span><br></pre></td></tr></table></figure></li></ul></li><li><p>安装head插件</p><ul><li><p>下载地址：<a href="https://github.com/mobz/elasticsearch-head/archive/master.zip" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head/archive/master.zip</a></p></li><li><p>解压 需要使用unzip命令</p></li><li><p>下载unzip</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y unzip</span><br></pre></td></tr></table></figure></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip elasticsearch-head-master.zip</span><br></pre></td></tr></table></figure></li><li><p>然后cd进入到解压的文件中</p></li><li><p>安装两个插件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm audit fix</span><br><span class="line">npm audit fix --force</span><br></pre></td></tr></table></figure></li><li><p>执行安装命令安装head</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install</span><br></pre></td></tr></table></figure></li><li><p>启动之前修改Gruntfile.js文件，增加hostname参数</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi Gruntfile.js</span><br><span class="line">options: &#123;</span><br><span class="line">hostname: &apos;hadoop100&apos;,</span><br><span class="line">port: 9101,</span><br><span class="line">base: &apos;.&apos;,</span><br><span class="line">keepalive: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>启动服务</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grunt Server</span><br></pre></td></tr></table></figure></li><li><p>启动服务后还需要修改Elasticsearch中的一些配置</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span><br><span class="line">http.cors.enabled: true </span><br><span class="line">http.cors.allow-origin: &quot;*&quot;</span><br></pre></td></tr></table></figure></li><li><p>修改完成后重启ES</p></li><li><p>进入网址<a href="http://hadoop100:9101/可以看到以下信息就说明插件安装成功了" target="_blank" rel="noopener">http://hadoop100:9101/可以看到以下信息就说明插件安装成功了</a></p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566465169785.png" alt="1566465169785"></p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 安装部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch的安装部署</title>
      <link href="/2019/04/02/2019-04-2-Elasticsearch%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/04/02/2019-04-2-Elasticsearch%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h3 id="Elasticsearch安装部署"><a href="#Elasticsearch安装部署" class="headerlink" title="Elasticsearch安装部署"></a>Elasticsearch安装部署</h3><ol><li>安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了）</li></ol><a id="more"></a><ol><li><p>下载Elasticsearch </p><ul><li>网址：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3" target="_blank" rel="noopener">https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3</a></li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566399839410.png" alt="1566399839410"></li><li>选择合适的版本下载就可以</li></ul></li><li><p>下载完成以后上传到Linux系统中，解压</p></li><li><p>进入到解压后的文件中尝试进行开启</p><ul><li>bin/elasticsearch （-d 后台运行）</li><li>注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off)</li><li>会发现执行报错  就是不可以在root用户下打开，选择一个其他用户就可以了</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398085045.png" alt="1566398085045"></li></ul></li><li><p>修改配置变量</p><ul><li><p>修改Elasticsearch中config变量 （vi config/elasticsearch.yml)</p></li><li><p>增加两行代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bootstrap.system_call_filter: false  </span><br><span class="line">network.host: 192.168.32.110   //后面的的IP设置你自己的本地IP就可以</span><br></pre></td></tr></table></figure></li></ul></li></ol><ul><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398675928.png" alt="1566398675928"></p></li><li><p>修改Linux的配置变量</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line">vi /etc/sysctl.conf</span><br><span class="line">vm.max_map_count=262144</span><br><span class="line">vi /etc/security/limits.d/90-nproc.conf </span><br><span class="line">把1024修改为4096</span><br><span class="line">*          soft    nproc     4096</span><br></pre></td></tr></table></figure></li><li><p>修改完配置以后重启操作系统在启动就可以啦</p></li><li><p>启动之后出现以下情况</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456097622.png" alt="1566456097622"></p></li><li><p>没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456158512.png" alt="1566456158512"></p></li><li><p>然后可以使用web界面查看 在浏览器输入 <a href="http://yourip:9200" target="_blank" rel="noopener">http://yourip:9200</a>  就可以查看了</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 安装部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm高级以及优化</title>
      <link href="/2019/04/01/Storm%E9%AB%98%E7%BA%A7/"/>
      <url>/2019/04/01/Storm%E9%AB%98%E7%BA%A7/</url>
      
        <content type="html"><![CDATA[<h3 id="Storm高级"><a href="#Storm高级" class="headerlink" title="Storm高级"></a>Storm高级</h3><h4 id="Storm核心之流分组"><a href="#Storm核心之流分组" class="headerlink" title="Storm核心之流分组"></a>Storm核心之流分组</h4><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566219022407.png" alt="1566219022407"></p><h5 id="stream-grouping-分类"><a href="#stream-grouping-分类" class="headerlink" title="stream grouping 分类"></a>stream grouping 分类</h5><a id="more"></a><ul><li>Shuffle Grouping：随机分组。将stream中的tuple缓存后随机发放给所有bolt，可以使每个bolt中的数据量大致相等（可以较好的实现负载均衡）</li><li>Fields Grouping：按字段分组，例如按groupID字段进行分组，将同一个分组的tuple分到统一任务中</li><li>All Grouping:广播发送，每一个tuple都会发送到所有任务中，所以每一个bolt都会有所有的tuple</li><li>Global Grouping：全局分组，这个tuple会被分配到storm中的某一个bolt,具体一点就是分配到ID值最小的一个bolt之中 </li><li>Non Grouping：随机分派，效果和shuffle一样</li><li>Direct Grouping：直接分组，将tuple发送给制定好的任务中</li><li>localOrShuffleGrouping：指如果目标Bolt 中的一个或者多个Task 和当前产生数据的Task在同一个Worker 进程里面，那么就走内部的线程间通信，将Tuple 直接发给在当前Worker进程的目的Task。否则，同shuffleGrouping。</li></ul><h4 id="Storm可靠性剖析"><a href="#Storm可靠性剖析" class="headerlink" title="Storm可靠性剖析"></a>Storm可靠性剖析</h4><h6 id="Storm可能出现的问题"><a href="#Storm可能出现的问题" class="headerlink" title="Storm可能出现的问题"></a>Storm可能出现的问题</h6><ul><li>worker进程死掉</li><li>supervisor进程死掉</li><li>nimbus进程死掉</li><li>节点宕机</li></ul><h6 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h6><ul><li>(acker机制)ack/fail消息确认机制(确保一个tuple被完全处理)<ul><li>在spout中发射tuple的时候需要同时发送messageid，这样才相当于开启了消息确认机制</li><li>如果你的topology里面的tuple比较多的话，那么把acker的数量设置多一点,效率会高一点。</li><li>通过config.setNumAckers(num)来设置一个topology里面的acker的数量，默认值是1。</li><li>注意：acker用了特殊的算法，使得对于追踪每个spout tuple的状态所需要的内存量是恒定的（20 bytes) </li><li>注意：如果一个tuple在指定的timeout(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS默认值为30秒)时间内没有被成功处理，那么这个tuple会被认为处理失败了。</li></ul></li></ul><h4 id="Storm定时器分析"><a href="#Storm定时器分析" class="headerlink" title="Storm定时器分析"></a>Storm定时器分析</h4><ul><li>可以指定每隔一段时间将数据整合一次存入数据库<ul><li>在main中设置conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, 60);// 设置本Bolt定时发射数据</li><li>在bolt中使用下面代码判断是否是触发用的bolt tuple.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID)</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566305060434.png" alt="1566305060434"></li></ul></li></ul><h4 id="StormUI的详解"><a href="#StormUI的详解" class="headerlink" title="StormUI的详解"></a>StormUI的详解</h4><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566219892284.png" alt="1566219892284"></p><ul><li>deactive：未激活(暂停)</li><li>emitted:emitted tuple数<ul><li>与emitted的区别：如果一个task，emitted一个tuple到2个task中，则transferred tuple数是emitted tuple数的两倍</li></ul></li><li>completelatency: spout emitting 一个tuple到spout ack这个tuple的平均时间(可以认为是tuple以及该tuple树的整个处理时间)</li><li>processlatency:   bolt收到一个tuple到bolt ack这个tuple的平均时间，如果没有启动acker机制，那么值为0</li><li>execute latency：bolt处理一个tuple的平均时间，不包含acker操作，单位是毫秒(也就是bolt<br>执行 execute 方法的平均时间)</li><li>capacity：这个值越接近1，说明bolt或者spout基本一直在调用execute方法，说明并行度不够，需要扩展这个组件的executor数量。(调整组件并行度的依据)</li><li>总结：execute latency和proces latnecy是处理消息的时效性，而capacity则表示处理能力是否已经饱和，从这3个参数可以知道topology的瓶颈所在。</li></ul><h4 id="Storm的优化"><a href="#Storm的优化" class="headerlink" title="Storm的优化"></a>Storm的优化</h4><h5 id="并行度的优化"><a href="#并行度的优化" class="headerlink" title="并行度的优化"></a>并行度的优化</h5><ul><li>worker为storm提供工作进程，程序的并行度可以设置（包括spout和bolt的并行度，如果有acker的话还包括acker的并行度），并行度即为executor的数目。</li><li>一般情况下worker与executor的比例是一比十到十五，也可以根据实际需要修改。</li></ul><h5 id="worker的优化"><a href="#worker的优化" class="headerlink" title="worker的优化"></a>worker的优化</h5><ul><li>CPU 16核，建议配置20个worker。CPU 24或32核，30个worker</li><li>默认情况下，Storm启动worker进程时，JVM的最大内存是768M，可以通过在Strom的配置文件storm.yaml中设置worker的启动参数worker.childopts: “-Xmx2048m”</li><li>一个topology使用的worker数量，12个是比较合理的，这个时候吞吐量和整体性能最优。如果多增加worker进程的话，会将一些原本线程间的内存通信变为进程间的网络通信。</li></ul><h5 id="acker优化"><a href="#acker优化" class="headerlink" title="acker优化"></a>acker优化</h5><ul><li>如果可靠性对你来说不是那么重要，那么你可以通过不跟踪这些tuple树来获取更好的性能。不去跟踪消息的话会使得系统里面的消息数量减少一半，因为对于每一个tuple都要发送一个ack消息。</li><li>三种去掉可靠性的方法<ul><li>第一是把config.setNumAckers(0)设置为0，在这种情况下，storm会在spout发射一个tuple之后马上调用spout的ack方法。也就是说这个tuple树不会被跟踪。</li><li>第二个方法是在tuple层面去掉可靠性。你可以在发射tuple的时候不指定messageid来达到不跟踪spout中tuple的目的。</li><li>最后一个方法是如果你对于一个tuple树里面的某一部分到底成不成功不是很关心，那么可以在发射这些tuple的时候unanchor它们(anchor是锚定的意思，unanchor表示不把当前这个tuple包含到tuple树中，也就是说不跟踪这个消息了)。这样这些tuple就不在tuple树里面， 也就不会被跟踪了。</li></ul></li></ul><h4 id="雪崩问题的出现原因以及解决方法"><a href="#雪崩问题的出现原因以及解决方法" class="headerlink" title="雪崩问题的出现原因以及解决方法"></a>雪崩问题的出现原因以及解决方法</h4><ul><li>原因：spout发送的速度大于bolt接收的速度，导致数据堆积，不断消耗内存，最终系统崩溃，并引起数据链上多节点down掉。</li><li>解决方案<ul><li>增加bolt的并行度 增加它接收的速度</li><li>可以通过topology.max.spout.pending来控制spout发送消息的速度，通过代码这样设置config.setMaxSpoutPending(num);<ul><li>注意：这个参数表示，当下游的bolt还有topology.max.spout.pending个 tuple 没有消费完时，spout会停止调用nexttuple方法发射数据。等待下游bolt去消费，当tuple的个数少于topology.max.spout.pending个数时，spout 会继续发射数据(这个属性只对可靠消息处理有用，也就是说需要启用acker消息确认机制，在spout中emit数据的时候需要带有messageid)</li></ul></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> storm storm优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch简介</title>
      <link href="/2019/04/01/Elasticsearch%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/04/01/Elasticsearch%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h1><h3 id="Elasticsearch简介"><a href="#Elasticsearch简介" class="headerlink" title="Elasticsearch简介"></a>Elasticsearch简介</h3><p>​    Elasticsearch是一个实时分布式搜索和分析引擎。它对Lucene进行了封装。能够满足实时搜索的稳定、可靠、快速等。基于REST接口。</p><a id="more"></a><h4 id="ES与MySQL的对比"><a href="#ES与MySQL的对比" class="headerlink" title="ES与MySQL的对比"></a>ES与MySQL的对比</h4><table><thead><tr><th>Elasticsearch</th><th>MySQL</th></tr></thead><tbody><tr><td>index 索引库</td><td>database 数据库</td></tr><tr><td>type 类型</td><td>table 类型</td></tr><tr><td>document 文档</td><td>row 行</td></tr><tr><td>field 字段</td><td>column 列</td></tr></tbody></table><h3 id="Elasticsearch安装部署"><a href="#Elasticsearch安装部署" class="headerlink" title="Elasticsearch安装部署"></a>Elasticsearch安装部署</h3><ol><li><p>安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了）</p></li><li><p>下载Elasticsearch </p><ul><li>网址：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3" target="_blank" rel="noopener">https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3</a></li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566399839410.png" alt="1566399839410"></li><li>选择合适的版本下载就可以</li></ul></li><li><p>下载完成以后上传到Linux系统中，解压</p></li><li><p>进入到解压后的文件中尝试进行开启</p><ul><li>bin/elasticsearch （-d 后台运行）</li><li>注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off)</li><li>会发现执行报错  就是不可以在root用户下打开，选择一个其他用户就可以了</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398085045.png" alt="1566398085045"></li></ul></li><li><p>修改配置变量</p><ul><li><p>修改Elasticsearch中config变量 （vi config/elasticsearch.yml)</p></li><li><p>增加两行代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bootstrap.system_call_filter: false  </span><br><span class="line">network.host: 192.168.32.110   //后面的的IP设置你自己的本地IP就可以</span><br></pre></td></tr></table></figure></li></ul></li></ol><ul><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398675928.png" alt="1566398675928"></p></li><li><p>修改Linux的配置变量</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line">vi /etc/sysctl.conf</span><br><span class="line">vm.max_map_count=262144</span><br><span class="line">vi /etc/security/limits.d/90-nproc.conf </span><br><span class="line">把1024修改为4096</span><br><span class="line">*          soft    nproc     4096</span><br></pre></td></tr></table></figure></li><li><p>修改完配置以后重启操作系统在启动就可以啦</p></li><li><p>启动之后出现以下情况</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456097622.png" alt="1566456097622"></p></li><li><p>没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456158512.png" alt="1566456158512"></p></li><li><p>然后可以使用web界面查看 在浏览器输入 <a href="http://yourip:9200" target="_blank" rel="noopener">http://yourip:9200</a>  就可以查看了</p></li></ul><h3 id="简单基本操作"><a href="#简单基本操作" class="headerlink" title="简单基本操作"></a>简单基本操作</h3><h4 id="CURL简介"><a href="#CURL简介" class="headerlink" title="CURL简介"></a>CURL简介</h4><ul><li>curl起始就是一个可以在命令行下访问URL的工具</li><li>curl可以利用URL语法在命令行的方式下操作开源的文件</li><li>这样即可以方便我们其他不同部门对我们数据库的操作，也方便我们管理数据库，方便管理其他用户的权限</li></ul><h4 id="CURL的简单操作"><a href="#CURL的简单操作" class="headerlink" title="CURL的简单操作"></a>CURL的简单操作</h4><ul><li><p>-x 是指定http请求的方法</p><ul><li>他的类型有很多种包括 GET POST  PUT DELETE  查询、修改、增加、删除等很多操作</li></ul></li><li><p>-d 是指需要传递的参数</p></li><li><p>首先我们先<strong>创建</strong>一个简单的索引</p><ul><li>curl  -XPUT ‘<a href="http://localhost:9200/test/&#39;" target="_blank" rel="noopener">http://localhost:9200/test/&#39;</a>   localhost一定要换成你之前设置的IP</li><li>这样它就为我们创建了test索引库</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566457384502.png" alt="1566457384502"></li></ul></li><li><p>然后我们可以创建一个索引并为创建的索引添加一些内容然后进行一些列<strong>查询</strong></p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; //—H指定添加内容的类型为json类型</span><br><span class="line">-XPOST http://localhost:9200/test/emp/1 //1是指定索引的IP 不加系统也会自动生成</span><br><span class="line">-d &apos;&#123;</span><br><span class="line">&quot;name&quot; : &quot;tom&quot;,</span><br><span class="line">&quot;age&quot; : 25</span><br><span class="line">&#125;&apos;   //加入索引中的具体内容</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458468549.png" alt="1566458468549"></p></li><li><p>查询我们刚刚创建的索引</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://localhost:9200/test/emp/1?pretty</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458541679.png" alt="1566458541679"></p></li><li><p>检索索引中的一部分内容</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET &apos;http://localhost:9200/test/emp/1?_source=name&amp;pretty&apos;</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458667647.png" alt="1566458667647"></p></li><li><p>查询指定索引库指定类型的所有数据</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://hadoop110:9200/test/emp/_search?pretty </span><br><span class="line">查看tem类型下的所有数据</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458873757.png" alt="1566458873757"></p></li></ul></li><li><p>对ES进行<strong>更新</strong>操作，ES中可以使用put或者post两种方式进行更新操作</p><ul><li><p>执行更新操作的时候ES的操作细节</p><ul><li>首先将旧的文件标记为删除状态</li><li>添加新的文件</li><li>旧文件不会立即消失但是我们看不见</li><li>ES在后续你添加更多文件的时候在后台清理掉标记为删除状态的文件</li></ul></li><li><p>执行局部更新的操作</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPOST  http://hadoop110:9200/test/emp/1/_update -d &apos;&#123;&quot;doc&quot;:&#123;&quot;age&quot;:20&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li><li><p>我们接着进行一次查询看数据是否已经更新</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566459328843.png" alt="1566459328843"></p></li><li><p>可以看到年龄已将改成20了 所以说明更新操作成功了 我们可以根据这个操作做很多事情</p></li></ul></li><li><p>对ES进行<strong>删除</strong>操作</p><ul><li><p>删除我们之前创建的索引</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XDELETE http://hadoop110:9200/test/emp/1</span><br></pre></td></tr></table></figure></li><li><p>删除以后我们在进行get获取操作就会报错说明我们的删除操作已经执行成功了</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566459547355.png" alt="1566459547355"></p></li><li><p>如果删除文档存在 则会返回：200 ok的状态码，found属性值为true，_version属性的值+1</p></li><li><p>如果想要删除的文件不存在就会返回：404 NotFound的状态码，found属性值为false，但是_version属性的值依然会+1，这个就是内部管理的一部分，它保证了我们在多个节点间的不同操作的顺序都被正确标记了</p></li></ul></li><li><p>对ES进行批量操作 包括很多步的增删改查等</p><ul><li><p>批量操作就是bulk API帮助我们同时执行多个操作</p></li><li><p>语法的格式：</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">action：index/create/update/delete  //需要执行的操作类型</span><br><span class="line">metadata：_index,_type,_id   //指定需要操作的索引的索引库、类型、ID等</span><br><span class="line">request body：_source(删除操作不需要)  </span><br><span class="line">&#123; action: &#123; metadata &#125;&#125;   //具体要执行的操作</span><br><span class="line">&#123; request body        &#125;</span><br><span class="line">.........</span><br></pre></td></tr></table></figure></li><li><p>create与index的区别</p><ul><li>在创建数据时，如果数据已存在 create会返回创建失败，文件已存在，但是index会执行成功</li></ul></li><li><p>使用方法： 我们创建一个文件保存我们需要执行的操作</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi requests</span><br><span class="line">&#123; &quot;index&quot; : &#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;emp&quot;,&quot;_id&quot;:&quot;21&quot;&#125;&#125;</span><br><span class="line">&#123; &quot;name&quot; : &quot;test21&quot;&#125;</span><br><span class="line">执行：</span><br><span class="line">curl -H &quot;Content-Type: application/json&quot;  -XPUT localhost:9200/test/emp/_bulk --data-binary @requests</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566461216320.png" alt="1566461216320"></p></li><li><p>出现下面结果表示执行成功了</p></li><li><p>我们可以放多条指令进去 同时执行多条指令但是要保证中间格式不出错</p></li></ul></li></ul><h3 id="插件的介绍"><a href="#插件的介绍" class="headerlink" title="插件的介绍"></a>插件的介绍</h3><h4 id="Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES"><a href="#Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES" class="headerlink" title="Elasticsearch Head Plugin站点插件可以以网页形式展现ES"></a>Elasticsearch Head Plugin站点插件可以以网页形式展现ES</h4><ul><li><p>注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt</p><ul><li><p>nodejs下载地址<a href="https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz" target="_blank" rel="noopener">https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz</a></p><ul><li>复制此链接就可以直接下或者<a href="https://nodejs.org/dist" target="_blank" rel="noopener">https://nodejs.org/dist</a> 使用这个两节找适合自己的版本</li></ul></li><li><p>因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压  如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">解压：</span><br><span class="line">xz -d node-v10.15.3-linux-x64.tar.xz</span><br><span class="line">tar -xvf node-v10.15.3-linux-x64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/node  /usr/bin/node</span><br><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm</span><br></pre></td></tr></table></figure></li><li><p>设定nodejs安装软件的dialing服务器</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li><li><p>安装grunt</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g grunt </span><br><span class="line">npm install -g grunt-cli</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt  /usr/bin/grunt</span><br></pre></td></tr></table></figure></li><li><p>安装phantomjs</p><ul><li>下载地址<a href="http://phantomjs.org/download.html" target="_blank" rel="noopener">http://phantomjs.org/download.html</a></li></ul></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2</span><br><span class="line">tar -xvf phantomjs-2.1.1-linux-x86_64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs  /usr/bin/phantomjs</span><br></pre></td></tr></table></figure></li><li><p>安装依赖软件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install wget fontconfig</span><br></pre></td></tr></table></figure></li></ul></li><li><p>安装head插件</p><ul><li><p>下载地址：<a href="https://github.com/mobz/elasticsearch-head/archive/master.zip" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head/archive/master.zip</a></p></li><li><p>解压 需要使用unzip命令</p></li><li><p>下载unzip</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y unzip</span><br></pre></td></tr></table></figure></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip elasticsearch-head-master.zip</span><br></pre></td></tr></table></figure></li><li><p>然后cd进入到解压的文件中</p></li><li><p>安装两个插件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm audit fix</span><br><span class="line">npm audit fix --force</span><br></pre></td></tr></table></figure></li><li><p>执行安装命令安装head</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install</span><br></pre></td></tr></table></figure></li><li><p>启动之前修改Gruntfile.js文件，增加hostname参数</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi Gruntfile.js</span><br><span class="line">options: &#123;</span><br><span class="line">hostname: &apos;hadoop100&apos;,</span><br><span class="line">port: 9101,</span><br><span class="line">base: &apos;.&apos;,</span><br><span class="line">keepalive: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>启动服务</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grunt Server</span><br></pre></td></tr></table></figure></li><li><p>启动服务后还需要修改Elasticsearch中的一些配置</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span><br><span class="line">http.cors.enabled: true </span><br><span class="line">http.cors.allow-origin: &quot;*&quot;</span><br></pre></td></tr></table></figure></li><li><p>修改完成后重启ES</p></li><li><p>进入网址<a href="http://hadoop100:9101/可以看到以下信息就说明插件安装成功了" target="_blank" rel="noopener">http://hadoop100:9101/可以看到以下信息就说明插件安装成功了</a></p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566465169785.png" alt="1566465169785"></p></li></ul></li></ul><h3 id="配置参数详解"><a href="#配置参数详解" class="headerlink" title="配置参数详解"></a>配置参数详解</h3><ul><li>配置文件elasticsearch.yml</li><li>ES已经为大多数的参数设置了合理的默认值 我们只需要在有特殊需求的时候进行修改</li><li>书写规范<ul><li>属性顶格写，不能有空格</li><li>缩进一定要是用空格而不能使用制表符</li><li>属性与属性值之间必须有一个空格</li></ul></li><li>常见的配置文件以及其含义<ul><li>cluster.name:   集群名称</li><li>node.name  节点名称</li><li>path.data: /path/to/data     es的数据存储目录</li><li>path.logs: /path/to/logs   es的日志存储目录</li><li>bootstrap.memory_lock: true   锁定物理内存地址，防止elasticsearch内存被交换出去,也就是避免es使用swap交换分区中的内存</li><li>network.host: 192.168.0.1   为es设置ip绑定</li><li>http.port: 9200   为es设置自定义端口，默认是9200</li><li>discovery.zen.ping.unicast.hosts: [“host1”, “host2”]    当启动新节点时，通过这个ip列表进行节点发现，组建集群</li><li>discovery.zen.minimum_master_nodes:   通过配置这个参数来防止集群脑裂现象 (集群总节点数量/2)+1</li><li>gateway.recover_after_nodes: 3   一个集群中的N个节点启动后,才允许进行数据恢复处理，默认是1</li><li>action.destructive_requires_name: true   设置是否可以通过正则或者_all删除或者关闭索引库</li></ul></li></ul><h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><ul><li><p>cluster</p><ul><li>代表的是一个集群，集群中有很多节点，其中有一个主节点，这个主节点通过选举产生，主从节点时对于集群内部而言的。es有一个概念叫去中心化，就是说没有中心节点，这个是对于外部来说的，在外部来看，集群就是一个整体，我们两节集群中的任何一个节点与集群通信跟直接与集群通信是等价的。</li><li>主节点的主要职责就是负责管理集群的状态，包括管理分片以及副本的状态，以及节点的删除、新节点的发现等</li><li>注意：主节点不负责对进群的增删改查处理，只负责管理集群状态</li></ul></li><li><p>shards</p><ul><li><p>代表的是索引分片，ES将一个完整的索引分成多个分片，这样的好处是可以把一个大的索引分成多个分片后分布到不同的节点上，构成分布式搜索。提高性能和吞吐量</p></li><li><p>分片的的数量只能在创建索引库的时候指定，索引库创建以后不可以更改</p></li><li><p>索引库默认是5个分片 每个分片最多存储2,147,483,519条数据</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_shards&quot;:3&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>replicas</p><ul><li><p>代表的是分片的副本，es给分片设置副本是为了提高系统的容错性，当某个节点的某个分片损坏或者丢失了可以从副本中恢复。</p></li><li><p>提高es的查询效率，es会自动搜索并请求进行负载均衡</p></li><li><p>默认每个分区只有一个副本，主副本不会存在于一个节点之上，副本数量可以在创建索引库的时候进行设置吧</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_replicas&quot;:2&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>recovery</p><ul><li>代表数据的恢复或者数据的重新分布</li><li>es在所有节点的加入或者退出后会根据机器的负载对索引分片进行重新分配，挂掉的节点重启时也会进行数据恢复</li></ul></li></ul><h3 id="ElasticsearchJavaAPI操作"><a href="#ElasticsearchJavaAPI操作" class="headerlink" title="ElasticsearchJavaAPI操作"></a>ElasticsearchJavaAPI操作</h3><h4 id="使用Java对ES进行操作"><a href="#使用Java对ES进行操作" class="headerlink" title="使用Java对ES进行操作"></a>使用Java对ES进行操作</h4><ul><li><p>添加maven依赖</p><ul><li><p>可以maven仓库中寻找适合你的版本</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;transport&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;6.4.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Java中对ES的简单增删改查操作</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">package EsTest;</span><br><span class="line"></span><br><span class="line">import org.elasticsearch.action.delete.DeleteResponse;</span><br><span class="line">import org.elasticsearch.action.get.GetResponse;</span><br><span class="line">import org.elasticsearch.action.index.IndexResponse;</span><br><span class="line">import org.elasticsearch.action.update.UpdateResponse;</span><br><span class="line">import org.elasticsearch.client.transport.TransportClient;</span><br><span class="line">import org.elasticsearch.common.settings.Settings;</span><br><span class="line">import org.elasticsearch.common.transport.TransportAddress;</span><br><span class="line">import org.elasticsearch.common.xcontent.XContentType;</span><br><span class="line">import org.elasticsearch.transport.client.PreBuiltTransportClient;</span><br><span class="line"></span><br><span class="line">import java.net.InetAddress;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">/**</span><br><span class="line"> * Es简单操作测试</span><br><span class="line"> */</span><br><span class="line">public class EsDemo1 &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        //给集群添加自动嗅探的功能</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;) //集群名称</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, true)  //开启自动嗅探功能，可以自动识别集群内的其他节点信息</span><br><span class="line">                .build();</span><br><span class="line">        //创建连接</span><br><span class="line">        TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)</span><br><span class="line">                //可添加多个节点</span><br><span class="line">                //.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop100&quot;), 9300))</span><br><span class="line">                .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300));</span><br><span class="line"></span><br><span class="line">        //获取节点的信息</span><br><span class="line">        int size = client.connectedNodes().size();</span><br><span class="line">        //System.out.println(size);</span><br><span class="line">        String index = &quot;test&quot;;</span><br><span class="line">        String type = &quot;emp&quot;;</span><br><span class="line">        //添加数据 使用json字符串</span><br><span class="line">        String json = &quot;&#123;\&quot;name\&quot;:\&quot;jack\&quot;,\&quot;age\&quot;:10&#125;&quot;;</span><br><span class="line">        IndexResponse res = client.prepareIndex(index, type, &quot;1&quot;)</span><br><span class="line">                .setSource(json, XContentType.JSON).get();</span><br><span class="line">        //System.out.println(res.toString());</span><br><span class="line">        //添加数据 使用map结构</span><br><span class="line">        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();</span><br><span class="line">        map.put(&quot;name&quot;,&quot;zs&quot;);</span><br><span class="line">        map.put(&quot;age&quot;,21);</span><br><span class="line">        IndexResponse res2 = client.prepareIndex(index, type, &quot;101&quot;)</span><br><span class="line">                .setSource(map)</span><br><span class="line">                .execute()</span><br><span class="line">                .actionGet();</span><br><span class="line">        //更新操作 update</span><br><span class="line">        UpdateResponse updateResponse = client.prepareUpdate(index, type, &quot;101&quot;).setDoc(&quot;&#123;\&quot;age\&quot;:18&#125;&quot;, XContentType.JSON).get();</span><br><span class="line">        //根据ID进行数据查询</span><br><span class="line">        GetResponse get1 = client.prepareGet(index, type, &quot;101&quot;).get();</span><br><span class="line">        System.out.println(get1.getSourceAsString());</span><br><span class="line">        //删除操作 delete</span><br><span class="line">        DeleteResponse deleteResponse = client.prepareDelete(index, type, &quot;101&quot;).get();</span><br><span class="line">        System.out.println(deleteResponse.toString());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm 基础</title>
      <link href="/2019/03/21/Storm%E7%9A%84%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/"/>
      <url>/2019/03/21/Storm%E7%9A%84%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h3 id="Storm的详细分析"><a href="#Storm的详细分析" class="headerlink" title="Storm的详细分析"></a>Storm的详细分析</h3><h4 id="Storm人的概述"><a href="#Storm人的概述" class="headerlink" title="Storm人的概述"></a>Storm人的概述</h4><ul><li>Storm是Twitter开源的一个实时处理框架</li><li>Storm能够实现高频数据和大规模数据的实时处理</li></ul><a id="more"></a><h5 id="Storm与MapReduce的区别Storm"><a href="#Storm与MapReduce的区别Storm" class="headerlink" title="Storm与MapReduce的区别Storm"></a>Storm与MapReduce的区别Storm</h5><table><thead><tr><th>type</th><th>MapReduce</th><th>Storm</th></tr></thead><tbody><tr><td>数据来源</td><td>hdfs上TB级别历史数据</td><td>实时新增的某一条数据</td></tr><tr><td>处理过程</td><td>map阶段和reduce阶段</td><td>可以有很多阶段包含spout以及bolt</td></tr><tr><td>是否会结束</td><td>执行完结束</td><td>不会结束</td></tr><tr><td>处理速度</td><td>主要以执行TB级别数据速度较慢</td><td>只处理新增数据速度很快</td></tr><tr><td>适用场景</td><td>处理批数据不讲时效性</td><td>处理新增数据将时效性</td></tr></tbody></table><h5 id="Spark-Streaming与Storm的区别"><a href="#Spark-Streaming与Storm的区别" class="headerlink" title="Spark Streaming与Storm的区别"></a>Spark Streaming与Storm的区别</h5><table><thead><tr><th>type</th><th>Spark Streaming</th><th>Storm</th></tr></thead><tbody><tr><td>计算模型</td><td>是近实时处理框架</td><td>全实时处理框架</td></tr><tr><td>延迟度</td><td>最高支持秒级别的延迟</td><td>可以支持毫秒级别的延迟</td></tr><tr><td>吞吐量</td><td>因为是批处理所以吞吐量高</td><td>吞吐量相对来说较低</td></tr><tr><td>动态调整并行度</td><td>不支持</td><td>支持</td></tr><tr><td>事务机制</td><td>支持但是不够完善</td><td>支持且完善</td></tr></tbody></table><h5 id="Storm各个组件解释"><a href="#Storm各个组件解释" class="headerlink" title="Storm各个组件解释"></a>Storm各个组件解释</h5><ul><li>Topology：用于封装一个实时计算应用程序的逻辑</li><li>Stream：消息流，是一个没有边界的tuple序列，这些tuple会以分布式的方式进行创建以及处理</li><li>Spout：消息源，消息的生产者，会从外部源获取消息然后向Topology发出：tuple</li><li>Bolt：消息处理者，消息的处理逻辑被封装到bolt之中，处理输入的数据然后产生新的输出数据流</li></ul><h5 id="Storm的设计思想"><a href="#Storm的设计思想" class="headerlink" title="Storm的设计思想"></a>Storm的设计思想</h5><ul><li>是对stream流的一个抽象即一个不间断的连续tuple</li><li>将流中的元素抽象为一个tuple，一个tuple就是一个值列表value list，list中的每个value都有一个name，并且这个value可以是很多数据类型例如基本类型、字符类型等</li><li>每一个stream流都有一个数据源，称为Spout</li><li>stream从spout中获取不间断数据tuple需要经过处理。处理的过程就是stream流转换的过程称为bolt，bolt可以消费任意数量的流，它是将stream汇总的tuple挨个实时进行处理转换成一个新的stream流经过多个bolt处理就可以得到目标数据</li><li>spout+tuple+bolt这个过程可以称为是Topology拓扑。Topology是Storm中最高的一个抽象概念他可以被提交到集群中执行</li><li>Topology的每个节点都要指定他所发射数据的name，其他节点只需要订阅该name就可以接收数据进行处理</li></ul><h5 id="Topology的整个流程"><a href="#Topology的整个流程" class="headerlink" title="Topology的整个流程"></a>Topology的整个流程</h5><ul><li>如果将stream比作是一列火车的话 spout就是这列火车的始发站每一节车厢就是一个tuple乘客就是tuple中的values 中间的站点就相当于是bolt进行处理上下乘客终点站就相当于stream的目标数据</li></ul><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566209818229.png" alt="1566209818229"></p><h5 id="Storm的整体架构图"><a href="#Storm的整体架构图" class="headerlink" title="Storm的整体架构图"></a>Storm的整体架构图</h5><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566215107025.png" alt="1566215107025"></p><h5 id="Storm的简单实例开发"><a href="#Storm的简单实例开发" class="headerlink" title="Storm的简单实例开发"></a>Storm的简单实例开发</h5><ul><li><p>需求：一个源源不断的数据1，2，3，4……求每出现一个数字就要计算出现的所有数字的和</p></li><li><p>开发过程</p><ul><li><p>在IDE中创建maven工程</p></li><li><p>在pom中添加、Storm依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;storm-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0.6&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">package Storme;</span><br><span class="line"></span><br><span class="line">import java.util.Map;</span><br><span class="line"></span><br><span class="line">import org.apache.storm.Config;</span><br><span class="line">import org.apache.storm.LocalCluster;</span><br><span class="line">import org.apache.storm.generated.StormTopology;</span><br><span class="line">import org.apache.storm.spout.SpoutOutputCollector;</span><br><span class="line">import org.apache.storm.task.OutputCollector;</span><br><span class="line">import org.apache.storm.task.TopologyContext;</span><br><span class="line">import org.apache.storm.topology.OutputFieldsDeclarer;</span><br><span class="line">import org.apache.storm.topology.TopologyBuilder;</span><br><span class="line">import org.apache.storm.topology.base.BaseRichBolt;</span><br><span class="line">import org.apache.storm.topology.base.BaseRichSpout;</span><br><span class="line">import org.apache.storm.tuple.Fields;</span><br><span class="line">import org.apache.storm.tuple.Tuple;</span><br><span class="line">import org.apache.storm.tuple.Values;</span><br><span class="line">import org.apache.storm.utils.Utils;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 需求：实现数字累加求和</span><br><span class="line"> * 分析：</span><br><span class="line"> * 需要有一个spout负责源源不断的产生从1开始的递增数字</span><br><span class="line"> * 还需要有一个bolt负责对spout产生的数据进行累加求和，并且把结果打印到控制台</span><br><span class="line"> * 最后把这个spout和bolt组装成一个topology</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public class WordCount &#123;</span><br><span class="line">/**</span><br><span class="line"> * 实现自己的数据源spout，</span><br><span class="line"> * 该spout负责源源不断产生从1开始的递增数字</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public static class MySpout extends BaseRichSpout&#123;</span><br><span class="line"></span><br><span class="line">private Map conf;//这里面存储配置信息</span><br><span class="line">private TopologyContext context;//代表上下文</span><br><span class="line">private SpoutOutputCollector collector;//收集器，主要负责向外面发射数据</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 是一个初始化的方法，这个方法在本实例运行的之后，首先被调用，仅且仅被调用一次</span><br><span class="line"> * 所以这个方法内一般放一些初始化的代码</span><br><span class="line"> * 例子：针对操作mysql数据的案例，使用jdbc获取数据库连接的代码需要放到这里面实现</span><br><span class="line"> */</span><br><span class="line">public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123;</span><br><span class="line">this.conf = conf;</span><br><span class="line">this.context = context;</span><br><span class="line">this.collector = collector;</span><br><span class="line">//System.err.println(&quot;spout------&quot;+conf.get(&quot;name&quot;));</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 这个方法会被循环调用</span><br><span class="line"> */</span><br><span class="line">int i = 1;</span><br><span class="line">public void nextTuple() &#123;</span><br><span class="line">//注意：针对需要发射的数据，需要封装成tuple，可以使用storm中的values对象快速封装tuple</span><br><span class="line">System.out.println(&quot;spout:&quot;+i);</span><br><span class="line">this.collector.emit(new Values(i++));</span><br><span class="line">//让线程每发射一条数据，休息1秒</span><br><span class="line">Utils.sleep(1000);</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 声明输出字段</span><br><span class="line"> * 定义两个组件之间数据传输的一个规则</span><br><span class="line"> * 注意：只要这个组件(spout/spout)向外发射了数据，那么这个declareOutputFields就需要实现</span><br><span class="line"> */</span><br><span class="line">public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;</span><br><span class="line">//注意：Fields中的字段列表和Values中的数据列表是一一对应的</span><br><span class="line">declarer.declare(new Fields(&quot;num&quot;));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 聚合的Bolt，负责把Spout发射出来的数据进行累加求和，并且打印到控制台</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public static class SumBolt extends BaseRichBolt&#123;</span><br><span class="line">private Map stormConf;</span><br><span class="line">private TopologyContext context; </span><br><span class="line">private OutputCollector collector;</span><br><span class="line">/**</span><br><span class="line"> * prepare是一个初始化方法，只会执行一次，这里面也是可以放一些初始化的代码</span><br><span class="line"> */</span><br><span class="line">public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123;</span><br><span class="line">this.stormConf = stormConf;</span><br><span class="line">this.context = context;</span><br><span class="line">this.collector = collector;</span><br><span class="line">//System.err.println(&quot;bolt------&quot;+stormConf.get(&quot;name&quot;));</span><br><span class="line">&#125;</span><br><span class="line">int sum = 0;</span><br><span class="line">/**</span><br><span class="line"> * 这个方法也是会被循环调用</span><br><span class="line"> * 主要上一个组件向外发射了数据，那么这个方法就会被调用一次</span><br><span class="line"> */</span><br><span class="line">public void execute(Tuple input) &#123;</span><br><span class="line">//input.getInteger(0);//通过角标获取数据</span><br><span class="line">Integer num = input.getIntegerByField(&quot;num&quot;);</span><br><span class="line">sum += num;</span><br><span class="line">System.out.println(&quot;和为：&quot;+sum);</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 注意：这个方法在这里就不需要实现了，因为这个bolt没有向下一个组件发射数据</span><br><span class="line"> */</span><br><span class="line">public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">TopologyBuilder builder = new TopologyBuilder();</span><br><span class="line">//组装spout</span><br><span class="line">builder.setSpout(&quot;spoutid&quot;, new MySpout());</span><br><span class="line">//组装bolt,并且告诉bolt接收哪个组件的数据</span><br><span class="line">builder.setBolt(&quot;bolt-1&quot;, new SumBolt()).shuffleGrouping(&quot;spoutid&quot;);</span><br><span class="line">StormTopology createTopology = builder.createTopology();</span><br><span class="line">//通过代码创建一个本地集群</span><br><span class="line">LocalCluster localCluster = new LocalCluster();</span><br><span class="line">String topologyName = WordCount.class.getSimpleName();</span><br><span class="line">Config config = new Config();</span><br><span class="line">config.put(&quot;name&quot;, &quot;zs&quot;);</span><br><span class="line">//把代码提交到本地集群中运行</span><br><span class="line">localCluster.submitTopology(topologyName, config, createTopology);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Storm核心之并行度"><a href="#Storm核心之并行度" class="headerlink" title="Storm核心之并行度"></a>Storm核心之并行度</h4><h5 id="组件解释"><a href="#组件解释" class="headerlink" title="组件解释"></a>组件解释</h5><ul><li>worker：worker是一个进程，每一个worker进程里面都执行的是一个Topology的任务（不会出现一个worker执行多个Topology的任务）。一个worker中会启动一个或多个executor线程来执行Topology的spout或者bolt组件。一个Topology会使用一个或者多个worker来执行任务</li><li>executor：是worker进程内部启动的独立线程，每一个executor会产生一个或者多个task（storm默认是一个task即一个spout或者bolt有一个task，如果有多个task，executor会循环调用所有task中的实例）</li><li>task：是最终运行spout或者bolt中具体代码的执行单元。Topology启动（spout或者bolt）的task的数目是不变的，但是executor线程的数量可以动态进行调整（例如：1个executor线程可以执行该(spout或bolt)的1个或多个task实例）。这意味着，对于1个(spout或bolt)存在这样的条件：#threads&lt;=#tasks（即：线程数小于等于task数目）。默认情况下task的数目等于executor线程数目，即1个executor线程只运行1个task。</li><li>默认情况下，一个supervisor节点最多可以启动4个worker进程，每一个topology默认占用一个worker进程，每个spout或者bolt会占用1个executor，每个executor启动1个task。</li></ul><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566213893905.png" alt="1566213893905"></p><h5 id="提高Storm组件的并行度"><a href="#提高Storm组件的并行度" class="headerlink" title="提高Storm组件的并行度"></a>提高Storm组件的并行度</h5><ul><li>worker（slot）【间接】<ul><li>默认一个节点最多可以启动四个worker进程，可以修改进程数量strom-core.jar/defaults.yaml/supervisor.slots.ports</li><li>默认一个Topology只有一个worker进程，可以通过代码指定一个Topology使用多个worker进程config.setNumWorkers(workersnum)</li><li>注意：如果worker使用完在提交Topology就不会执行，会处于等待状态。worker之是通过Netty通信的</li></ul></li><li>executor【直接】<ul><li>默认情况下一个executor只会运行一个task，可以直接通过代码修改增加task数量，会直接提高Storm组件的并行度</li><li>builder.setSpout(id, spout,parallelism_hint);</li><li>builder.setBolt(id, bolt,parallelism_hint);</li></ul></li><li>task【间接】<ul><li>通过boltDeclarer.setNumTasks(num);来设置实例的个数</li><li>executor的数量会小于等于task的数量(为了rebalance)</li></ul></li></ul><h5 id="弹性计算rebalance"><a href="#弹性计算rebalance" class="headerlink" title="弹性计算rebalance"></a>弹性计算rebalance</h5><ul><li>前提是Topology中的task数量要大于executor线程数</li><li>通过shell调整<ul><li>storm rebalance mytopology -w 10 -n 5 -e blue-spout=3 -e yellow-bolt=10</li><li>注意：acker的树木运行时是不会变化的，所以多指定几个worker进程，acker的数量也不会增加</li><li>-w：表示超时时间，Rebalance会在一个超时时间内注销掉Topology，然后在集群中重新分配worker</li><li>-n：表示的是worker的数量</li><li>-e：调整组件的并行度</li><li>注：-n 以及 -e 都可以单独使用或者组合起来使用</li></ul></li><li>通过UI界面进行调整，不建议使用所以就不具体解释使用方法了</li></ul><h5 id="并行度设置多少合适"><a href="#并行度设置多少合适" class="headerlink" title="并行度设置多少合适"></a>并行度设置多少合适</h5><ul><li>单spout每秒大概可以发送500个tuple</li><li>单bolt每秒大概可以接收2000个tuple</li><li>单acker每秒大概可以接收6000个tuple</li><li>根据需要进行调整</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maven 简介</title>
      <link href="/2019/01/21/maven%E7%9A%84%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/01/21/maven%E7%9A%84%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Maven-简介"><a href="#Maven-简介" class="headerlink" title="Maven 简介"></a>Maven 简介</h2><h3 id="为什么需要maven"><a href="#为什么需要maven" class="headerlink" title="为什么需要maven"></a>为什么需要maven</h3><ul><li>同样的代码要在不同的机器上运行他所需要的依赖可以放在maven仓库</li><li>项目组加入新成员可以快速的配置好环境</li><li>在开发其他项目的时候需要用到跟之前项目开发一样的jar包</li></ul><a id="more"></a><h3 id="maven是什么"><a href="#maven是什么" class="headerlink" title="maven是什么"></a>maven是什么</h3><ul><li>maven是基于项目对象模型POM的软件项目管理工具</li><li>是可以跨平台的，主要服务基于Java平台的仙姑构建、依赖管理、项目信息管理等</li></ul><h5 id="构建的过程"><a href="#构建的过程" class="headerlink" title="构建的过程"></a>构建的过程</h5><ul><li>清理</li><li>编译</li><li>测试</li><li>报告</li><li>打包</li><li>部署</li></ul><h3 id="maven的工程结构"><a href="#maven的工程结构" class="headerlink" title="maven的工程结构"></a>maven的工程结构</h3><ul><li>src </li><li><ul><li>mian</li><li><ul><li>java   – 存放Java的文件 源代码等</li><li>resource   –存放资源文件 比如 spring，hibernate等的配置文件</li></ul></li><li>test</li><li><ul><li>Java   – 存放所有的.Java的测试文件，比如JUnit 测试类</li><li>resource   –测试的资源文件夹</li></ul></li></ul></li><li>target       —目标文件的输出位置比如jar包、war包等</li><li>pom.xml      —maven的项目核心配置文件</li></ul><h3 id="maven常用命令"><a href="#maven常用命令" class="headerlink" title="maven常用命令"></a>maven常用命令</h3><ul><li>mvn compile  执行编译 会将生成文件存放在target目录中</li><li>mvn clean  删除target中的目录文件</li><li>mvn test    执行测试命令 执行后会在target目录中生成三个目录文件surefire、surefire-reports（测试报告）、test-classes（测试的字节码文件）</li><li>mvn  package 进行打包操作 操作后的文件存放在target目录之中 例如jar包war包</li><li>mvn install  将制定的jar包安装到本地仓库以便于其他工程的引用</li><li>mvn clean compile 清除测试类再执行compile执行编译操作</li><li>mvn clean test  先清除在进行test测试操作</li><li>mvn clean package 先执行clean清除在执行package打包</li><li>mvn clean install  先进行clean在执行install</li></ul>]]></content>
      
      
      <categories>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> maven </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
