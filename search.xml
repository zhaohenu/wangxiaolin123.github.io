<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Elasticsearch高级</title>
      <link href="/2019/05/11/Elasticsearch%E5%9F%BA%E7%A1%80/"/>
      <url>/2019/05/11/Elasticsearch%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h2 id="Elasticsearch高级"><a href="#Elasticsearch高级" class="headerlink" title="Elasticsearch高级"></a>Elasticsearch高级</h2><h3 id="Elasticsearch批量操作的查询类型"><a href="#Elasticsearch批量操作的查询类型" class="headerlink" title="Elasticsearch批量操作的查询类型"></a>Elasticsearch批量操作的查询类型</h3><h5 id="Bulk批量查询的Java实现"><a href="#Bulk批量查询的Java实现" class="headerlink" title="Bulk批量查询的Java实现"></a>Bulk批量查询的Java实现</h5><a id="more"></a><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">package EsTest;</span><br><span class="line"></span><br><span class="line">import org.elasticsearch.action.bulk.BulkItemResponse;</span><br><span class="line">import org.elasticsearch.action.bulk.BulkRequestBuilder;</span><br><span class="line">import org.elasticsearch.action.bulk.BulkResponse;</span><br><span class="line">import org.elasticsearch.action.delete.DeleteRequest;</span><br><span class="line">import org.elasticsearch.action.index.IndexRequest;</span><br><span class="line">import org.elasticsearch.client.transport.TransportClient;</span><br><span class="line">import org.elasticsearch.common.settings.Settings;</span><br><span class="line">import org.elasticsearch.common.transport.TransportAddress;</span><br><span class="line">import org.elasticsearch.transport.client.PreBuiltTransportClient;</span><br><span class="line"></span><br><span class="line">import java.net.InetAddress;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Es简单操作测试</span><br><span class="line"> */</span><br><span class="line">public class EsDemo2 &#123;</span><br><span class="line">    static String index = &quot;test&quot;;</span><br><span class="line">    static String type = &quot;emp&quot;;</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, true)</span><br><span class="line">                .build();</span><br><span class="line">        TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)</span><br><span class="line">                .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300));</span><br><span class="line">        testBulk(client);</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * bulk批量查询测试</span><br><span class="line">     */</span><br><span class="line">    public static  void testBulk(TransportClient client)&#123;</span><br><span class="line">        BulkRequestBuilder bulkRequestBuilder = client.prepareBulk();</span><br><span class="line">        //创建请求</span><br><span class="line">        IndexRequest indexRequest = new IndexRequest(index, type)</span><br><span class="line">                                    .source(&quot;&#123;\&quot;name\&quot;:\&quot;zs1\&quot;,\&quot;age\&quot;:25&#125;&quot;);</span><br><span class="line">        //删除请求</span><br><span class="line">        DeleteRequest deleteRequest = new DeleteRequest(index, type, &quot;21&quot;,,XContentType.JSON);</span><br><span class="line">        //将操作整合到buider中</span><br><span class="line">        bulkRequestBuilder.add(indexRequest);</span><br><span class="line">        bulkRequestBuilder.add(deleteRequest);</span><br><span class="line">        //执行批量操作</span><br><span class="line">        BulkResponse bulkItemResponses = bulkRequestBuilder.get();</span><br><span class="line">        //查看执行过程中失败的信息</span><br><span class="line">        if(bulkItemResponses.hasFailures())&#123;</span><br><span class="line">            BulkItemResponse[] items = bulkItemResponses.getItems();</span><br><span class="line">            for (BulkItemResponse item: items) &#123;</span><br><span class="line">                System.out.println(item.getFailureMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            System.out.println(&quot;所有bulk指令都执行成功了&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="SearchType的详解"><a href="#SearchType的详解" class="headerlink" title="SearchType的详解"></a>SearchType的详解</h4><ul><li><p>query and fetch :当客户机向es集群中的某一个节点发送请求时，这个节点会将请求复制到每一个节点上，然后每一个节点会将所请求的数据返回到查询节点上，然后由查询节点返回到客户机上，这样的优点就是速度快，缺点是不准确，客户想要10条数据，集群返回的是10*n条数据，n是集群的节点数</p></li><li><p>query then fetch:当客户机向集群发送请求时，集群中接收请求的节点也会将查询请求发送到每一个节点之上，但是每个节点只返回查询结果的ID等值给主节点，主节点将受到的数据在进行排序取出所需要的条数，然后根据其ID等到相应节点上取的数据，在将数据返回至客户机。优点是可以准确返回需要条数的请求，且结果相对来说准确，缺点是查询速度慢，是es的默认查询类型</p></li><li><p>DFS D是Distributed，F是frequency的缩写，S是Scatter的缩写，整个单词可能是分布式词频率和文档频率散发的缩写</p><ul><li>dfs简称是初始化散发</li><li>官方解释是初始化散发其实就是在进行真正的查询之前，先把各个分片的词频率和文档频率收集一下，然后进行词搜索的时候，各分片依据全局的词频率和文档频率进行搜索和排名。</li><li>通俗一点来说就是统计所有节点的搜索排名的算法，总结到一起可以对整个文档进行精确的算法排名</li></ul></li><li><p>dfs query and fetch：就是加了dfs的query and fetch依然是速度快，但是结果条数多</p></li><li><p>dfs query then fetch:执行过程：首先，从各个节点的搜索排序算法即词频率文档频率等，然后根据整合好的算法在每个节点上取出相应数据的ID等信息，在主节点上再次通过该算法获取准确的数据信息，在通过他们的ID等信息去各个节点获取具体数据返回至客户机上。优点是查询准确率高，但是查询速度慢</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566654050267.png" alt="1566654050267"></p></li><li><p>代码写法</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Es简单操作测试</span><br><span class="line"> */</span><br><span class="line">public class EsDemo2 &#123;</span><br><span class="line">    static String index = &quot;test&quot;;</span><br><span class="line">    static String type = &quot;emp&quot;;</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;)</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, true)</span><br><span class="line">                .build();</span><br><span class="line">        TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)</span><br><span class="line">                .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300));</span><br><span class="line">        //testBulk(client);</span><br><span class="line">        testSeType(client);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 查询类型的测试</span><br><span class="line">     */</span><br><span class="line">    public  static void testSeType(TransportClient client)&#123;</span><br><span class="line">        SearchResponse searchResponse = client.prepareSearch(index)  //索引库信息</span><br><span class="line">                .setQuery(QueryBuilders.matchAllQuery())  //查询规则 所有队列</span><br><span class="line">                .setPreference(&quot;_shards:1&quot;)  //指点分片</span><br><span class="line">                .setSearchType(SearchType.QUERY_THEN_FETCH)  //类型可以自己指定</span><br><span class="line">                .get();</span><br><span class="line">        SearchHits hits = searchResponse.getHits();</span><br><span class="line">        //获取总条数</span><br><span class="line">        long totalHits = hits.getTotalHits();</span><br><span class="line">        System.out.println(&quot;数据的总条数&quot;+totalHits);</span><br><span class="line">        //打印所有数据内容</span><br><span class="line">        SearchHit[] hits1 = hits.getHits();</span><br><span class="line">        for (SearchHit hit:hits1) &#123;</span><br><span class="line">            System.out.println(hit.getSourceAsString());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Elasticsearch分词详解"><a href="#Elasticsearch分词详解" class="headerlink" title="Elasticsearch分词详解"></a>Elasticsearch分词详解</h3><h4 id="es索引建立和搜索过程图解"><a href="#es索引建立和搜索过程图解" class="headerlink" title="es索引建立和搜索过程图解"></a>es索引建立和搜索过程图解</h4><ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566654133368.png" alt="sss"></li></ul><h4 id="倒排索引介绍"><a href="#倒排索引介绍" class="headerlink" title="倒排索引介绍"></a>倒排索引介绍</h4><ul><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566654223385.png" alt="1566654223385"></li><li><img src="C:%5CUsers%5CAdministrator%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1566654235494.png" alt="1566654235494"></li><li>表中的各个单词表示文档意思<ul><li>单词ID：记录每个单词的编号</li><li>单词：单词ID对应的单词</li><li>文档频率：单词在几个文档中出现过</li><li>倒排列表：单词出现的文档信息以及单词出现位置信息</li><li>DocID：单词出现的文档的ID</li><li>TF：单词在该文档出现的次数</li><li>POS：单词在文档中出现的位置</li></ul></li></ul><h4 id="分词器Analyzer的介绍"><a href="#分词器Analyzer的介绍" class="headerlink" title="分词器Analyzer的介绍"></a>分词器Analyzer的介绍</h4><ul><li><p>分词器就是将数据按以此为单位分开</p></li><li><p>分词器的作用</p><ul><li>是吧文本中的词按照一定的规则进行切分。</li><li>分词器所对应的的类是Analyzer是一个抽象类，具体的实现方法要靠他的子类，所以对于不同的语言就可以提供不同的分词器</li><li>在创建索引以及搜索的时候都会用到分词器，而且这两个过程所用到的分析器必须是同一种分词器</li></ul></li><li><p>分词器的工作流程</p><ul><li>切分关键词</li><li>取出停用词</li><li>对于英文字母，将所有字母转换为小写</li></ul></li><li><p>停用词的介绍</p><ul><li>有些词在文本中出现的概率很高但是对于文本所携带的信息并没有什么影响</li><li>英文中的<ul><li>a,an,the,of 等   <ul><li><a href="http://www.ranks.nl/stopwords" target="_blank" rel="noopener">http://www.ranks.nl/stopwords</a></li></ul></li></ul></li><li>中文的<ul><li>的，了，是，着，以及各种标点符号等等<ul><li><a href="http://www.ranks.nl/stopwords/chinese-stopwords" target="_blank" rel="noopener">http://www.ranks.nl/stopwords/chinese-stopwords</a></li></ul></li></ul></li><li>文本经过分词的过程以后，这种停用词一般都会被过滤掉，不会被索引</li><li>如果搜索的词含有停用词一本也会被过滤掉</li><li>过滤掉停用词可以加快建立索引，减小索引库的文件的大小</li></ul></li><li><p>几个重要的分词器介绍</p><ul><li><table><thead><tr><th align="center">分词器</th><th align="center">分词方式</th></tr></thead><tbody><tr><td align="center">StandardAnalyzer</td><td align="center">单词分词器</td></tr><tr><td align="center">ChineseAnalyzer</td><td align="center">单字分词器</td></tr><tr><td align="center">CJKAnalyzer</td><td align="center">二分法分词器</td></tr><tr><td align="center">IKAnalyzer</td><td align="center">词库分词器</td></tr></tbody></table></li><li><p>单字分词以及单词分词的意思是一样的</p><ul><li>“我们是中国人”效果：“我”“们”“是”“中”“国”“人”</li></ul></li><li><p>二分法分词器：按两个字的方式分词</p><ul><li>“我们是中国人”，效果：“我们”、“们是”、“是中”、“中国”、“国人”</li></ul></li><li><p>词库分词器</p><ul><li>按某种算法造词，然后将词存入到词库，把搜索内容匹配到词库的词然后进行拆分。</li></ul></li></ul></li></ul><h3 id="Elasticsearch分词插件介绍以及使用es-ik"><a href="#Elasticsearch分词插件介绍以及使用es-ik" class="headerlink" title="Elasticsearch分词插件介绍以及使用es-ik"></a>Elasticsearch分词插件介绍以及使用es-ik</h3><ul><li>官方默认的分词插件对中文的支持不是很好，所以我们需要采用第三方的词库来进行分词，IK就是一个分成不错的分词工具</li><li>下载地址<ul><li><a href="https://github.com/medcl/elasticsearch-analysis-ik/releases" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik/releases</a></li></ul></li><li>将下载好的文件放在ES_HOME/plugins/ik目录下解压</li><li>解压后就可以使用</li><li>自己添加词库<ul><li>进入到config文件中</li><li>创建一个自己存放自己词库的文件夹</li><li>在文件夹中创建dic文件将自己的词库内容放到所创建的文件中</li><li>修改IKAnalyzer.cfg.xml文件信息</li><li>将自己创建的词库加进去</li><li>重启es就可以使用自己创建的词库了</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566655870803.png" alt="1566655870803"></li></ul></li><li>实现热词的自动更新不需要重启es<ul><li>在一台服务器上部署一个Tomcat</li><li>在Tomcat中的webapp/ROOT 创建hot.doc热词词库</li><li>通过访问网络端口确定这个热词库可以访问</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566656094187.png" alt="1566656094187"></li><li>修改IK的配置<ul><li><entry key="remote_ext_dict"><a href="http://192.168.80.100:8080/hot.dic" target="_blank" rel="noopener">http://192.168.80.100:8080/hot.dic</a></entry></li></ul></li><li>重启es之后就可以在hot.dic文件中动态添加热词，IK会定时从端口中访问该文件然后进行更新热词</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 提高 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch IK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch Head Plugin</title>
      <link href="/2019/04/02/Elasticsearch%20Head%20Plugin%20%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
      <url>/2019/04/02/Elasticsearch%20Head%20Plugin%20%E8%AF%A6%E7%BB%86%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Elasticsearch-Head-Plugin-详细安装教程"><a href="#Elasticsearch-Head-Plugin-详细安装教程" class="headerlink" title="Elasticsearch Head Plugin 详细安装教程"></a>Elasticsearch Head Plugin 详细安装教程</h2><h4 id="Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES"><a href="#Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES" class="headerlink" title="Elasticsearch Head Plugin站点插件可以以网页形式展现ES"></a>Elasticsearch Head Plugin站点插件可以以网页形式展现ES</h4><a id="more"></a><ul><li><p>注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt</p><ul><li><p>nodejs下载地址<a href="https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz" target="_blank" rel="noopener">https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz</a></p><ul><li>复制此链接就可以直接下或者<a href="https://nodejs.org/dist" target="_blank" rel="noopener">https://nodejs.org/dist</a> 使用这个两节找适合自己的版本</li></ul></li><li><p>因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压  如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">解压：</span><br><span class="line">xz -d node-v10.15.3-linux-x64.tar.xz</span><br><span class="line">tar -xvf node-v10.15.3-linux-x64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/node  /usr/bin/node</span><br><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm</span><br></pre></td></tr></table></figure></li><li><p>设定nodejs安装软件的dialing服务器</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li><li><p>安装grunt</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g grunt </span><br><span class="line">npm install -g grunt-cli</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt  /usr/bin/grunt</span><br></pre></td></tr></table></figure></li><li><p>安装phantomjs</p><ul><li>下载地址<a href="http://phantomjs.org/download.html" target="_blank" rel="noopener">http://phantomjs.org/download.html</a></li></ul></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2</span><br><span class="line">tar -xvf phantomjs-2.1.1-linux-x86_64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs  /usr/bin/phantomjs</span><br></pre></td></tr></table></figure></li><li><p>安装依赖软件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install wget fontconfig</span><br></pre></td></tr></table></figure></li></ul></li><li><p>安装head插件</p><ul><li><p>下载地址：<a href="https://github.com/mobz/elasticsearch-head/archive/master.zip" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head/archive/master.zip</a></p></li><li><p>解压 需要使用unzip命令</p></li><li><p>下载unzip</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y unzip</span><br></pre></td></tr></table></figure></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip elasticsearch-head-master.zip</span><br></pre></td></tr></table></figure></li><li><p>然后cd进入到解压的文件中</p></li><li><p>安装两个插件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm audit fix</span><br><span class="line">npm audit fix --force</span><br></pre></td></tr></table></figure></li><li><p>执行安装命令安装head</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install</span><br></pre></td></tr></table></figure></li><li><p>启动之前修改Gruntfile.js文件，增加hostname参数</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi Gruntfile.js</span><br><span class="line">options: &#123;</span><br><span class="line">hostname: &apos;hadoop100&apos;,</span><br><span class="line">port: 9101,</span><br><span class="line">base: &apos;.&apos;,</span><br><span class="line">keepalive: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>启动服务</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grunt Server</span><br></pre></td></tr></table></figure></li><li><p>启动服务后还需要修改Elasticsearch中的一些配置</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span><br><span class="line">http.cors.enabled: true </span><br><span class="line">http.cors.allow-origin: &quot;*&quot;</span><br></pre></td></tr></table></figure></li><li><p>修改完成后重启ES</p></li><li><p>进入网址<a href="http://hadoop100:9101/可以看到以下信息就说明插件安装成功了" target="_blank" rel="noopener">http://hadoop100:9101/可以看到以下信息就说明插件安装成功了</a></p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566465169785.png" alt="1566465169785"></p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 安装部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch的安装部署</title>
      <link href="/2019/04/02/2019-04-2-Elasticsearch%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/04/02/2019-04-2-Elasticsearch%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h3 id="Elasticsearch安装部署"><a href="#Elasticsearch安装部署" class="headerlink" title="Elasticsearch安装部署"></a>Elasticsearch安装部署</h3><ol><li>安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了）</li></ol><a id="more"></a><ol><li><p>下载Elasticsearch </p><ul><li>网址：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3" target="_blank" rel="noopener">https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3</a></li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566399839410.png" alt="1566399839410"></li><li>选择合适的版本下载就可以</li></ul></li><li><p>下载完成以后上传到Linux系统中，解压</p></li><li><p>进入到解压后的文件中尝试进行开启</p><ul><li>bin/elasticsearch （-d 后台运行）</li><li>注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off)</li><li>会发现执行报错  就是不可以在root用户下打开，选择一个其他用户就可以了</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398085045.png" alt="1566398085045"></li></ul></li><li><p>修改配置变量</p><ul><li><p>修改Elasticsearch中config变量 （vi config/elasticsearch.yml)</p></li><li><p>增加两行代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bootstrap.system_call_filter: false  </span><br><span class="line">network.host: 192.168.32.110   //后面的的IP设置你自己的本地IP就可以</span><br></pre></td></tr></table></figure></li></ul></li></ol><ul><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398675928.png" alt="1566398675928"></p></li><li><p>修改Linux的配置变量</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line">vi /etc/sysctl.conf</span><br><span class="line">vm.max_map_count=262144</span><br><span class="line">vi /etc/security/limits.d/90-nproc.conf </span><br><span class="line">把1024修改为4096</span><br><span class="line">*          soft    nproc     4096</span><br></pre></td></tr></table></figure></li><li><p>修改完配置以后重启操作系统在启动就可以啦</p></li><li><p>启动之后出现以下情况</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456097622.png" alt="1566456097622"></p></li><li><p>没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456158512.png" alt="1566456158512"></p></li><li><p>然后可以使用web界面查看 在浏览器输入 <a href="http://yourip:9200" target="_blank" rel="noopener">http://yourip:9200</a>  就可以查看了</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 安装部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm高级以及优化</title>
      <link href="/2019/04/01/Storm%E9%AB%98%E7%BA%A7/"/>
      <url>/2019/04/01/Storm%E9%AB%98%E7%BA%A7/</url>
      
        <content type="html"><![CDATA[<h3 id="Storm高级"><a href="#Storm高级" class="headerlink" title="Storm高级"></a>Storm高级</h3><h4 id="Storm核心之流分组"><a href="#Storm核心之流分组" class="headerlink" title="Storm核心之流分组"></a>Storm核心之流分组</h4><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566219022407.png" alt="1566219022407"></p><h5 id="stream-grouping-分类"><a href="#stream-grouping-分类" class="headerlink" title="stream grouping 分类"></a>stream grouping 分类</h5><a id="more"></a><ul><li>Shuffle Grouping：随机分组。将stream中的tuple缓存后随机发放给所有bolt，可以使每个bolt中的数据量大致相等（可以较好的实现负载均衡）</li><li>Fields Grouping：按字段分组，例如按groupID字段进行分组，将同一个分组的tuple分到统一任务中</li><li>All Grouping:广播发送，每一个tuple都会发送到所有任务中，所以每一个bolt都会有所有的tuple</li><li>Global Grouping：全局分组，这个tuple会被分配到storm中的某一个bolt,具体一点就是分配到ID值最小的一个bolt之中 </li><li>Non Grouping：随机分派，效果和shuffle一样</li><li>Direct Grouping：直接分组，将tuple发送给制定好的任务中</li><li>localOrShuffleGrouping：指如果目标Bolt 中的一个或者多个Task 和当前产生数据的Task在同一个Worker 进程里面，那么就走内部的线程间通信，将Tuple 直接发给在当前Worker进程的目的Task。否则，同shuffleGrouping。</li></ul><h4 id="Storm可靠性剖析"><a href="#Storm可靠性剖析" class="headerlink" title="Storm可靠性剖析"></a>Storm可靠性剖析</h4><h6 id="Storm可能出现的问题"><a href="#Storm可能出现的问题" class="headerlink" title="Storm可能出现的问题"></a>Storm可能出现的问题</h6><ul><li>worker进程死掉</li><li>supervisor进程死掉</li><li>nimbus进程死掉</li><li>节点宕机</li></ul><h6 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h6><ul><li>(acker机制)ack/fail消息确认机制(确保一个tuple被完全处理)<ul><li>在spout中发射tuple的时候需要同时发送messageid，这样才相当于开启了消息确认机制</li><li>如果你的topology里面的tuple比较多的话，那么把acker的数量设置多一点,效率会高一点。</li><li>通过config.setNumAckers(num)来设置一个topology里面的acker的数量，默认值是1。</li><li>注意：acker用了特殊的算法，使得对于追踪每个spout tuple的状态所需要的内存量是恒定的（20 bytes) </li><li>注意：如果一个tuple在指定的timeout(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS默认值为30秒)时间内没有被成功处理，那么这个tuple会被认为处理失败了。</li></ul></li></ul><h4 id="Storm定时器分析"><a href="#Storm定时器分析" class="headerlink" title="Storm定时器分析"></a>Storm定时器分析</h4><ul><li>可以指定每隔一段时间将数据整合一次存入数据库<ul><li>在main中设置conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, 60);// 设置本Bolt定时发射数据</li><li>在bolt中使用下面代码判断是否是触发用的bolt tuple.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID)</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566305060434.png" alt="1566305060434"></li></ul></li></ul><h4 id="StormUI的详解"><a href="#StormUI的详解" class="headerlink" title="StormUI的详解"></a>StormUI的详解</h4><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566219892284.png" alt="1566219892284"></p><ul><li>deactive：未激活(暂停)</li><li>emitted:emitted tuple数<ul><li>与emitted的区别：如果一个task，emitted一个tuple到2个task中，则transferred tuple数是emitted tuple数的两倍</li></ul></li><li>completelatency: spout emitting 一个tuple到spout ack这个tuple的平均时间(可以认为是tuple以及该tuple树的整个处理时间)</li><li>processlatency:   bolt收到一个tuple到bolt ack这个tuple的平均时间，如果没有启动acker机制，那么值为0</li><li>execute latency：bolt处理一个tuple的平均时间，不包含acker操作，单位是毫秒(也就是bolt<br>执行 execute 方法的平均时间)</li><li>capacity：这个值越接近1，说明bolt或者spout基本一直在调用execute方法，说明并行度不够，需要扩展这个组件的executor数量。(调整组件并行度的依据)</li><li>总结：execute latency和proces latnecy是处理消息的时效性，而capacity则表示处理能力是否已经饱和，从这3个参数可以知道topology的瓶颈所在。</li></ul><h4 id="Storm的优化"><a href="#Storm的优化" class="headerlink" title="Storm的优化"></a>Storm的优化</h4><h5 id="并行度的优化"><a href="#并行度的优化" class="headerlink" title="并行度的优化"></a>并行度的优化</h5><ul><li>worker为storm提供工作进程，程序的并行度可以设置（包括spout和bolt的并行度，如果有acker的话还包括acker的并行度），并行度即为executor的数目。</li><li>一般情况下worker与executor的比例是一比十到十五，也可以根据实际需要修改。</li></ul><h5 id="worker的优化"><a href="#worker的优化" class="headerlink" title="worker的优化"></a>worker的优化</h5><ul><li>CPU 16核，建议配置20个worker。CPU 24或32核，30个worker</li><li>默认情况下，Storm启动worker进程时，JVM的最大内存是768M，可以通过在Strom的配置文件storm.yaml中设置worker的启动参数worker.childopts: “-Xmx2048m”</li><li>一个topology使用的worker数量，12个是比较合理的，这个时候吞吐量和整体性能最优。如果多增加worker进程的话，会将一些原本线程间的内存通信变为进程间的网络通信。</li></ul><h5 id="acker优化"><a href="#acker优化" class="headerlink" title="acker优化"></a>acker优化</h5><ul><li>如果可靠性对你来说不是那么重要，那么你可以通过不跟踪这些tuple树来获取更好的性能。不去跟踪消息的话会使得系统里面的消息数量减少一半，因为对于每一个tuple都要发送一个ack消息。</li><li>三种去掉可靠性的方法<ul><li>第一是把config.setNumAckers(0)设置为0，在这种情况下，storm会在spout发射一个tuple之后马上调用spout的ack方法。也就是说这个tuple树不会被跟踪。</li><li>第二个方法是在tuple层面去掉可靠性。你可以在发射tuple的时候不指定messageid来达到不跟踪spout中tuple的目的。</li><li>最后一个方法是如果你对于一个tuple树里面的某一部分到底成不成功不是很关心，那么可以在发射这些tuple的时候unanchor它们(anchor是锚定的意思，unanchor表示不把当前这个tuple包含到tuple树中，也就是说不跟踪这个消息了)。这样这些tuple就不在tuple树里面， 也就不会被跟踪了。</li></ul></li></ul><h4 id="雪崩问题的出现原因以及解决方法"><a href="#雪崩问题的出现原因以及解决方法" class="headerlink" title="雪崩问题的出现原因以及解决方法"></a>雪崩问题的出现原因以及解决方法</h4><ul><li>原因：spout发送的速度大于bolt接收的速度，导致数据堆积，不断消耗内存，最终系统崩溃，并引起数据链上多节点down掉。</li><li>解决方案<ul><li>增加bolt的并行度 增加它接收的速度</li><li>可以通过topology.max.spout.pending来控制spout发送消息的速度，通过代码这样设置config.setMaxSpoutPending(num);<ul><li>注意：这个参数表示，当下游的bolt还有topology.max.spout.pending个 tuple 没有消费完时，spout会停止调用nexttuple方法发射数据。等待下游bolt去消费，当tuple的个数少于topology.max.spout.pending个数时，spout 会继续发射数据(这个属性只对可靠消息处理有用，也就是说需要启用acker消息确认机制，在spout中emit数据的时候需要带有messageid)</li></ul></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> storm storm优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch简介</title>
      <link href="/2019/04/01/Elasticsearch%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/04/01/Elasticsearch%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h1><h3 id="Elasticsearch简介"><a href="#Elasticsearch简介" class="headerlink" title="Elasticsearch简介"></a>Elasticsearch简介</h3><p>​    Elasticsearch是一个实时分布式搜索和分析引擎。它对Lucene进行了封装。能够满足实时搜索的稳定、可靠、快速等。基于REST接口。</p><a id="more"></a><h4 id="ES与MySQL的对比"><a href="#ES与MySQL的对比" class="headerlink" title="ES与MySQL的对比"></a>ES与MySQL的对比</h4><table><thead><tr><th>Elasticsearch</th><th>MySQL</th></tr></thead><tbody><tr><td>index 索引库</td><td>database 数据库</td></tr><tr><td>type 类型</td><td>table 类型</td></tr><tr><td>document 文档</td><td>row 行</td></tr><tr><td>field 字段</td><td>column 列</td></tr></tbody></table><h3 id="Elasticsearch安装部署"><a href="#Elasticsearch安装部署" class="headerlink" title="Elasticsearch安装部署"></a>Elasticsearch安装部署</h3><ol><li><p>安装JDK版本最好在1.8以上（因为这个比较基础就不详细解释了）</p></li><li><p>下载Elasticsearch </p><ul><li>网址：<a href="https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3" target="_blank" rel="noopener">https://www.elastic.co/downloads/past-releases/elasticsearch-6-4-3</a></li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566399839410.png" alt="1566399839410"></li><li>选择合适的版本下载就可以</li></ul></li><li><p>下载完成以后上传到Linux系统中，解压</p></li><li><p>进入到解压后的文件中尝试进行开启</p><ul><li>bin/elasticsearch （-d 后台运行）</li><li>注意：需要关闭机器的防火墙（service iptables stop）关闭开机自启动（chkconfig iptables off)</li><li>会发现执行报错  就是不可以在root用户下打开，选择一个其他用户就可以了</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398085045.png" alt="1566398085045"></li></ul></li><li><p>修改配置变量</p><ul><li><p>修改Elasticsearch中config变量 （vi config/elasticsearch.yml)</p></li><li><p>增加两行代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bootstrap.system_call_filter: false  </span><br><span class="line">network.host: 192.168.32.110   //后面的的IP设置你自己的本地IP就可以</span><br></pre></td></tr></table></figure></li></ul></li></ol><ul><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566398675928.png" alt="1566398675928"></p></li><li><p>修改Linux的配置变量</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line">vi /etc/sysctl.conf</span><br><span class="line">vm.max_map_count=262144</span><br><span class="line">vi /etc/security/limits.d/90-nproc.conf </span><br><span class="line">把1024修改为4096</span><br><span class="line">*          soft    nproc     4096</span><br></pre></td></tr></table></figure></li><li><p>修改完配置以后重启操作系统在启动就可以啦</p></li><li><p>启动之后出现以下情况</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456097622.png" alt="1566456097622"></p></li><li><p>没有报错就表示启动成功了 这时候jps查看进程就可以看到Elasticsearch了</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566456158512.png" alt="1566456158512"></p></li><li><p>然后可以使用web界面查看 在浏览器输入 <a href="http://yourip:9200" target="_blank" rel="noopener">http://yourip:9200</a>  就可以查看了</p></li></ul><h3 id="简单基本操作"><a href="#简单基本操作" class="headerlink" title="简单基本操作"></a>简单基本操作</h3><h4 id="CURL简介"><a href="#CURL简介" class="headerlink" title="CURL简介"></a>CURL简介</h4><ul><li>curl起始就是一个可以在命令行下访问URL的工具</li><li>curl可以利用URL语法在命令行的方式下操作开源的文件</li><li>这样即可以方便我们其他不同部门对我们数据库的操作，也方便我们管理数据库，方便管理其他用户的权限</li></ul><h4 id="CURL的简单操作"><a href="#CURL的简单操作" class="headerlink" title="CURL的简单操作"></a>CURL的简单操作</h4><ul><li><p>-x 是指定http请求的方法</p><ul><li>他的类型有很多种包括 GET POST  PUT DELETE  查询、修改、增加、删除等很多操作</li></ul></li><li><p>-d 是指需要传递的参数</p></li><li><p>首先我们先<strong>创建</strong>一个简单的索引</p><ul><li>curl  -XPUT ‘<a href="http://localhost:9200/test/&#39;" target="_blank" rel="noopener">http://localhost:9200/test/&#39;</a>   localhost一定要换成你之前设置的IP</li><li>这样它就为我们创建了test索引库</li><li><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566457384502.png" alt="1566457384502"></li></ul></li><li><p>然后我们可以创建一个索引并为创建的索引添加一些内容然后进行一些列<strong>查询</strong></p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; //—H指定添加内容的类型为json类型</span><br><span class="line">-XPOST http://localhost:9200/test/emp/1 //1是指定索引的IP 不加系统也会自动生成</span><br><span class="line">-d &apos;&#123;</span><br><span class="line">&quot;name&quot; : &quot;tom&quot;,</span><br><span class="line">&quot;age&quot; : 25</span><br><span class="line">&#125;&apos;   //加入索引中的具体内容</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458468549.png" alt="1566458468549"></p></li><li><p>查询我们刚刚创建的索引</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://localhost:9200/test/emp/1?pretty</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458541679.png" alt="1566458541679"></p></li><li><p>检索索引中的一部分内容</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET &apos;http://localhost:9200/test/emp/1?_source=name&amp;pretty&apos;</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458667647.png" alt="1566458667647"></p></li><li><p>查询指定索引库指定类型的所有数据</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://hadoop110:9200/test/emp/_search?pretty </span><br><span class="line">查看tem类型下的所有数据</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566458873757.png" alt="1566458873757"></p></li></ul></li><li><p>对ES进行<strong>更新</strong>操作，ES中可以使用put或者post两种方式进行更新操作</p><ul><li><p>执行更新操作的时候ES的操作细节</p><ul><li>首先将旧的文件标记为删除状态</li><li>添加新的文件</li><li>旧文件不会立即消失但是我们看不见</li><li>ES在后续你添加更多文件的时候在后台清理掉标记为删除状态的文件</li></ul></li><li><p>执行局部更新的操作</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPOST  http://hadoop110:9200/test/emp/1/_update -d &apos;&#123;&quot;doc&quot;:&#123;&quot;age&quot;:20&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li><li><p>我们接着进行一次查询看数据是否已经更新</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566459328843.png" alt="1566459328843"></p></li><li><p>可以看到年龄已将改成20了 所以说明更新操作成功了 我们可以根据这个操作做很多事情</p></li></ul></li><li><p>对ES进行<strong>删除</strong>操作</p><ul><li><p>删除我们之前创建的索引</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XDELETE http://hadoop110:9200/test/emp/1</span><br></pre></td></tr></table></figure></li><li><p>删除以后我们在进行get获取操作就会报错说明我们的删除操作已经执行成功了</p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566459547355.png" alt="1566459547355"></p></li><li><p>如果删除文档存在 则会返回：200 ok的状态码，found属性值为true，_version属性的值+1</p></li><li><p>如果想要删除的文件不存在就会返回：404 NotFound的状态码，found属性值为false，但是_version属性的值依然会+1，这个就是内部管理的一部分，它保证了我们在多个节点间的不同操作的顺序都被正确标记了</p></li></ul></li><li><p>对ES进行批量操作 包括很多步的增删改查等</p><ul><li><p>批量操作就是bulk API帮助我们同时执行多个操作</p></li><li><p>语法的格式：</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">action：index/create/update/delete  //需要执行的操作类型</span><br><span class="line">metadata：_index,_type,_id   //指定需要操作的索引的索引库、类型、ID等</span><br><span class="line">request body：_source(删除操作不需要)  </span><br><span class="line">&#123; action: &#123; metadata &#125;&#125;   //具体要执行的操作</span><br><span class="line">&#123; request body        &#125;</span><br><span class="line">.........</span><br></pre></td></tr></table></figure></li><li><p>create与index的区别</p><ul><li>在创建数据时，如果数据已存在 create会返回创建失败，文件已存在，但是index会执行成功</li></ul></li><li><p>使用方法： 我们创建一个文件保存我们需要执行的操作</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi requests</span><br><span class="line">&#123; &quot;index&quot; : &#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;emp&quot;,&quot;_id&quot;:&quot;21&quot;&#125;&#125;</span><br><span class="line">&#123; &quot;name&quot; : &quot;test21&quot;&#125;</span><br><span class="line">执行：</span><br><span class="line">curl -H &quot;Content-Type: application/json&quot;  -XPUT localhost:9200/test/emp/_bulk --data-binary @requests</span><br></pre></td></tr></table></figure></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566461216320.png" alt="1566461216320"></p></li><li><p>出现下面结果表示执行成功了</p></li><li><p>我们可以放多条指令进去 同时执行多条指令但是要保证中间格式不出错</p></li></ul></li></ul><h3 id="插件的介绍"><a href="#插件的介绍" class="headerlink" title="插件的介绍"></a>插件的介绍</h3><h4 id="Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES"><a href="#Elasticsearch-Head-Plugin站点插件可以以网页形式展现ES" class="headerlink" title="Elasticsearch Head Plugin站点插件可以以网页形式展现ES"></a>Elasticsearch Head Plugin站点插件可以以网页形式展现ES</h4><ul><li><p>注意：这个插件依赖于nodejs,phantomjs所以我们在安装插件之前需要安装nodejs以及grunt</p><ul><li><p>nodejs下载地址<a href="https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz" target="_blank" rel="noopener">https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz</a></p><ul><li>复制此链接就可以直接下或者<a href="https://nodejs.org/dist" target="_blank" rel="noopener">https://nodejs.org/dist</a> 使用这个两节找适合自己的版本</li></ul></li><li><p>因为此文件是.tar.xz，所以需要先使用xz解压在使用tar解压  如果解压xz的命令不存在就需要使用yum进行下载 yum -y install xz</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">解压：</span><br><span class="line">xz -d node-v10.15.3-linux-x64.tar.xz</span><br><span class="line">tar -xvf node-v10.15.3-linux-x64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/node  /usr/bin/node</span><br><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/npm /usr/bin/npm</span><br></pre></td></tr></table></figure></li><li><p>设定nodejs安装软件的dialing服务器</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></li><li><p>安装grunt</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g grunt </span><br><span class="line">npm install -g grunt-cli</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/node-v10.15.3-linux-x64/bin/grunt  /usr/bin/grunt</span><br></pre></td></tr></table></figure></li><li><p>安装phantomjs</p><ul><li>下载地址<a href="http://phantomjs.org/download.html" target="_blank" rel="noopener">http://phantomjs.org/download.html</a></li></ul></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bzip2 -d phantomjs-2.1.1-linux-x86_64.tar.bz2</span><br><span class="line">tar -xvf phantomjs-2.1.1-linux-x86_64.tar</span><br></pre></td></tr></table></figure></li><li><p>创建软连接</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /data/soft/phantomjs-2.1.1-linux-x86_64/bin/phantomjs  /usr/bin/phantomjs</span><br></pre></td></tr></table></figure></li><li><p>安装依赖软件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install wget fontconfig</span><br></pre></td></tr></table></figure></li></ul></li><li><p>安装head插件</p><ul><li><p>下载地址：<a href="https://github.com/mobz/elasticsearch-head/archive/master.zip" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head/archive/master.zip</a></p></li><li><p>解压 需要使用unzip命令</p></li><li><p>下载unzip</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y unzip</span><br></pre></td></tr></table></figure></li><li><p>解压</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip elasticsearch-head-master.zip</span><br></pre></td></tr></table></figure></li><li><p>然后cd进入到解压的文件中</p></li><li><p>安装两个插件</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm audit fix</span><br><span class="line">npm audit fix --force</span><br></pre></td></tr></table></figure></li><li><p>执行安装命令安装head</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install</span><br></pre></td></tr></table></figure></li><li><p>启动之前修改Gruntfile.js文件，增加hostname参数</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi Gruntfile.js</span><br><span class="line">options: &#123;</span><br><span class="line">hostname: &apos;hadoop100&apos;,</span><br><span class="line">port: 9101,</span><br><span class="line">base: &apos;.&apos;,</span><br><span class="line">keepalive: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>启动服务</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grunt Server</span><br></pre></td></tr></table></figure></li><li><p>启动服务后还需要修改Elasticsearch中的一些配置</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span><br><span class="line">http.cors.enabled: true </span><br><span class="line">http.cors.allow-origin: &quot;*&quot;</span><br></pre></td></tr></table></figure></li><li><p>修改完成后重启ES</p></li><li><p>进入网址<a href="http://hadoop100:9101/可以看到以下信息就说明插件安装成功了" target="_blank" rel="noopener">http://hadoop100:9101/可以看到以下信息就说明插件安装成功了</a></p></li><li><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566465169785.png" alt="1566465169785"></p></li></ul></li></ul><h3 id="配置参数详解"><a href="#配置参数详解" class="headerlink" title="配置参数详解"></a>配置参数详解</h3><ul><li>配置文件elasticsearch.yml</li><li>ES已经为大多数的参数设置了合理的默认值 我们只需要在有特殊需求的时候进行修改</li><li>书写规范<ul><li>属性顶格写，不能有空格</li><li>缩进一定要是用空格而不能使用制表符</li><li>属性与属性值之间必须有一个空格</li></ul></li><li>常见的配置文件以及其含义<ul><li>cluster.name:   集群名称</li><li>node.name  节点名称</li><li>path.data: /path/to/data     es的数据存储目录</li><li>path.logs: /path/to/logs   es的日志存储目录</li><li>bootstrap.memory_lock: true   锁定物理内存地址，防止elasticsearch内存被交换出去,也就是避免es使用swap交换分区中的内存</li><li>network.host: 192.168.0.1   为es设置ip绑定</li><li>http.port: 9200   为es设置自定义端口，默认是9200</li><li>discovery.zen.ping.unicast.hosts: [“host1”, “host2”]    当启动新节点时，通过这个ip列表进行节点发现，组建集群</li><li>discovery.zen.minimum_master_nodes:   通过配置这个参数来防止集群脑裂现象 (集群总节点数量/2)+1</li><li>gateway.recover_after_nodes: 3   一个集群中的N个节点启动后,才允许进行数据恢复处理，默认是1</li><li>action.destructive_requires_name: true   设置是否可以通过正则或者_all删除或者关闭索引库</li></ul></li></ul><h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><ul><li><p>cluster</p><ul><li>代表的是一个集群，集群中有很多节点，其中有一个主节点，这个主节点通过选举产生，主从节点时对于集群内部而言的。es有一个概念叫去中心化，就是说没有中心节点，这个是对于外部来说的，在外部来看，集群就是一个整体，我们两节集群中的任何一个节点与集群通信跟直接与集群通信是等价的。</li><li>主节点的主要职责就是负责管理集群的状态，包括管理分片以及副本的状态，以及节点的删除、新节点的发现等</li><li>注意：主节点不负责对进群的增删改查处理，只负责管理集群状态</li></ul></li><li><p>shards</p><ul><li><p>代表的是索引分片，ES将一个完整的索引分成多个分片，这样的好处是可以把一个大的索引分成多个分片后分布到不同的节点上，构成分布式搜索。提高性能和吞吐量</p></li><li><p>分片的的数量只能在创建索引库的时候指定，索引库创建以后不可以更改</p></li><li><p>索引库默认是5个分片 每个分片最多存储2,147,483,519条数据</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -H &quot;Content-Type: application/json&quot; -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_shards&quot;:3&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>replicas</p><ul><li><p>代表的是分片的副本，es给分片设置副本是为了提高系统的容错性，当某个节点的某个分片损坏或者丢失了可以从副本中恢复。</p></li><li><p>提高es的查询效率，es会自动搜索并请求进行负载均衡</p></li><li><p>默认每个分区只有一个副本，主副本不会存在于一个节点之上，副本数量可以在创建索引库的时候进行设置吧</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT &apos;localhost:9200/test/&apos; -d&apos;&#123;&quot;settings&quot;:&#123;&quot;number_of_replicas&quot;:2&#125;&#125;&apos;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>recovery</p><ul><li>代表数据的恢复或者数据的重新分布</li><li>es在所有节点的加入或者退出后会根据机器的负载对索引分片进行重新分配，挂掉的节点重启时也会进行数据恢复</li></ul></li></ul><h3 id="ElasticsearchJavaAPI操作"><a href="#ElasticsearchJavaAPI操作" class="headerlink" title="ElasticsearchJavaAPI操作"></a>ElasticsearchJavaAPI操作</h3><h4 id="使用Java对ES进行操作"><a href="#使用Java对ES进行操作" class="headerlink" title="使用Java对ES进行操作"></a>使用Java对ES进行操作</h4><ul><li><p>添加maven依赖</p><ul><li><p>可以maven仓库中寻找适合你的版本</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;transport&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;6.4.3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Java中对ES的简单增删改查操作</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">package EsTest;</span><br><span class="line"></span><br><span class="line">import org.elasticsearch.action.delete.DeleteResponse;</span><br><span class="line">import org.elasticsearch.action.get.GetResponse;</span><br><span class="line">import org.elasticsearch.action.index.IndexResponse;</span><br><span class="line">import org.elasticsearch.action.update.UpdateResponse;</span><br><span class="line">import org.elasticsearch.client.transport.TransportClient;</span><br><span class="line">import org.elasticsearch.common.settings.Settings;</span><br><span class="line">import org.elasticsearch.common.transport.TransportAddress;</span><br><span class="line">import org.elasticsearch.common.xcontent.XContentType;</span><br><span class="line">import org.elasticsearch.transport.client.PreBuiltTransportClient;</span><br><span class="line"></span><br><span class="line">import java.net.InetAddress;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">/**</span><br><span class="line"> * Es简单操作测试</span><br><span class="line"> */</span><br><span class="line">public class EsDemo1 &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        //给集群添加自动嗅探的功能</span><br><span class="line">        Settings settings = Settings.builder()</span><br><span class="line">                .put(&quot;cluster.name&quot;, &quot;elasticsearch&quot;) //集群名称</span><br><span class="line">                .put(&quot;client.transport.sniff&quot;, true)  //开启自动嗅探功能，可以自动识别集群内的其他节点信息</span><br><span class="line">                .build();</span><br><span class="line">        //创建连接</span><br><span class="line">        TransportClient client = new PreBuiltTransportClient(Settings.EMPTY)</span><br><span class="line">                //可添加多个节点</span><br><span class="line">                //.addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop100&quot;), 9300))</span><br><span class="line">                .addTransportAddress(new TransportAddress(InetAddress.getByName(&quot;hadoop110&quot;), 9300));</span><br><span class="line"></span><br><span class="line">        //获取节点的信息</span><br><span class="line">        int size = client.connectedNodes().size();</span><br><span class="line">        //System.out.println(size);</span><br><span class="line">        String index = &quot;test&quot;;</span><br><span class="line">        String type = &quot;emp&quot;;</span><br><span class="line">        //添加数据 使用json字符串</span><br><span class="line">        String json = &quot;&#123;\&quot;name\&quot;:\&quot;jack\&quot;,\&quot;age\&quot;:10&#125;&quot;;</span><br><span class="line">        IndexResponse res = client.prepareIndex(index, type, &quot;1&quot;)</span><br><span class="line">                .setSource(json, XContentType.JSON).get();</span><br><span class="line">        //System.out.println(res.toString());</span><br><span class="line">        //添加数据 使用map结构</span><br><span class="line">        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();</span><br><span class="line">        map.put(&quot;name&quot;,&quot;zs&quot;);</span><br><span class="line">        map.put(&quot;age&quot;,21);</span><br><span class="line">        IndexResponse res2 = client.prepareIndex(index, type, &quot;101&quot;)</span><br><span class="line">                .setSource(map)</span><br><span class="line">                .execute()</span><br><span class="line">                .actionGet();</span><br><span class="line">        //更新操作 update</span><br><span class="line">        UpdateResponse updateResponse = client.prepareUpdate(index, type, &quot;101&quot;).setDoc(&quot;&#123;\&quot;age\&quot;:18&#125;&quot;, XContentType.JSON).get();</span><br><span class="line">        //根据ID进行数据查询</span><br><span class="line">        GetResponse get1 = client.prepareGet(index, type, &quot;101&quot;).get();</span><br><span class="line">        System.out.println(get1.getSourceAsString());</span><br><span class="line">        //删除操作 delete</span><br><span class="line">        DeleteResponse deleteResponse = client.prepareDelete(index, type, &quot;101&quot;).get();</span><br><span class="line">        System.out.println(deleteResponse.toString());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm 基础</title>
      <link href="/2019/03/21/Storm%E7%9A%84%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/"/>
      <url>/2019/03/21/Storm%E7%9A%84%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h3 id="Storm的详细分析"><a href="#Storm的详细分析" class="headerlink" title="Storm的详细分析"></a>Storm的详细分析</h3><h4 id="Storm人的概述"><a href="#Storm人的概述" class="headerlink" title="Storm人的概述"></a>Storm人的概述</h4><ul><li>Storm是Twitter开源的一个实时处理框架</li><li>Storm能够实现高频数据和大规模数据的实时处理</li></ul><a id="more"></a><h5 id="Storm与MapReduce的区别Storm"><a href="#Storm与MapReduce的区别Storm" class="headerlink" title="Storm与MapReduce的区别Storm"></a>Storm与MapReduce的区别Storm</h5><table><thead><tr><th>type</th><th>MapReduce</th><th>Storm</th></tr></thead><tbody><tr><td>数据来源</td><td>hdfs上TB级别历史数据</td><td>实时新增的某一条数据</td></tr><tr><td>处理过程</td><td>map阶段和reduce阶段</td><td>可以有很多阶段包含spout以及bolt</td></tr><tr><td>是否会结束</td><td>执行完结束</td><td>不会结束</td></tr><tr><td>处理速度</td><td>主要以执行TB级别数据速度较慢</td><td>只处理新增数据速度很快</td></tr><tr><td>适用场景</td><td>处理批数据不讲时效性</td><td>处理新增数据将时效性</td></tr></tbody></table><h5 id="Spark-Streaming与Storm的区别"><a href="#Spark-Streaming与Storm的区别" class="headerlink" title="Spark Streaming与Storm的区别"></a>Spark Streaming与Storm的区别</h5><table><thead><tr><th>type</th><th>Spark Streaming</th><th>Storm</th></tr></thead><tbody><tr><td>计算模型</td><td>是近实时处理框架</td><td>全实时处理框架</td></tr><tr><td>延迟度</td><td>最高支持秒级别的延迟</td><td>可以支持毫秒级别的延迟</td></tr><tr><td>吞吐量</td><td>因为是批处理所以吞吐量高</td><td>吞吐量相对来说较低</td></tr><tr><td>动态调整并行度</td><td>不支持</td><td>支持</td></tr><tr><td>事务机制</td><td>支持但是不够完善</td><td>支持且完善</td></tr></tbody></table><h5 id="Storm各个组件解释"><a href="#Storm各个组件解释" class="headerlink" title="Storm各个组件解释"></a>Storm各个组件解释</h5><ul><li>Topology：用于封装一个实时计算应用程序的逻辑</li><li>Stream：消息流，是一个没有边界的tuple序列，这些tuple会以分布式的方式进行创建以及处理</li><li>Spout：消息源，消息的生产者，会从外部源获取消息然后向Topology发出：tuple</li><li>Bolt：消息处理者，消息的处理逻辑被封装到bolt之中，处理输入的数据然后产生新的输出数据流</li></ul><h5 id="Storm的设计思想"><a href="#Storm的设计思想" class="headerlink" title="Storm的设计思想"></a>Storm的设计思想</h5><ul><li>是对stream流的一个抽象即一个不间断的连续tuple</li><li>将流中的元素抽象为一个tuple，一个tuple就是一个值列表value list，list中的每个value都有一个name，并且这个value可以是很多数据类型例如基本类型、字符类型等</li><li>每一个stream流都有一个数据源，称为Spout</li><li>stream从spout中获取不间断数据tuple需要经过处理。处理的过程就是stream流转换的过程称为bolt，bolt可以消费任意数量的流，它是将stream汇总的tuple挨个实时进行处理转换成一个新的stream流经过多个bolt处理就可以得到目标数据</li><li>spout+tuple+bolt这个过程可以称为是Topology拓扑。Topology是Storm中最高的一个抽象概念他可以被提交到集群中执行</li><li>Topology的每个节点都要指定他所发射数据的name，其他节点只需要订阅该name就可以接收数据进行处理</li></ul><h5 id="Topology的整个流程"><a href="#Topology的整个流程" class="headerlink" title="Topology的整个流程"></a>Topology的整个流程</h5><ul><li>如果将stream比作是一列火车的话 spout就是这列火车的始发站每一节车厢就是一个tuple乘客就是tuple中的values 中间的站点就相当于是bolt进行处理上下乘客终点站就相当于stream的目标数据</li></ul><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566209818229.png" alt="1566209818229"></p><h5 id="Storm的整体架构图"><a href="#Storm的整体架构图" class="headerlink" title="Storm的整体架构图"></a>Storm的整体架构图</h5><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566215107025.png" alt="1566215107025"></p><h5 id="Storm的简单实例开发"><a href="#Storm的简单实例开发" class="headerlink" title="Storm的简单实例开发"></a>Storm的简单实例开发</h5><ul><li><p>需求：一个源源不断的数据1，2，3，4……求每出现一个数字就要计算出现的所有数字的和</p></li><li><p>开发过程</p><ul><li><p>在IDE中创建maven工程</p></li><li><p>在pom中添加、Storm依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;storm-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0.6&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">package Storme;</span><br><span class="line"></span><br><span class="line">import java.util.Map;</span><br><span class="line"></span><br><span class="line">import org.apache.storm.Config;</span><br><span class="line">import org.apache.storm.LocalCluster;</span><br><span class="line">import org.apache.storm.generated.StormTopology;</span><br><span class="line">import org.apache.storm.spout.SpoutOutputCollector;</span><br><span class="line">import org.apache.storm.task.OutputCollector;</span><br><span class="line">import org.apache.storm.task.TopologyContext;</span><br><span class="line">import org.apache.storm.topology.OutputFieldsDeclarer;</span><br><span class="line">import org.apache.storm.topology.TopologyBuilder;</span><br><span class="line">import org.apache.storm.topology.base.BaseRichBolt;</span><br><span class="line">import org.apache.storm.topology.base.BaseRichSpout;</span><br><span class="line">import org.apache.storm.tuple.Fields;</span><br><span class="line">import org.apache.storm.tuple.Tuple;</span><br><span class="line">import org.apache.storm.tuple.Values;</span><br><span class="line">import org.apache.storm.utils.Utils;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 需求：实现数字累加求和</span><br><span class="line"> * 分析：</span><br><span class="line"> * 需要有一个spout负责源源不断的产生从1开始的递增数字</span><br><span class="line"> * 还需要有一个bolt负责对spout产生的数据进行累加求和，并且把结果打印到控制台</span><br><span class="line"> * 最后把这个spout和bolt组装成一个topology</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public class WordCount &#123;</span><br><span class="line">/**</span><br><span class="line"> * 实现自己的数据源spout，</span><br><span class="line"> * 该spout负责源源不断产生从1开始的递增数字</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public static class MySpout extends BaseRichSpout&#123;</span><br><span class="line"></span><br><span class="line">private Map conf;//这里面存储配置信息</span><br><span class="line">private TopologyContext context;//代表上下文</span><br><span class="line">private SpoutOutputCollector collector;//收集器，主要负责向外面发射数据</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 是一个初始化的方法，这个方法在本实例运行的之后，首先被调用，仅且仅被调用一次</span><br><span class="line"> * 所以这个方法内一般放一些初始化的代码</span><br><span class="line"> * 例子：针对操作mysql数据的案例，使用jdbc获取数据库连接的代码需要放到这里面实现</span><br><span class="line"> */</span><br><span class="line">public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123;</span><br><span class="line">this.conf = conf;</span><br><span class="line">this.context = context;</span><br><span class="line">this.collector = collector;</span><br><span class="line">//System.err.println(&quot;spout------&quot;+conf.get(&quot;name&quot;));</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 这个方法会被循环调用</span><br><span class="line"> */</span><br><span class="line">int i = 1;</span><br><span class="line">public void nextTuple() &#123;</span><br><span class="line">//注意：针对需要发射的数据，需要封装成tuple，可以使用storm中的values对象快速封装tuple</span><br><span class="line">System.out.println(&quot;spout:&quot;+i);</span><br><span class="line">this.collector.emit(new Values(i++));</span><br><span class="line">//让线程每发射一条数据，休息1秒</span><br><span class="line">Utils.sleep(1000);</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 声明输出字段</span><br><span class="line"> * 定义两个组件之间数据传输的一个规则</span><br><span class="line"> * 注意：只要这个组件(spout/spout)向外发射了数据，那么这个declareOutputFields就需要实现</span><br><span class="line"> */</span><br><span class="line">public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;</span><br><span class="line">//注意：Fields中的字段列表和Values中的数据列表是一一对应的</span><br><span class="line">declarer.declare(new Fields(&quot;num&quot;));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 聚合的Bolt，负责把Spout发射出来的数据进行累加求和，并且打印到控制台</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public static class SumBolt extends BaseRichBolt&#123;</span><br><span class="line">private Map stormConf;</span><br><span class="line">private TopologyContext context; </span><br><span class="line">private OutputCollector collector;</span><br><span class="line">/**</span><br><span class="line"> * prepare是一个初始化方法，只会执行一次，这里面也是可以放一些初始化的代码</span><br><span class="line"> */</span><br><span class="line">public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123;</span><br><span class="line">this.stormConf = stormConf;</span><br><span class="line">this.context = context;</span><br><span class="line">this.collector = collector;</span><br><span class="line">//System.err.println(&quot;bolt------&quot;+stormConf.get(&quot;name&quot;));</span><br><span class="line">&#125;</span><br><span class="line">int sum = 0;</span><br><span class="line">/**</span><br><span class="line"> * 这个方法也是会被循环调用</span><br><span class="line"> * 主要上一个组件向外发射了数据，那么这个方法就会被调用一次</span><br><span class="line"> */</span><br><span class="line">public void execute(Tuple input) &#123;</span><br><span class="line">//input.getInteger(0);//通过角标获取数据</span><br><span class="line">Integer num = input.getIntegerByField(&quot;num&quot;);</span><br><span class="line">sum += num;</span><br><span class="line">System.out.println(&quot;和为：&quot;+sum);</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line"> * 注意：这个方法在这里就不需要实现了，因为这个bolt没有向下一个组件发射数据</span><br><span class="line"> */</span><br><span class="line">public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">TopologyBuilder builder = new TopologyBuilder();</span><br><span class="line">//组装spout</span><br><span class="line">builder.setSpout(&quot;spoutid&quot;, new MySpout());</span><br><span class="line">//组装bolt,并且告诉bolt接收哪个组件的数据</span><br><span class="line">builder.setBolt(&quot;bolt-1&quot;, new SumBolt()).shuffleGrouping(&quot;spoutid&quot;);</span><br><span class="line">StormTopology createTopology = builder.createTopology();</span><br><span class="line">//通过代码创建一个本地集群</span><br><span class="line">LocalCluster localCluster = new LocalCluster();</span><br><span class="line">String topologyName = WordCount.class.getSimpleName();</span><br><span class="line">Config config = new Config();</span><br><span class="line">config.put(&quot;name&quot;, &quot;zs&quot;);</span><br><span class="line">//把代码提交到本地集群中运行</span><br><span class="line">localCluster.submitTopology(topologyName, config, createTopology);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Storm核心之并行度"><a href="#Storm核心之并行度" class="headerlink" title="Storm核心之并行度"></a>Storm核心之并行度</h4><h5 id="组件解释"><a href="#组件解释" class="headerlink" title="组件解释"></a>组件解释</h5><ul><li>worker：worker是一个进程，每一个worker进程里面都执行的是一个Topology的任务（不会出现一个worker执行多个Topology的任务）。一个worker中会启动一个或多个executor线程来执行Topology的spout或者bolt组件。一个Topology会使用一个或者多个worker来执行任务</li><li>executor：是worker进程内部启动的独立线程，每一个executor会产生一个或者多个task（storm默认是一个task即一个spout或者bolt有一个task，如果有多个task，executor会循环调用所有task中的实例）</li><li>task：是最终运行spout或者bolt中具体代码的执行单元。Topology启动（spout或者bolt）的task的数目是不变的，但是executor线程的数量可以动态进行调整（例如：1个executor线程可以执行该(spout或bolt)的1个或多个task实例）。这意味着，对于1个(spout或bolt)存在这样的条件：#threads&lt;=#tasks（即：线程数小于等于task数目）。默认情况下task的数目等于executor线程数目，即1个executor线程只运行1个task。</li><li>默认情况下，一个supervisor节点最多可以启动4个worker进程，每一个topology默认占用一个worker进程，每个spout或者bolt会占用1个executor，每个executor启动1个task。</li></ul><p><img src="https://raw.githubusercontent.com/zhaoxintaoaa/images/master/1566213893905.png" alt="1566213893905"></p><h5 id="提高Storm组件的并行度"><a href="#提高Storm组件的并行度" class="headerlink" title="提高Storm组件的并行度"></a>提高Storm组件的并行度</h5><ul><li>worker（slot）【间接】<ul><li>默认一个节点最多可以启动四个worker进程，可以修改进程数量strom-core.jar/defaults.yaml/supervisor.slots.ports</li><li>默认一个Topology只有一个worker进程，可以通过代码指定一个Topology使用多个worker进程config.setNumWorkers(workersnum)</li><li>注意：如果worker使用完在提交Topology就不会执行，会处于等待状态。worker之是通过Netty通信的</li></ul></li><li>executor【直接】<ul><li>默认情况下一个executor只会运行一个task，可以直接通过代码修改增加task数量，会直接提高Storm组件的并行度</li><li>builder.setSpout(id, spout,parallelism_hint);</li><li>builder.setBolt(id, bolt,parallelism_hint);</li></ul></li><li>task【间接】<ul><li>通过boltDeclarer.setNumTasks(num);来设置实例的个数</li><li>executor的数量会小于等于task的数量(为了rebalance)</li></ul></li></ul><h5 id="弹性计算rebalance"><a href="#弹性计算rebalance" class="headerlink" title="弹性计算rebalance"></a>弹性计算rebalance</h5><ul><li>前提是Topology中的task数量要大于executor线程数</li><li>通过shell调整<ul><li>storm rebalance mytopology -w 10 -n 5 -e blue-spout=3 -e yellow-bolt=10</li><li>注意：acker的树木运行时是不会变化的，所以多指定几个worker进程，acker的数量也不会增加</li><li>-w：表示超时时间，Rebalance会在一个超时时间内注销掉Topology，然后在集群中重新分配worker</li><li>-n：表示的是worker的数量</li><li>-e：调整组件的并行度</li><li>注：-n 以及 -e 都可以单独使用或者组合起来使用</li></ul></li><li>通过UI界面进行调整，不建议使用所以就不具体解释使用方法了</li></ul><h5 id="并行度设置多少合适"><a href="#并行度设置多少合适" class="headerlink" title="并行度设置多少合适"></a>并行度设置多少合适</h5><ul><li>单spout每秒大概可以发送500个tuple</li><li>单bolt每秒大概可以接收2000个tuple</li><li>单acker每秒大概可以接收6000个tuple</li><li>根据需要进行调整</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maven 简介</title>
      <link href="/2019/01/21/maven%E7%9A%84%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/01/21/maven%E7%9A%84%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Maven-简介"><a href="#Maven-简介" class="headerlink" title="Maven 简介"></a>Maven 简介</h2><h3 id="为什么需要maven"><a href="#为什么需要maven" class="headerlink" title="为什么需要maven"></a>为什么需要maven</h3><ul><li>同样的代码要在不同的机器上运行他所需要的依赖可以放在maven仓库</li><li>项目组加入新成员可以快速的配置好环境</li><li>在开发其他项目的时候需要用到跟之前项目开发一样的jar包</li></ul><a id="more"></a><h3 id="maven是什么"><a href="#maven是什么" class="headerlink" title="maven是什么"></a>maven是什么</h3><ul><li>maven是基于项目对象模型POM的软件项目管理工具</li><li>是可以跨平台的，主要服务基于Java平台的仙姑构建、依赖管理、项目信息管理等</li></ul><h5 id="构建的过程"><a href="#构建的过程" class="headerlink" title="构建的过程"></a>构建的过程</h5><ul><li>清理</li><li>编译</li><li>测试</li><li>报告</li><li>打包</li><li>部署</li></ul><h3 id="maven的工程结构"><a href="#maven的工程结构" class="headerlink" title="maven的工程结构"></a>maven的工程结构</h3><ul><li>src </li><li><ul><li>mian</li><li><ul><li>java   – 存放Java的文件 源代码等</li><li>resource   –存放资源文件 比如 spring，hibernate等的配置文件</li></ul></li><li>test</li><li><ul><li>Java   – 存放所有的.Java的测试文件，比如JUnit 测试类</li><li>resource   –测试的资源文件夹</li></ul></li></ul></li><li>target       —目标文件的输出位置比如jar包、war包等</li><li>pom.xml      —maven的项目核心配置文件</li></ul><h3 id="maven常用命令"><a href="#maven常用命令" class="headerlink" title="maven常用命令"></a>maven常用命令</h3><ul><li>mvn compile  执行编译 会将生成文件存放在target目录中</li><li>mvn clean  删除target中的目录文件</li><li>mvn test    执行测试命令 执行后会在target目录中生成三个目录文件surefire、surefire-reports（测试报告）、test-classes（测试的字节码文件）</li><li>mvn  package 进行打包操作 操作后的文件存放在target目录之中 例如jar包war包</li><li>mvn install  将制定的jar包安装到本地仓库以便于其他工程的引用</li><li>mvn clean compile 清除测试类再执行compile执行编译操作</li><li>mvn clean test  先清除在进行test测试操作</li><li>mvn clean package 先执行clean清除在执行package打包</li><li>mvn clean install  先进行clean在执行install</li></ul>]]></content>
      
      
      <categories>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> maven </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
